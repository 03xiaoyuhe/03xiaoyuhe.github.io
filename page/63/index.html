<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
<link rel="stylesheet" href="/css/custom.css">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="O3xiaoyuhe">
<meta property="og:url" content="http://example.com/page/63/index.html">
<meta property="og:site_name" content="O3xiaoyuhe">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="听">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/63/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>O3xiaoyuhe</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">O3xiaoyuhe</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%93%BE%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E6%83%B3%E5%AD%A6%E2%80%9C%E8%82%B2%E8%8A%B1%E2%80%9D%E8%BF%98%E6%98%AF%E2%80%9C%E6%8F%92%E8%8A%B1%E2%80%9D%EF%BC%9F%E7%94%A8RouterChain%E7%A1%AE%E5%AE%9A%E5%AE%A2%E6%88%B7%E6%84%8F%E5%9B%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%93%BE%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E6%83%B3%E5%AD%A6%E2%80%9C%E8%82%B2%E8%8A%B1%E2%80%9D%E8%BF%98%E6%98%AF%E2%80%9C%E6%8F%92%E8%8A%B1%E2%80%9D%EF%BC%9F%E7%94%A8RouterChain%E7%A1%AE%E5%AE%9A%E5%AE%A2%E6%88%B7%E6%84%8F%E5%9B%BE/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:09:18" itemprop="dateModified" datetime="2024-11-18T19:09:18+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳。欢迎来到 LangChain 实战课！</p>
<p>上一节课中，我带着你学习了 Chain 的基本概念，还使用了 LLMChain 和 SequencialChain，这一节课，我们再来看看其他类型的一些 Chain 的用法。</p>
<h2 id="任务设定">任务设定</h2>
<p>首先，还是先看一下今天要完成一个什么样的任务。</p>
<p>这里假设咱们的鲜花运营智能客服 ChatBot 通常会接到两大类问题。</p>
<ol>
<li class="lvl-4">
<p><strong>鲜花养护</strong>（保持花的健康、如何浇水、施肥等）</p>
</li>
<li class="lvl-4">
<p><strong>鲜花装饰</strong>（如何搭配花、如何装饰场地等）</p>
</li>
</ol>
<p>你的需求是，<strong>如果接到的是第一类问题，你要给 ChatBot A 指示；如果接到第二类的问题，你要给 ChatBot B 指示</strong>。</p>
<p><img src="https://static001.geekbang.org/resource/image/d8/59/d8491e696c03f49a331c94e31d20e559.jpg?wh=1490x1077" alt=""></p>
<p>我们可以根据这两个场景来构建两个不同的目标链。遇到不同类型的问题，LangChain 会通过 RouterChain 来自动引导大语言模型选择不同的模板。</p>
<p>当然我们的运营过程会遇到更多种类的问题，你只需要通过同样的方法扩充逻辑即可。</p>
<h2 id="整体框架">整体框架</h2>
<p>RouterChain，也叫路由链，能动态选择用于给定输入的下一个链。我们会根据用户的问题内容，首先使用路由器链确定问题更适合哪个处理模板，然后将问题发送到该处理模板进行回答。如果问题不适合任何已定义的处理模板，它会被发送到默认链。</p>
<p>在这里，我们会用 LLMRouterChain 和 MultiPromptChain（也是一种路由链）组合实现路由功能，该 MultiPromptChain 会调用 LLMRouterChain 选择与给定问题最相关的提示，然后使用该提示回答问题。</p>
<p><strong>具体步骤如下：</strong></p>
<ol>
<li class="lvl-4">
<p>构建处理模板：为鲜花护理和鲜花装饰分别定义两个字符串模板。</p>
</li>
<li class="lvl-4">
<p>提示信息：使用一个列表来组织和存储这两个处理模板的关键信息，如模板的键、描述和实际内容。</p>
</li>
<li class="lvl-4">
<p>初始化语言模型：导入并实例化语言模型。</p>
</li>
<li class="lvl-4">
<p>构建目标链：根据提示信息中的每个模板构建了对应的 LLMChain，并存储在一个字典中。</p>
</li>
<li class="lvl-4">
<p>构建 LLM 路由链：这是决策的核心部分。首先，它根据提示信息构建了一个路由模板，然后使用这个模板创建了一个 LLMRouterChain。</p>
</li>
<li class="lvl-4">
<p>构建默认链：如果输入不适合任何已定义的处理模板，这个默认链会被触发。</p>
</li>
<li class="lvl-4">
<p>构建多提示链：使用 MultiPromptChain 将 LLM 路由链、目标链和默认链组合在一起，形成一个完整的决策系统。</p>
</li>
</ol>
<h2 id="具体实现">具体实现</h2>
<p>下面，就是用路由链自动选择处理模板的具体代码实现。</p>
<h3 id="构建提示信息的模板">构建提示信息的模板</h3>
<p>首先，我们针对两种场景，构建两个提示信息的模板。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 构建两个场景的模板</span><br><span class="line">flower_care_template = &quot;&quot;&quot;你是一个经验丰富的园丁，擅长解答关于养花育花的问题。</span><br><span class="line">                        下面是需要你来回答的问题:</span><br><span class="line">                        &#123;input&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">flower_deco_template = &quot;&quot;&quot;你是一位网红插花大师，擅长解答关于鲜花装饰的问题。</span><br><span class="line">                        下面是需要你来回答的问题:</span><br><span class="line">                        &#123;input&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 构建提示信息</span><br><span class="line">prompt_infos = [</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;key&quot;: &quot;flower_care&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;适合回答关于鲜花护理的问题&quot;,</span><br><span class="line">        &quot;template&quot;: flower_care_template,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;key&quot;: &quot;flower_decoration&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;适合回答关于鲜花装饰的问题&quot;,</span><br><span class="line">        &quot;template&quot;: flower_deco_template,</span><br><span class="line">    &#125;]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="初始化语言模型">初始化语言模型</h3>
<p>接下来，我们初始化语言模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 初始化语言模型</span><br><span class="line">from langchain.llms import OpenAI</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI Key&#x27;</span><br><span class="line">llm = OpenAI()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="构建目标链">构建目标链</h3>
<p>下面，我们循环 prompt_infos 这个列表，构建出两个目标链，分别负责处理不同的问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 构建目标链</span><br><span class="line">from langchain.chains.llm import LLMChain</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line">chain_map = &#123;&#125;</span><br><span class="line">for info in prompt_infos:</span><br><span class="line">    prompt = PromptTemplate(template=info[&#x27;template&#x27;], </span><br><span class="line">                            input_variables=[&quot;input&quot;])</span><br><span class="line">    print(&quot;目标提示:\n&quot;,prompt)</span><br><span class="line">    chain = LLMChain(llm=llm, prompt=prompt,verbose=True)</span><br><span class="line">    chain_map[info[&quot;key&quot;]] = chain</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里，目标链提示是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">目标提示:</span><br><span class="line">input_variables=[&#x27;input&#x27;] </span><br><span class="line">output_parser=None partial_variables=&#123;&#125; </span><br><span class="line">template=&#x27;你是一个经验丰富的园丁，擅长解答关于养花育花的问题。\n                        下面是需要你来回答的问题:\n                        </span><br><span class="line">&#123;input&#125;&#x27; template_format=&#x27;f-string&#x27; </span><br><span class="line">validate_template=True</span><br><span class="line"></span><br><span class="line">目标提示:</span><br><span class="line">input_variables=[&#x27;input&#x27;] </span><br><span class="line">output_parser=None partial_variables=&#123;&#125; </span><br><span class="line">template=&#x27;你是一位网红插花大师，擅长解答关于鲜花装饰的问题。\n                        下面是需要你来回答的问题:\n                        </span><br><span class="line">&#123;input&#125;&#x27; template_format=&#x27;f-string&#x27; </span><br><span class="line">validate_template=True</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>对于每个场景，我们创建一个 LLMChain（语言模型链）。每个链会根据其场景模板生成对应的提示，然后将这个提示送入语言模型获取答案。</p>
<h3 id="构建路由链">构建路由链</h3>
<p>下面，我们构建路由链，负责查看用户输入的问题，确定问题的类型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 构建路由链</span><br><span class="line">from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser</span><br><span class="line">from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE as RounterTemplate</span><br><span class="line">destinations = [f&quot;&#123;p[&#x27;key&#x27;]&#125;: &#123;p[&#x27;description&#x27;]&#125;&quot; for p in prompt_infos]</span><br><span class="line">router_template = RounterTemplate.format(destinations=&quot;\n&quot;.join(destinations))</span><br><span class="line">print(&quot;路由模板:\n&quot;,router_template)</span><br><span class="line">router_prompt = PromptTemplate(</span><br><span class="line">    template=router_template,</span><br><span class="line">    input_variables=[&quot;input&quot;],</span><br><span class="line">    output_parser=RouterOutputParser(),)</span><br><span class="line">print(&quot;路由提示:\n&quot;,router_prompt)</span><br><span class="line">router_chain = LLMRouterChain.from_llm(llm, </span><br><span class="line">                                       router_prompt,</span><br><span class="line">                                       verbose=True)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">路由模板:</span><br><span class="line"> Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.</span><br><span class="line"></span><br><span class="line">&lt;&lt; FORMATTING &gt;&gt;</span><br><span class="line">Return a markdown code snippet with a JSON object formatted to look like:</span><br><span class="line">```json</span><br><span class="line">&#123;&#123;</span><br><span class="line">    &quot;destination&quot;: string \ name of the prompt to use or &quot;DEFAULT&quot;</span><br><span class="line">    &quot;next_inputs&quot;: string \ a potentially modified version of the original input</span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>REMEMBER: “destination” MUST be one of the candidate prompt names specified below OR it can be “DEFAULT” if the input is not well suited for any of the candidate prompts.<br>
REMEMBER: “next_inputs” can just be the original input if you don</p>
<p>&lt;&lt; CANDIDATE PROMPTS &gt;&gt;<br>
flower_care: 适合回答关于鲜花护理的问题<br>
flower_decoration: 适合回答关于鲜花装饰的问题</p>
<p>&lt;&lt; INPUT &gt;&gt;<br>
{input}</p>
<p>&lt;&lt; OUTPUT &gt;&gt;</p>
<p>路由提示:<br>
input_variables=[<br>
partial_variables={}<br>
template=<br>
&lt;&lt; FORMATTING &gt;&gt;\n<br>
Return a markdown code snippet with a JSON object formatted to look like:\n<code>json\n&#123;&#123;\n "destination": string \\ name of the prompt to use or "DEFAULT"\n    "next_inputs": string \\ a potentially modified version of the original input\n&#125;&#125;\n</code>\n\n<br>
REMEMBER: “destination” MUST be one of the candidate prompt names specified below OR it can be “DEFAULT” if the input is not well suited for any of the candidate prompts.\n<br>
REMEMBER: “next_inputs” can just be the original input if you don<br>
flower_care: 适合回答关于鲜花护理的问题\n<br>
flower_decoration: 适合回答关于鲜花装饰的问题\n\n<br>
&lt;&lt; INPUT &gt;&gt;\n{input}\n\n&lt;&lt; OUTPUT &gt;&gt;\n<br>
template_format=<br>
validate_template=True</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这里我说一下路由器链是如何构造提示信息，来引导大模型查看用户输入的问题并确定问题的类型的。</span><br><span class="line"></span><br><span class="line">先看路由模板部分，这段模板字符串是一个指导性的说明，目的是引导语言模型正确处理用户的输入，并将其定向到适当的模型提示。</span><br><span class="line"></span><br><span class="line">**1. 路由模板的解释**</span><br><span class="line"></span><br><span class="line">路由模板是路由功能得以实现的核心。我们来详细分解一下这个模板的每个部分。</span><br><span class="line"></span><br><span class="line">**引言**</span><br><span class="line"></span><br><span class="line">Given a raw text input to a language model select the model prompt best suited for the input.</span><br><span class="line"></span><br><span class="line">这是一个简单的引导语句，告诉模型你将给它一个输入，它需要根据这个输入选择最适合的模型提示。</span><br><span class="line"></span><br><span class="line">You will be given the names of the available prompts and a description of what the prompt is best suited for.</span><br><span class="line"></span><br><span class="line">这里进一步提醒模型，它将获得各种模型提示的名称和描述。</span><br><span class="line"></span><br><span class="line">You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.</span><br><span class="line"></span><br><span class="line">这是一个可选的步骤，告诉模型它可以更改原始输入以获得更好的响应。</span><br><span class="line"></span><br><span class="line">**格式说明 (&lt;&lt;FORMATTING&gt;&gt;)**</span><br><span class="line"></span><br><span class="line">指导模型如何格式化其输出，使其以特定的方式返回结果。</span><br><span class="line"></span><br><span class="line">Return a markdown code snippet with a JSON object formatted to look like:</span><br><span class="line"></span><br><span class="line">表示模型的输出应该是一个 Markdown 代码片段，其中包含一个特定格式的 JSON 对象。</span><br><span class="line"></span><br><span class="line">下面的代码块显示了期望的 JSON 结构，其中 destination 是模型选择的提示名称（或 “DEFAULT”），而 next_inputs 是可能被修订的原始输入。</span><br><span class="line"></span><br><span class="line">**额外的说明和要求**</span><br><span class="line"></span><br><span class="line">REMEMBER: &quot;destination&quot; MUST be one of the candidate prompt names specified below OR it can be &quot;DEFAULT&quot;...</span><br><span class="line"></span><br><span class="line">这是一个重要的指导，提醒模型 &quot;destination&quot; 字段的值必须是下面列出的提示之一或是 “DEFAULT”。</span><br><span class="line"></span><br><span class="line">REMEMBER: &quot;next_inputs&quot; can just be the original input if you don&#x27;t think any modifications are needed.</span><br><span class="line"></span><br><span class="line">这再次强调，除非模型认为有必要，否则原始输入不需要修改。</span><br><span class="line"></span><br><span class="line">**候选提示 (&lt;&lt;CANDIDATE PROMPTS&gt;&gt;)**</span><br><span class="line"></span><br><span class="line">列出了两个示例模型提示及其描述：</span><br><span class="line"></span><br><span class="line">*   “flower_care: 适合回答关于鲜花护理的问题”，适合处理与花卉护理相关的问题。</span><br><span class="line">*   “flower_decoration: 适合回答关于鲜花装饰的问题”，适合处理与花卉装饰相关的问题。</span><br><span class="line"></span><br><span class="line">**输入 / 输出部分**</span><br><span class="line"></span><br><span class="line">&lt;&lt;INPUT&gt;&gt;\n&#123;input&#125;\n\n&lt;&lt; OUTPUT &gt;&gt;\n：</span><br><span class="line"></span><br><span class="line">这部分为模型提供了一个格式化的框架，其中它将接收一个名为 &#123;input&#125; 的输入，并在此后的部分输出结果。</span><br><span class="line"></span><br><span class="line">总的来说，这个模板的目的是让模型知道如何处理用户的输入，并根据提供的提示列表选择一个最佳的模型提示来回应。</span><br><span class="line"></span><br><span class="line">**2. 路由提示的解释**</span><br><span class="line"></span><br><span class="line">路由提示 (router_prompt）则根据路由模板，生成了具体传递给 LLM 的路由提示信息。</span><br><span class="line"></span><br><span class="line">*   其中 input_variables 指定模板接收的输入变量名，这里只有 `&quot;input&quot;`。</span><br><span class="line">*   output_parser 是一个用于解析模型输出的对象，它有一个默认的目的地和一个指向下一输入的键。</span><br><span class="line">*   template 是实际的路由模板，用于给模型提供指示。这就是刚才详细解释的模板内容。</span><br><span class="line">*   template_format 指定模板的格式，这里是 `&quot;f-string&quot;`。</span><br><span class="line">*   validate_template 是一个布尔值，如果为 True，则会在使用模板前验证其有效性。</span><br><span class="line"></span><br><span class="line">简而言之，这个构造允许你将用户的原始输入送入路由器，然后路由器会决定将该输入发送到哪个具体的模型提示，或者是否需要对输入进行修订以获得最佳的响应。</span><br><span class="line"></span><br><span class="line">### 构建默认链</span><br><span class="line"></span><br><span class="line">除了处理目标链和路由链之外，我们还需要准备一个默认链。如果路由链没有找到适合的链，那么，就以默认链进行处理。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1>构建默认链</h1>
<p>from langchain.chains import ConversationChain<br>
default_chain = ConversationChain(llm=llm,<br>
                                  output_key=“text”,<br>
                                  verbose=True)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 构建多提示链</span><br><span class="line"></span><br><span class="line">最后，我们使用 MultiPromptChain 类把前几个链整合在一起，实现路由功能。这个 MultiPromptChain 类是一个多路选择链，它使用一个 LLM 路由器链在多个提示之间进行选择。</span><br><span class="line"></span><br><span class="line">**MultiPromptChain 中有三个关键元素。**</span><br><span class="line"></span><br><span class="line">*   router_chain（类型 RouterChain）：这是用于决定目标链和其输入的链。当给定某个输入时，这个 router_chain 决定哪一个 destination_chain 应该被选中，以及传给它的具体输入是什么。</span><br><span class="line">*   destination_chains（类型 Mapping[str, LLMChain]）：这是一个映射，将名称映射到可以将输入路由到的候选链。例如，你可能有多种处理文本输入的方法（或 “链”），每种方法针对特定类型的问题。destination_chains 可以是这样一个字典：`&#123;&#x27;weather&#x27;: weather_chain, &#x27;news&#x27;: news_chain&#125;`。在这里，weather_chain 可能专门处理与天气相关的问题，而 news_chain 处理与新闻相关的问题。</span><br><span class="line">*   default_chain（类型 LLMChain）：当 router_chain 无法将输入映射到 destination_chains 中的任何一个链时，LLMChain 将使用此默认链。这是一个备选方案，确保即使路由器不能决定正确的链，也总有一个链可以处理输入。</span><br><span class="line"></span><br><span class="line">**它的工作流程如下：**</span><br><span class="line"></span><br><span class="line">1.  输入首先传递给 router_chain。</span><br><span class="line">2.  router_chain 根据某些标准或逻辑决定应该使用哪一个 destination_chain。</span><br><span class="line">3.  输入随后被路由到选定的 destination_chain，该链进行处理并返回结果。</span><br><span class="line">4.  如果 router_chain 不能决定正确的 destination_chain，则输入会被传递给 default_chain。</span><br><span class="line"></span><br><span class="line">这样，MultiPromptChain 就为我们提供了一个在多个处理链之间动态路由输入的机制，以得到最相关或最优的输出。</span><br><span class="line"></span><br><span class="line">实现代码如下：</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1>构建多提示链</h1>
<p>from langchain.chains.router import MultiPromptChain<br>
chain = MultiPromptChain(<br>
    router_chain=router_chain,<br>
    destination_chains=chain_map,<br>
    default_chain=default_chain,<br>
    verbose=True)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 运行路由链</span><br><span class="line"></span><br><span class="line">好了，至此我们的链路已经准备好了。现在开始提出各种问题，测试一下我们的链。</span><br><span class="line"></span><br><span class="line">**测试 A：**</span><br><span class="line"></span><br><span class="line">print(chain.run(&quot;如何为玫瑰浇水？&quot;))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"></span><br><span class="line">![](https://static001.geekbang.org/resource/image/89/a2/89d0bfac97b259b93240a10cf777d9a2.png?wh=1097x821)</span><br><span class="line"></span><br><span class="line">**测试 B：**</span><br><span class="line"></span><br><span class="line">print(chain.run(&quot;如何为婚礼场地装饰花朵？&quot;))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"></span><br><span class="line">![](https://static001.geekbang.org/resource/image/4f/ed/4f848ca6592476358a25bf91996aa0ed.png?wh=1095x834)</span><br><span class="line"></span><br><span class="line">**测试 C：**</span><br><span class="line"></span><br><span class="line">print(chain.run(&quot;如何考入哈佛大学？&quot;))</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"></span><br><span class="line">![](https://static001.geekbang.org/resource/image/ac/12/acd4a69df2cef81b1f7bcf33f9b4bb12.png?wh=1093x806)</span><br><span class="line"></span><br><span class="line">这三个测试，分别被路由到了三个不同的目标链，其中两个是我们预设的 “专家类型” 目标链，而第三个问题：如何考入哈佛大学？被模型一眼看穿，并不属于任何鲜花运营业务场景，路由链把它抛入了一个 “default chain” —— ConversationChain 去解决。</span><br><span class="line"></span><br><span class="line">## 总结时刻</span><br><span class="line"></span><br><span class="line">在这个示例中，我们看到了 LLMRouterChain 以及 MultiPromptChain。其中，LLMRouterChain 继承自 RouterChain；而 MultiPromptChain 则继承自 MultiRouteChain。</span><br><span class="line"></span><br><span class="line">整体上，我们通过 MultiPromptChain 把其他链组织起来，完成了路由功能。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>chain = MultiPromptChain(<br>
    router_chain=router_chain,<br>
    destination_chains=chain_map,<br>
    default_chain=default_chain,<br>
    verbose=True)</p>
<pre><code>
在 LangChain 的 chains -&gt; router -&gt; base.py 文件中，可以看到 RouterChain 和 MultiRouteChain 的代码实现。

## 思考题

1.  通过 verbose=True 这个选项的设定，在输出时显示了链的开始和结束日志，从而得到其相互调用流程。请你尝试把该选项设置为 False，看一看输出结果有何不同。
2.  在这个例子中，我们使用了 ConversationChain 作为 default_chain，这个 Chain 是 LLMChain 的子类，你能否把这个 Chain 替换为 LLMChain？

期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。

## 延伸阅读

1.  代码，RouterChain 和 MultiRouteChain 的[实现细节](https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain%2Fblob%2Fmaster%2Flibs%2Flangchain%2Flangchain%2Fchains%2Frouter%2Fbase.py &quot;https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/router/base.py&quot;)
2.  代码，MultiPromptChain 的[实现细节](https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain%2Fblob%2Fmaster%2Flibs%2Flangchain%2Flangchain%2Fchains%2Frouter%2Fmulti_prompt.py &quot;https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/router/multi_prompt.py&quot;)</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E9%B2%9C%E8%8A%B1%E7%BD%91%E7%BB%9C%E7%94%B5%E5%95%86%E7%9A%84%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%8A%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E9%B2%9C%E8%8A%B1%E7%BD%91%E7%BB%9C%E7%94%B5%E5%95%86%E7%9A%84%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%8A%EF%BC%89/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:35:22" itemprop="dateModified" datetime="2024-11-18T19:35:22+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>从今天开始，我要用 4 节课的篇幅，带着你设计两个有趣而又实用的应用程序。设计这两个应用程序的目的，是为了让你能够把 LangChain 中的各个组件灵活地组合起来，融会贯通，并以此作为启发，在你熟悉的业务场景中，利用 LangChain 和 LLM 的能力，开发出更多、更强大的效率工具。</p>
<p>第一个应用程序，是用 LangChain 创建出一个专属于 “易速鲜花” 的网络人脉工具。光这么说，有些模糊，这个人脉工具长啥样？有些啥具体功能？</p>
<p>动手之前，让我先给你把这个所谓 “人脉” 工具的能力和细节说清楚。</p>
<h2 id="“人脉工具”-项目说明">“人脉工具” 项目说明</h2>
<p><strong>项目背景</strong>：易速鲜花电商网络自从创建以来，通过微信、抖音、小红书等自媒体宣传推广，短期内获得了广泛流量展示。目前，营销部门希望以此为契机，再接再厉，继续扩大品牌影响力。经过调研，发现很多用户会通过微博热搜推荐的新闻来购买鲜花赠送给明星、达人等，因此各部门一致认为应该联络相关微博大 V，共同推广，带动品牌成长。</p>
<p>然而，发掘并选择适合于 “鲜花推广” 的微博大 V 有一定难度。营销部门员工表示，这个任务比找微信、抖音和小红书达人要难得多。他们都希望技术部门能够给出一个 “人脉搜索工具” 来协助完成这一目标。</p>
<p><strong>项目目标：</strong> 帮助市场营销部门的员工找到微博上适合做鲜花推广的大 V，并给出具体的联络方案。</p>
<h2 id="项目的技术实现细节">项目的技术实现细节</h2>
<p>这个项目的具体技术实现细节，这里简述如下。</p>
<p><strong>第一步：</strong> 通过 LangChain 的搜索工具，以模糊搜索的方式，帮助运营人员找到微博中有可能对相关鲜花推广感兴趣的大 V（比如喜欢玫瑰花的大 V），并返回 UID。</p>
<p><strong>第二步：</strong> 根据微博 UID，通过爬虫工具拿到相关大 V 的微博公开信息，并以 JSON 格式返回大 V 的数据。</p>
<p><img src="https://static001.geekbang.org/resource/image/00/5f/0049810d3cfe1aee633d29722ded8e5f.png?wh=1860x1612" alt=""></p>
<p><strong>第三步：</strong> 通过 LangChain 调用 LLM，通过 LLM 的总结整理以及生成功能，根据大 V 的个人信息，写一篇热情洋溢的介绍型文章，谋求与该大 V 的合作。</p>
<p><strong>第四步：</strong> 把 LangChain 输出解析功能加入进来，让 LLM 生成可以嵌入提示模板的格式化数据结构。</p>
<p><strong>第五步：</strong> 添加 HTML、CSS，并用 Flask 创建一个 App，在网络上部署及发布这个鲜花电商人脉工具，供市场营销部门的人员使用。</p>
<p>在上面的 5 个步骤中，我们使用到很多 LangChain 技术，包括<strong>提示工程、模型、链、代理、输出解析</strong>等。</p>
<p>这节课我们先来实现项目的前两个部分。</p>
<h2 id="第一步：找到大-V">第一步：找到大 V</h2>
<p>因为咱们的项目需要用到很多工具，所以我创建了一个项目目录，叫做 socializer_v0（项目每完成一步，我就创建一个新目录，并把版本号加 1）。当第一个步骤 “找到大 V” 实现之后，项目中的文档结构如下。</p>
<p><img src="https://static001.geekbang.org/resource/image/5d/0c/5dab492f802d34086975616d06708e0c.jpg?wh=152x270" alt=""></p>
<p>这里，主程序是 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>。意思就是派程序来作为智能代理，找到喜欢鲜花的微博大 V。</p>
<h2 id="主程序-findbigV-py">主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a></h2>
<p>主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a> 在第一步完成之后，是这样的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;&#x27;</span><br><span class="line">os.environ[&quot;SERPAPI_API_KEY&quot;] = &#x27;&#x27;</span><br><span class="line"></span><br><span class="line"># 导入所取的库</span><br><span class="line">import re</span><br><span class="line">from agents.weibo_agent import lookup_V</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    # 拿到UID</span><br><span class="line">    response_UID = lookup_V(flower_type = &quot;牡丹&quot; )</span><br><span class="line">    print(response_UID)</span><br><span class="line"></span><br><span class="line">    # 抽取UID里面的数字</span><br><span class="line">    UID = re.findall(r&#x27;\d+&#x27;, response_UID)[0]</span><br><span class="line">    print(&quot;这位鲜花大V的微博ID是&quot;, UID)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里，我们要搜到的，是一个热爱鲜花的大 V 的微博 UID，而不是 URL。</p>
<p><img src="https://static001.geekbang.org/resource/image/52/f1/520688ef98a70c3d3651420bcc26bef1.jpg?wh=1805x1292" alt=""></p>
<p>比如，上面这位喜欢牡丹花的大 V，他的 UID 是 6053338099。这些都是公开的信息。</p>
<p>为什么我们希望得到 UID 呢？因为我们可以通过这个 ID，爬取他个人主页里的更多介绍信息，有利于进一步了解他。</p>
<h3 id="微博-Agent：查找大-V-的-ID">微博 Agent：查找大 V 的 ID</h3>
<p>下面，我们就来看看，文件 agents\weibo_agent.py 中的 lookup_V 函数是如何实现这个搜寻 UID 的功能的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 导入一个搜索UID的工具</span><br><span class="line">from tools.search_tool import get_UID</span><br><span class="line"></span><br><span class="line"># 导入所需的库</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.agents import initialize_agent, Tool</span><br><span class="line">from langchain.agents import AgentType</span><br><span class="line"></span><br><span class="line"># 通过LangChain代理找到UID的函数</span><br><span class="line">def lookup_V(flower_type: str) :</span><br><span class="line">    # 初始化大模型</span><br><span class="line">    llm = ChatOpenAI(temperature=0, model_name=&quot;gpt-3.5-turbo&quot;)</span><br><span class="line"></span><br><span class="line">    # 寻找UID的模板</span><br><span class="line">    template = &quot;&quot;&quot;given the &#123;flower&#125; I want you to get a related 微博 UID.</span><br><span class="line">                  Your answer should contain only a UID.</span><br><span class="line">                  The URL always starts with https://weibo.com/u/</span><br><span class="line">                  for example, if https://weibo.com/u/1669879400 is her 微博, then 1669879400 is her UID</span><br><span class="line">                  This is only the example don&#x27;t give me this, but the actual UID&quot;&quot;&quot;</span><br><span class="line">    # 完整的提示模板</span><br><span class="line">    prompt_template = PromptTemplate(</span><br><span class="line">        input_variables=[&quot;flower&quot;], template=template</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 代理的工具</span><br><span class="line">    tools = [</span><br><span class="line">        Tool(</span><br><span class="line">            name=&quot;Crawl Google for 微博 page&quot;,</span><br><span class="line">            func=get_UID,</span><br><span class="line">            description=&quot;useful for when you need get the 微博 UID&quot;,</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    # 初始化代理</span><br><span class="line">    agent = initialize_agent(</span><br><span class="line">        tools, </span><br><span class="line">        llm, </span><br><span class="line">        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, </span><br><span class="line">        verbose=True</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 返回找到的UID</span><br><span class="line">    ID = agent.run(prompt_template.format_prompt(flower=flower_type))</span><br><span class="line"></span><br><span class="line">    return ID</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这段代码的目的，是为了通过提供的花的类型（flower type）来查找与之相关的微博 UID。其中使用了 LangChain 中的代理和工具。</p>
<p>这里有两点需要特别说明：</p>
<ol>
<li class="lvl-4">
<p>搜索 UID 的工具通过 from tools.search_tool import get_UID 导入，这个内容后面还会介绍。</p>
</li>
<li class="lvl-4">
<p>下面的提示模板说明，强调了需要的是 UID，而不是 URL。刚才说了，这是因为后续的爬虫工具需要一个特定的 UID，来获取该微博大 V 的个人信息（公开）。然后我们会继续利用这些信息让 LLM 为我们写 “勾搭” 文案。</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    # 寻找UID的模板</span><br><span class="line">    template = &quot;&quot;&quot;given the &#123;flower&#125; I want you to get a related 微博 UID.</span><br><span class="line">                  Your answer should contain only a UID.</span><br><span class="line">                  The URL always starts with https://weibo.com/u/</span><br><span class="line">                  for example, if https://weibo.com/u/1669879400 is her 微博, then 1669879400 is her UID</span><br><span class="line">                  This is only the example don&#x27;t give me this, but the actual UID&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="定制的-SerpAPI：getUID">定制的 SerpAPI：getUID</h3>
<p>上面的程序只是调用了代理，但是没有给出具体的工具实现。现在我们来继续实现搜索大 V 的 UID 的功能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 导入一个搜索UID的工具</span><br><span class="line">from tools.search_tool import get_UID</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个具体的实现，在代码 \tools\search_tool.py 中。</p>
<p>说到通过 LangChain 来搜索微博，相信你会马上想到已经多次使用过的 SerpAPI。我们先来试一试标准的 SerpAPI，看看它能否满足我们的需求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from langchain.utilities import SerpAPIWrapper</span><br><span class="line"></span><br><span class="line">def get_UID(flower: str):</span><br><span class="line">    &quot;&quot;&quot;Searches for Linkedin or twitter Profile Page.&quot;&quot;&quot;</span><br><span class="line">    search = SerpAPIWrapper()</span><br><span class="line">    res = search.run(f&quot;&#123;flower&#125;&quot;)</span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>写好了这段代码，第一步就可以说是完成了。下面我们跑一遍 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，看看程序会给出我们什么样的结果。</p>
<p><img src="https://static001.geekbang.org/resource/image/a2/d8/a22043515a4b9686c58ccedcda2075d8.jpg?wh=1336x670" alt=""></p>
<p>结果还好，不算太失望，SerpAPI 找到了一个貌似喜欢牡丹花的大 V，名叫戏精牡丹，搜到的信息也都是真实的。看起来他蛮适合为我们的牡丹花代言。然而，这个大 V 的微博 ID 肯定不是 6。</p>
<p>中间哪里或许是出了点小问题。</p>
<p>像这样的错误，明显发生在 LangChain 内部，那你的 trouble_shooting 也只能通过 Debug 来解决。这里，我就忽略掉一长串的错误排查过程，直接指出问题的根本原因所在。</p>
<p>让我们把断点设置在 SerpAPIWrapper 类的_process_response 中。</p>
<p><img src="https://static001.geekbang.org/resource/image/86/88/86238bae7452cde52f23e4d6ea3a1688.jpg?wh=1909x1398" alt=""></p>
<p>当程序进入 <code>if &quot;organic_results&quot; in res.keys()</code> 这段逻辑之后，我发现，它返回的总是一个 snippet（摘要文字），而不是 link（URL）。</p>
<p><img src="https://static001.geekbang.org/resource/image/38/be/38643a041b2f24ed9e8405a485580cbe.jpg?wh=1275x1473" alt=""></p>
<p>无论这背后的逻辑何在，这并不是我们所想要的。在 Debug 过程中，我们发现，新浪微博的 UID，实际上包含在 URL 中，也就是 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fweibo.com%2Fu%2F6053338099" title="https://weibo.com/u/6053338099">weibo.com/u/605333809…</a>。因此，如果我们不返回微博的简短说明（戏精牡丹，搞笑视频自媒体……），而是返回 URL，会更有利于大模型提炼出 UID。</p>
<p><img src="https://static001.geekbang.org/resource/image/2a/c0/2a78e4b1cc0734f1c5ce171a05f153c0.jpg?wh=1275x465" alt=""></p>
<p>如何做呢？直接修改 LangChain 的 SerpAPIWrapper 类的_process_response 源代码肯定不是一个好办法。</p>
<p>因此，这里我们可以继承 SerpAPIWrapper 类，并构造一个 CustomSerpAPIWrapper 类，在这个类中，我们重构_process_response 这个静态方法。</p>
<p>新的 search_tool.py 完整代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># 导入SerpAPIWrapper</span><br><span class="line">from langchain.utilities import SerpAPIWrapper</span><br><span class="line"></span><br><span class="line"># 重新定制SerpAPIWrapper，重构_process_response，返回URL</span><br><span class="line">class CustomSerpAPIWrapper(SerpAPIWrapper):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CustomSerpAPIWrapper, self).__init__()</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def _process_response(res: dict) -&gt; str:</span><br><span class="line">        &quot;&quot;&quot;Process response from SerpAPI.&quot;&quot;&quot;</span><br><span class="line">        if &quot;error&quot; in res.keys():</span><br><span class="line">            raise ValueError(f&quot;Got error from SerpAPI: &#123;res[&#x27;error&#x27;]&#125;&quot;)</span><br><span class="line">        if &quot;answer_box_list&quot; in res.keys():</span><br><span class="line">            res[&quot;answer_box&quot;] = res[&quot;answer_box_list&quot;]</span><br><span class="line">        &#x27;&#x27;&#x27;删去很多无关代码&#x27;&#x27;&#x27;</span><br><span class="line">        snippets = []</span><br><span class="line">        if &quot;knowledge_graph&quot; in res.keys():</span><br><span class="line">            knowledge_graph = res[&quot;knowledge_graph&quot;]</span><br><span class="line">            title = knowledge_graph[&quot;title&quot;] if &quot;title&quot; in knowledge_graph else &quot;&quot;</span><br><span class="line">            if &quot;description&quot; in knowledge_graph.keys():</span><br><span class="line">                snippets.append(knowledge_graph[&quot;description&quot;])</span><br><span class="line">            for key, value in knowledge_graph.items():</span><br><span class="line">                if (</span><br><span class="line">                    isinstance(key, str)</span><br><span class="line">                    and isinstance(value, str)</span><br><span class="line">                    and key not in [&quot;title&quot;, &quot;description&quot;]</span><br><span class="line">                    and not key.endswith(&quot;_stick&quot;)</span><br><span class="line">                    and not key.endswith(&quot;_link&quot;)</span><br><span class="line">                    and not value.startswith(&quot;http&quot;)</span><br><span class="line">                ):</span><br><span class="line">                    snippets.append(f&quot;&#123;title&#125; &#123;key&#125;: &#123;value&#125;.&quot;)</span><br><span class="line">        if &quot;organic_results&quot; in res.keys():</span><br><span class="line">            first_organic_result = res[&quot;organic_results&quot;][0]</span><br><span class="line">            if &quot;snippet&quot; in first_organic_result.keys():</span><br><span class="line">                # 此处是关键修改</span><br><span class="line">                # snippets.append(first_organic_result[&quot;snippet&quot;])</span><br><span class="line">                snippets.append(first_organic_result[&quot;link&quot;])                </span><br><span class="line">            elif &quot;snippet_highlighted_words&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;snippet_highlighted_words&quot;])</span><br><span class="line">            elif &quot;rich_snippet&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;rich_snippet&quot;])</span><br><span class="line">            elif &quot;rich_snippet_table&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;rich_snippet_table&quot;])</span><br><span class="line">            elif &quot;link&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;link&quot;])</span><br><span class="line">        if &quot;buying_guide&quot; in res.keys():</span><br><span class="line">            snippets.append(res[&quot;buying_guide&quot;])</span><br><span class="line">        if &quot;local_results&quot; in res.keys() and &quot;places&quot; in res[&quot;local_results&quot;].keys():</span><br><span class="line">            snippets.append(res[&quot;local_results&quot;][&quot;places&quot;])</span><br><span class="line"></span><br><span class="line">        if len(snippets) &gt; 0:</span><br><span class="line">            return str(snippets)</span><br><span class="line">        else:</span><br><span class="line">            return &quot;No good search result found&quot;</span><br><span class="line"></span><br><span class="line"># 获取与某种鲜花相关的微博UID的函数</span><br><span class="line">def get_UID(flower: str):</span><br><span class="line">    &quot;&quot;&quot;Searches for Linkedin or twitter Profile Page.&quot;&quot;&quot;</span><br><span class="line">    # search = SerpAPIWrapper()</span><br><span class="line">    search = CustomSerpAPIWrapper()</span><br><span class="line">    res = search.run(f&quot;&#123;flower&#125;&quot;)</span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>唯一的区别就是，我们在下面的逻辑中返回了 link，而不是 snippet。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">        if &quot;organic_results&quot; in res.keys():</span><br><span class="line">            first_organic_result = res[&quot;organic_results&quot;][0]</span><br><span class="line">            if &quot;snippet&quot; in first_organic_result.keys():</span><br><span class="line">                # snippets.append(first_organic_result[&quot;snippet&quot;])</span><br><span class="line">                snippets.append(first_organic_result[&quot;link&quot;]) </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>再次 Debug，我们发现返回的 snippets 里面包含了 URL 信息，其中 UID 信息包含在 URL 中了。</p>
<p><img src="https://static001.geekbang.org/resource/image/06/5e/06da407d1f6d93eeaefa6e9f450cdf5e.jpg?wh=1837x1631" alt=""></p>
<p>此时运行主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，会发现代理中返回了 URL 信息，并且经过进一步思考，提炼出了 UID。</p>
<p><img src="https://static001.geekbang.org/resource/image/ab/6f/ab67de9be2b4f26be53c0e8af713f16f.jpg?wh=819x326" alt=""></p>
<h2 id="第二步：爬取大-V-资料">第二步：爬取大 V 资料</h2>
<p>好的，第一步虽然是有磕有绊，但是经过了调整的 CustomSerpAPIWrapper 工具和代理，在 LLM 的帮助之下，总算是不辱使命，完成了找到 UID 的任务。</p>
<p>这位大 V，看起来又喜欢牡丹，又喜欢搞笑。我们很想和他联络一下，也许他很适合为我们的牡丹花品牌代言。（到底是否适合，不必特别认真哈，总之搜索 “牡丹”，Agent 给了这个 ID，就可以了。咱学的是 LangChain，不是真的要找他代言）</p>
<p>不过，知己知彼，百战不殆。想要和他沟通，就得了解他更多。下面，我们将使用爬虫程序，通过 UID 来爬取他的更多信息。</p>
<h3 id="主程序-findbigV-py-2">主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a></h3>
<p>第二步完成之后，主程序代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;Your OpenAI API Key&#x27;</span><br><span class="line">os.environ[&quot;SERPAPI_API_KEY&quot;] = &#x27;Your SerpAPI Key&#x27;</span><br><span class="line"></span><br><span class="line"># 导入所取的库</span><br><span class="line">import re</span><br><span class="line">from agents.weibo_agent import lookup_V</span><br><span class="line">from tools.general_tool import remove_non_chinese_fields</span><br><span class="line">from tools.scraping_tool import get_data</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    # 拿到UID</span><br><span class="line">    response_UID = lookup_V(flower_type = &quot;牡丹&quot; )</span><br><span class="line"></span><br><span class="line">    # 抽取UID里面的数字</span><br><span class="line">    UID = re.findall(r&#x27;\d+&#x27;, response_UID)[0]</span><br><span class="line">    print(&quot;这位鲜花大V的微博ID是&quot;, UID)</span><br><span class="line"></span><br><span class="line">    # 根据UID爬取大V信息</span><br><span class="line">    person_info = get_data(UID)</span><br><span class="line">    print(person_info)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从第一步到第二步，我们主要是完成了一次微博信息的爬取。</p>
<h3 id="scraping-tool-py-中的-scrape-weibo-方法">scraping_tool.py 中的 scrape_weibo 方法</h3>
<p>第二步中的关键逻辑是 scraping_tool.py 中的 scrape_weibo 方法，具体代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 导入所需的库</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># 定义爬取微博用户信息的函数</span><br><span class="line">def scrape_weibo(url: str):</span><br><span class="line">    &#x27;&#x27;&#x27;爬取相关鲜花服务商的资料&#x27;&#x27;&#x27;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36&quot;,</span><br><span class="line">        &quot;Referer&quot;: &quot;https://weibo.com&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    cookies = &#123;</span><br><span class="line">        &quot;cookie&quot;: &#x27;&#x27;&#x27;SINAGLOBAL=3762226753815.13.1696496172299; ALF=1699182321; SCF=AiOo8xtPwGonZcAbYyHXZbz9ixm97mWi0vHt_VvuOKB-u4-rcvlGtWCrE6MfMucpxiOy5bYpkIFNWTj7nYGcyp4.; _sc_token=v2%3A2qyeqD3cTZFNTl0sn3KAYe4fNqzMUEP-C7nxNsd_Q1r-vpYMlF2K3xc4vWNuLNBbp3RsohghkJdlSVN09cymVo5AKAm0V92004V8cSRe9O5v9B65jd4yiG_sATDeB06GnjiJulXUrEF_6XsHh1ozK6jvbTKEUIkF7v0_BlbX6IcWrPkwh6xL_WM_0YUV2v7CtNPwyxfbAjaWnG32TsxG_ftN3s5m7qfaRftU6iTOSnE%3D; XSRF-TOKEN=4o0E6jaUQ0BlN77az0sURTg3; PC_TOKEN=dcf0e7607f; login_sid_t=36ebf31f1b3694fb71e77e35d30f052f; cross_origin_proto=SSL; WBStorage=4d96c54e|undefined; _s_tentry=passport.weibo.com; UOR=www.google.com,weibo.com,login.sina.com.cn; Apache=7563213131783.361.1696667509205; ULV=1696667509207:2:2:2:7563213131783.361.1696667509205:1696496172302; wb_view_log=3440*14401; WBtopGlobal_register_version=2023100716; crossidccode=CODE-gz-1QP2Jh-13l47h-79FGqrAQgQbR8ccb7b504; SSOLoginState=1696667553; SUB=_2A25IJWfwDeThGeFJ6lsQ-SbNzjuIHXVr5gm4rDV8PUJbkNAbLUWtkW1NfJd_XHamKIzj5RlT_-RGMma6z3YQZUK3; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFDKvBlvg14YuHk_4c6MEH_5NHD95QNS024eK.ReK-NWs4DqcjZCJ8oIN.pSKzceBtt; WBPSESS=gyY2mn77F4p5VxWF2IB_yFR0phHVTNfaJAHAMprnW7MeUr-NHPZNyeeyKae3tHELlc_RbcI1XPSz-TjSJqWrIXs-yh1fwhxL4mSDrnpPZEogFt8ScF5NEwSqPGn7x2KMAgTHtWde-3MBm6orQ98PDA==&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url, headers=headers, cookies=cookies)</span><br><span class="line">    time.sleep(3)   # 加上3s 的延时防止被反爬</span><br><span class="line">    return response.text</span><br><span class="line"></span><br><span class="line"># 根据UID构建URL爬取信息</span><br><span class="line">def get_data(id):</span><br><span class="line">    url = &quot;https://weibo.com/ajax/profile/detail?uid=&#123;&#125;&quot;.format(id)</span><br><span class="line">    html = scrape_weibo(url)</span><br><span class="line">    response = json.loads(html)</span><br><span class="line"></span><br><span class="line">    return response</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我这段爬虫代码特别简洁，不需要过多的解释，唯一需要说明的部分是怎么找到你自己的 Cookies。</p>
<p>Cookie 是由服务器发送到用户浏览器的一小段数据，并可能在随后的请求中被回传。它的主要目的是让服务器知道用户的上下文信息或状态。在 Web 爬虫中，使用正确的 Cookie 可以模拟登录状态，从而获取到需要权限的网页内容。</p>
<p>首先，我是用 QQ ID 登录的微博，我发现通过这样的方式找到的 Cookie 能用得比较久。</p>
<p>然后，从我的浏览器中获取 Cookie，以下是简单步骤：</p>
<ol>
<li class="lvl-4">
<p>使用浏览器（如 Chrome、Firefox）访问微博并登录。</p>
</li>
<li class="lvl-4">
<p>登录后，右键单击页面并选择 “检查”（Inspect）。</p>
</li>
<li class="lvl-4">
<p>打开开发者工具，点击 Network 选项卡。</p>
</li>
<li class="lvl-4">
<p>在页面上进行一些操作（如刷新页面），然后在 Network 选项卡下查看请求列表。</p>
</li>
<li class="lvl-4">
<p>选择任一请求项，然后在右侧的 Headers 选项卡中查找 Request Headers 部分。</p>
</li>
<li class="lvl-4">
<p>在这部分中，你应该可以看到一个名为 Cookie 的字段，这就是你需要的 Cookie 值。</p>
</li>
</ol>
<p>将获取到的完整 Cookie 值复制（挺长的），并替换上述代码中的 <code>&quot;你的Cookie&quot;</code> 部分。</p>
<p><img src="https://static001.geekbang.org/resource/image/92/38/92ea6832ea69c8a1342a62180b7da538.jpg?wh=3842x1944" alt=""></p>
<p>但请注意，微博的 Cookie 可能有过期时间，所以如果你发现一段时间后你的爬虫无法正常工作，你可能需要再次获取新的 Cookie。同时，频繁地爬取或大量请求可能会导致你的账号被封禁，所以请谨慎使用爬虫。</p>
<p>此时，运行 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，就得到了下面的输出。</p>
<p><img src="https://static001.geekbang.org/resource/image/e6/94/e696b6dae6332a486763e29e9f310594.jpg?wh=1318x665" alt=""></p>
<h3 id="精简爬取输出">精简爬取输出</h3>
<p>最后一个步骤，是精简上面的输出，因为类似 <code>'word_color': '#FFEA8011', 'background_color': '#FF181818'</code> 这样的内容会占据很多 Token 空间，而且对于 LLM 总结整理信息，也没啥作用。</p>
<p>因此，我创建了一个额外的步骤，就是 \ tools\general_tool.py 中的 remove_non_chinese_fields 函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">def contains_chinese(s):</span><br><span class="line">    return bool(re.search(&#x27;[\u4e00-\u9fa5]&#x27;, s))</span><br><span class="line"></span><br><span class="line">def remove_non_chinese_fields(d):</span><br><span class="line">    if isinstance(d, dict):</span><br><span class="line">        to_remove = [key for key, value in d.items() if isinstance(value, (str, int, float, bool)) and (not contains_chinese(str(value)))]</span><br><span class="line">        for key in to_remove:</span><br><span class="line">            del d[key]</span><br><span class="line">        </span><br><span class="line">        for key, value in d.items():</span><br><span class="line">            if isinstance(value, (dict, list)):</span><br><span class="line">                remove_non_chinese_fields(value)</span><br><span class="line">    elif isinstance(d, list):</span><br><span class="line">        to_remove_indices = []</span><br><span class="line">        for i, item in enumerate(d):</span><br><span class="line">            if isinstance(item, (str, int, float, bool)) and (not contains_chinese(str(item))):</span><br><span class="line">                to_remove_indices.append(i)</span><br><span class="line">            else:</span><br><span class="line">                remove_non_chinese_fields(item)</span><br><span class="line">        </span><br><span class="line">        for index in reversed(to_remove_indices):</span><br><span class="line">            d.pop(index)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a> 中，调用这个函数，对爬虫的输出结果进行了精简。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    # 移除无用的信息</span><br><span class="line">    remove_non_chinese_fields(person_info)</span><br><span class="line">    print(person_info)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>重新运行 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，结果如下：</p>
<p><img src="https://static001.geekbang.org/resource/image/16/10/16aaa4d952428bb1960ecfee9df6df10.jpg?wh=1310x217" alt=""></p>
<p>此时，爬取的内容就只剩下了干货。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>这节课我们完成了前两步的工作。分别是，找到适合推广某种鲜花的大 V 的微博 UID，并且爬取了大 V 的资料。这为我们后续生成文本、进一步链接大 V 打下了良好的基础。</p>
<p>其中，我们用到了大量之前学习过的 LangChain 组件，具体包括：</p>
<ol>
<li class="lvl-4">
<p>用提示模板告诉大模型我们要找到内容（UID）。</p>
</li>
<li class="lvl-4">
<p>调用 LLM。</p>
</li>
<li class="lvl-4">
<p>使用 Chain。</p>
</li>
<li class="lvl-4">
<p>使用 Agent。</p>
</li>
<li class="lvl-4">
<p>在 Agent 中，我们使用了一个 Customized Tool，因为 LangChain 内置的 SerpAPI Tool 不能完全满足我们的需要。这给了我们一个好机会创建自己的 “私人定制” Tool。</p>
</li>
</ol>
<p>在下节课中，我们还要继续利用大模型的总结文本、生成文本的功能，来为我们撰写能够打动大 V 和咱易速鲜花合作的文案，我们还将利用 Output Parser 把文案解析成需要的格式，部署到网络服务器端。敬请期待！</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>如果 Agent 不返回 UID，而是返回 URL，是不是也能够完成这个任务？你可以尝试重构提示模板以及后续逻辑，返回 URL，然后手动从 URL 中解析出 UID。</p>
</li>
<li class="lvl-4">
<p>研究一下 SerpAPIWrapper 类的_process_response 中的代码，看看这个方法具体是怎么设计的，用来实现了什么功能？</p>
</li>
</ol>
<p>期待在留言区看到你的分享，如果觉得内容对你有帮助，也欢迎分享给有需要的朋友！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%93%BE%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E5%86%99%E4%B8%80%E7%AF%87%E5%AE%8C%E7%BE%8E%E9%B2%9C%E8%8A%B1%E6%8E%A8%E6%96%87%EF%BC%9F%E7%94%A8SequencialChain%E9%93%BE%E6%8E%A5%E4%B8%8D%E5%90%8C%E7%9A%84%E7%BB%84%E4%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%93%BE%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E5%86%99%E4%B8%80%E7%AF%87%E5%AE%8C%E7%BE%8E%E9%B2%9C%E8%8A%B1%E6%8E%A8%E6%96%87%EF%BC%9F%E7%94%A8SequencialChain%E9%93%BE%E6%8E%A5%E4%B8%8D%E5%90%8C%E7%9A%84%E7%BB%84%E4%BB%B6/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:08:56" itemprop="dateModified" datetime="2024-11-18T19:08:56+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳。欢迎来到 LangChain 实战课！</p>
<p>到这节课，我们已经学到了不少 LangChain 的应用，也体会到了 LangChain 功能的强大。但也许你心里开始出现了一个疑问：LangChain，其中的 <strong>Chain</strong> 肯定是关键组件，为什么我们还没有讲到呢？</p>
<p>这的确是个好问题。对于简单的应用程序来说，直接调用 LLM 就已经足够了。因此，在前几节课的示例中，我们主要通过 LangChain 中提供的提示模板、模型接口以及输出解析器就实现了想要的功能。</p>
<h2 id="什么是-Chain">什么是 Chain</h2>
<p>但是，如果你想开发更复杂的应用程序，那么就需要通过 “Chain” 来链接 LangChain 的各个组件和功能——模型之间彼此链接，或模型与其他组件链接。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3808c2a6e06d4e6a860d6d5aba7b3949~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1965&amp;h=1363&amp;s=497499&amp;e=png&amp;b=ffffff" alt=""></p>
<p>这种将多个组件相互链接，组合成一个链的想法简单但很强大。它简化了复杂应用程序的实现，并使之更加模块化，能够创建出单一的、连贯的应用程序，从而使调试、维护和改进应用程序变得容易。</p>
<p><strong>说到链的实现和使用，也简单。</strong></p>
<ul class="lvl-0">
<li class="lvl-4">
<p>首先 LangChain 通过设计好的接口，实现一个具体的链的功能。例如，LLM 链（LLMChain）能够接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化的响应传递给 LLM。这就相当于把整个 Model I/O 的流程封装到链里面。</p>
</li>
<li class="lvl-4">
<p>实现了链的具体功能之后，我们可以通过将多个链组合在一起，或者将链与其他组件组合来构建更复杂的链。</p>
</li>
</ul>
<p>所以你看，链在内部把一系列的功能进行封装，而链的外部则又可以组合串联。<strong>链其实可以被视为 LangChain 中的一种基本功能单元。</strong></p>
<p>LangChain 中提供了很多种类型的预置链，目的是使各种各样的任务实现起来更加方便、规范。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d1fd75ae07f241c4a7f956296d5691ae~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1702&amp;h=861&amp;s=478647&amp;e=jpg&amp;b=fafbfe" alt=""></p>
<p>我们先使用一下最基础也是最常见的 LLMChain。</p>
<h2 id="LLMChain：最简单的链">LLMChain：最简单的链</h2>
<p>LLMChain 围绕着语言模型推理功能又添加了一些功能，整合了 PromptTemplate、语言模型（LLM 或聊天模型）和 Output Parser，相当于把 Model I/O 放在一个链中整体操作。它使用提示模板格式化输入，将格式化的字符串传递给 LLM，并返回 LLM 输出。</p>
<p>举例来说，如果我想让大模型告诉我某种花的花语，如果不使用链，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#----第一步 创建提示</span><br><span class="line"># 导入LangChain中的提示模板</span><br><span class="line">from langchain import PromptTemplate</span><br><span class="line"># 原始字符串模板</span><br><span class="line">template = &quot;&#123;flower&#125;的花语是?&quot;</span><br><span class="line"># 创建LangChain模板</span><br><span class="line">prompt_temp = PromptTemplate.from_template(template) </span><br><span class="line"># 根据模板创建提示</span><br><span class="line">prompt = prompt_temp.format(flower=&#x27;玫瑰&#x27;)</span><br><span class="line"># 打印提示的内容</span><br><span class="line">print(prompt)</span><br><span class="line"></span><br><span class="line">#----第二步 创建并调用模型 </span><br><span class="line"># 导入LangChain中的OpenAI模型接口</span><br><span class="line">from langchain import OpenAI</span><br><span class="line"># 创建模型实例</span><br><span class="line">model = OpenAI(temperature=0)</span><br><span class="line"># 传入提示，调用模型，返回结果</span><br><span class="line">result = model(prompt)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">玫瑰的花语是?</span><br><span class="line">爱情、浪漫、美丽、永恒、誓言、坚贞不渝。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>此时 Model I/O 的实现分为两个部分，提示模板的构建和模型的调用独立处理。</p>
<p>如果使用链，代码结构则显得更简洁。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 导入所需的库</span><br><span class="line">from langchain import PromptTemplate, OpenAI, LLMChain</span><br><span class="line"># 原始字符串模板</span><br><span class="line">template = &quot;&#123;flower&#125;的花语是?&quot;</span><br><span class="line"># 创建模型实例</span><br><span class="line">llm = OpenAI(temperature=0)</span><br><span class="line"># 创建LLMChain</span><br><span class="line">llm_chain = LLMChain(</span><br><span class="line">    llm=llm,</span><br><span class="line">    prompt=PromptTemplate.from_template(template))</span><br><span class="line"># 调用LLMChain，返回结果</span><br><span class="line">result = llm_chain(&quot;玫瑰&quot;)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;flower&#x27;: &#x27;玫瑰&#x27;, &#x27;text&#x27;: &#x27;\n\n爱情、浪漫、美丽、永恒、誓言、坚贞不渝。&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在这里，我们就把提示模板的构建和模型的调用封装在一起了。</p>
<h2 id="链的调用方式">链的调用方式</h2>
<p>链有很多种调用方式。</p>
<h3 id="直接调用">直接调用</h3>
<p>刚才我们是直接调用的链对象。当我们像函数一样调用一个对象时，它实际上会调用该对象内部实现的__call__方法。</p>
<p>如果你的提示模板中包含多个变量，在调用链的时候，可以使用字典一次性输入它们。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[&quot;flower&quot;, &quot;season&quot;],</span><br><span class="line">    template=&quot;&#123;flower&#125;在&#123;season&#125;的花语是?&quot;,</span><br><span class="line">)</span><br><span class="line">llm_chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line">print(llm_chain(&#123;</span><br><span class="line">    &#x27;flower&#x27;: &quot;玫瑰&quot;,</span><br><span class="line">    &#x27;season&#x27;: &quot;夏季&quot; &#125;))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;flower&#x27;: &#x27;玫瑰&#x27;, &#x27;season&#x27;: &#x27;夏季&#x27;, &#x27;text&#x27;: &#x27;\n\n玫瑰在夏季的花语是爱的誓言，热情，美丽，坚定的爱情。&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="通过-run-方法">通过 run 方法</h3>
<p>通过 run 方法，也等价于直接调用_call_函数。</p>
<p>语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">llm_chain(&quot;玫瑰&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>等价于：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">llm_chain.run(&quot;玫瑰&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="通过-predict-方法">通过 predict 方法</h3>
<p>predict 方法类似于 run，只是输入键被指定为关键字参数而不是 Python 字典。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = llm_chain.predict(flower=&quot;玫瑰&quot;)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="通过-apply-方法">通过 apply 方法</h3>
<p>apply 方法允许我们针对输入列表运行链，一次处理多个输入。</p>
<p>示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># apply允许您针对输入列表运行链</span><br><span class="line">input_list = [</span><br><span class="line">    &#123;&quot;flower&quot;: &quot;玫瑰&quot;,&#x27;season&#x27;: &quot;夏季&quot;&#125;,</span><br><span class="line">    &#123;&quot;flower&quot;: &quot;百合&quot;,&#x27;season&#x27;: &quot;春季&quot;&#125;,</span><br><span class="line">    &#123;&quot;flower&quot;: &quot;郁金香&quot;,&#x27;season&#x27;: &quot;秋季&quot;&#125;</span><br><span class="line">]</span><br><span class="line">result = llm_chain.apply(input_list)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;[&#123;&#x27;text&#x27;: &#x27;\n\n玫瑰在夏季的花语是“恋爱”、“热情”和“浪漫”。&#x27;&#125;, </span><br><span class="line">&#123;&#x27;text&#x27;: &#x27;\n\n百合在春季的花语是“爱情”和“友谊”。&#x27;&#125;,</span><br><span class="line"> &#123;&#x27;text&#x27;: &#x27;\n\n郁金香在秋季的花语表达的是“热情”、“思念”、“爱恋”、“回忆”和“持久的爱”。&#x27;&#125;]&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="通过-generate-方法">通过 generate 方法</h3>
<p>generate 方法类似于 apply，只不过它返回一个 LLMResult 对象，而不是字符串。LLMResult 通常包含模型生成文本过程中的一些相关信息，例如令牌数量、模型名称等。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = llm_chain.generate(input_list)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">generations=[[Generation(text=&#x27;\n\n玫瑰在夏季的花语是“热情”、“爱情”和“幸福”。&#x27;, </span><br><span class="line">generation_info=&#123;&#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;logprobs&#x27;: None&#125;)], </span><br><span class="line">[Generation(text=&#x27;\n\n春季的花语是爱情、幸福、美满、坚贞不渝。&#x27;, </span><br><span class="line">generation_info=&#123;&#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;logprobs&#x27;: None&#125;)], </span><br><span class="line">[Generation(text=&#x27;\n\n秋季的花语是“思念”。银色的百合象征着“真爱”，而淡紫色的郁金香则象征着“思念”，因为它们在秋天里绽放的时候，犹如在思念着夏天的温暖。&#x27;, </span><br><span class="line">generation_info=&#123;&#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;logprobs&#x27;: None&#125;)]] </span><br><span class="line">llm_output=&#123;&#x27;token_usage&#x27;: &#123;&#x27;completion_tokens&#x27;: 243, &#x27;total_tokens&#x27;: 301, &#x27;prompt_tokens&#x27;: 58&#125;, &#x27;model_name&#x27;: &#x27;gpt-3.5-turbo-instruct&#x27;&#125; </span><br><span class="line">run=[RunInfo(run_id=UUID(&#x27;13058cca-881d-4b76-b0cf-0f9c831af6c4&#x27;)), </span><br><span class="line">RunInfo(run_id=UUID(&#x27;7f38e33e-bab5-4d03-b77c-f50cd195affb&#x27;)), </span><br><span class="line">RunInfo(run_id=UUID(&#x27;7a1e45fd-77ee-4133-aab0-431147186db8&#x27;))]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Sequential-Chain：顺序链">Sequential Chain：顺序链</h2>
<p>好，到这里，你已经掌握了最基本的 LLMChain 的用法。下面，我要带着你用 Sequential Chain 把几个 LLMChain 串起来，形成一个顺序链。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/114bc3ade4764d2e9aaa18a646f9367e~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=665&amp;h=360&amp;s=114905&amp;e=png&amp;b=fefefe" alt=""></p>
<p>这个示例中，我们的目标是这样的：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>第一步，我们假设大模型是一个植物学家，让他给出某种特定鲜花的知识和介绍。</p>
</li>
<li class="lvl-4">
<p>第二步，我们假设大模型是一个鲜花评论者，让他参考上面植物学家的文字输出，对鲜花进行评论。</p>
</li>
<li class="lvl-4">
<p>第三步，我们假设大模型是易速鲜花的社交媒体运营经理，让他参考上面植物学家和鲜花评论者的文字输出，来写一篇鲜花运营文案。</p>
</li>
</ul>
<p>下面我们就来一步步地实现这个示例。</p>
<p>首先，导入所有需要的库。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI API Key&#x27;</span><br><span class="line"></span><br><span class="line">from langchain.llms import OpenAI</span><br><span class="line">from langchain.chains import LLMChain</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line">from langchain.chains import SequentialChain</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>然后，添加第一个 LLMChain，生成鲜花的知识性说明。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 这是第一个LLMChain，用于生成鲜花的介绍，输入为花的名称和种类</span><br><span class="line">llm = OpenAI(temperature=.7)</span><br><span class="line">template = &quot;&quot;&quot;</span><br><span class="line">你是一个植物学家。给定花的名称和类型，你需要为这种花写一个200字左右的介绍。</span><br><span class="line"></span><br><span class="line">花名: &#123;name&#125;</span><br><span class="line">颜色: &#123;color&#125;</span><br><span class="line">植物学家: 这是关于上述花的介绍:&quot;&quot;&quot;</span><br><span class="line">prompt_template = PromptTemplate(input_variables=[&quot;name&quot;, &quot;color&quot;], template=template)</span><br><span class="line">introduction_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=&quot;introduction&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>接着，添加第二个 LLMChain，根据鲜花的知识性说明生成评论。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 这是第二个LLMChain，用于根据鲜花的介绍写出鲜花的评论</span><br><span class="line">llm = OpenAI(temperature=.7)</span><br><span class="line">template = &quot;&quot;&quot;</span><br><span class="line">你是一位鲜花评论家。给定一种花的介绍，你需要为这种花写一篇200字左右的评论。</span><br><span class="line"></span><br><span class="line">鲜花介绍:</span><br><span class="line">&#123;introduction&#125;</span><br><span class="line">花评人对上述花的评论:&quot;&quot;&quot;</span><br><span class="line">prompt_template = PromptTemplate(input_variables=[&quot;introduction&quot;], template=template)</span><br><span class="line">review_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=&quot;review&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>接着，添加第三个 LLMChain，根据鲜花的介绍和评论写出一篇自媒体的文案。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 这是第三个LLMChain，用于根据鲜花的介绍和评论写出一篇自媒体的文案</span><br><span class="line">template = &quot;&quot;&quot;</span><br><span class="line">你是一家花店的社交媒体经理。给定一种花的介绍和评论，你需要为这种花写一篇社交媒体的帖子，300字左右。</span><br><span class="line"></span><br><span class="line">鲜花介绍:</span><br><span class="line">&#123;introduction&#125;</span><br><span class="line">花评人对上述花的评论:</span><br><span class="line">&#123;review&#125;</span><br><span class="line"></span><br><span class="line">社交媒体帖子:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">prompt_template = PromptTemplate(input_variables=[&quot;introduction&quot;, &quot;review&quot;], template=template)</span><br><span class="line">social_post_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=&quot;social_post_text&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后，添加 SequentialChain，把前面三个链串起来。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 这是总的链，我们按顺序运行这三个链</span><br><span class="line">overall_chain = SequentialChain(</span><br><span class="line">    chains=[introduction_chain, review_chain, social_post_chain],</span><br><span class="line">    input_variables=[&quot;name&quot;, &quot;color&quot;],</span><br><span class="line">    output_variables=[&quot;introduction&quot;,&quot;review&quot;,&quot;social_post_text&quot;],</span><br><span class="line">    verbose=True)</span><br><span class="line"></span><br><span class="line"># 运行链，并打印结果</span><br><span class="line">result = overall_chain(&#123;&quot;name&quot;:&quot;玫瑰&quot;, &quot;color&quot;: &quot;黑色&quot;&#125;)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最终的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; Entering new  chain...</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br><span class="line">&#123;&#x27;name&#x27;: &#x27;玫瑰&#x27;, &#x27;color&#x27;: &#x27;黑色&#x27;, </span><br><span class="line">&#x27;introduction&#x27;: &#x27;\n\n黑色玫瑰，这是一种对传统玫瑰花的独特颠覆，它的出现挑战了我们对玫瑰颜色的固有认知。它的花瓣如煤炭般黑亮，反射出独特的微光，而花蕊则是金黄色的，宛如夜空中的一颗星，强烈的颜色对比营造出一种前所未有的视觉效果。在植物学中，黑色玫瑰的出现无疑提供了一种新的研究方向，对于我们理解花朵色彩形成的机制有着重要的科学价值。&#x27;, </span><br><span class="line">&#x27;review&#x27;: &#x27;\n\n黑色玫瑰，这不仅仅是一种花朵，更是一种完全颠覆传统的艺术表现形式。黑色的花瓣仿佛在诉说一种不可言喻的悲伤与神秘，而黄色的蕊瓣犹如漆黑夜空中的一抹亮色，给人带来无尽的想象。它将悲伤与欢乐，神秘与明亮完美地结合在一起，这是一种全新的视觉享受，也是一种对生活理解的深度表达。&#x27;, </span><br><span class="line">&#x27;social_post_text&#x27;: &#x27;\n欢迎来到我们的自媒体平台，今天，我们要向您展示的是我们的全新产品——黑色玫瑰。这不仅仅是一种花，这是一种对传统观念的挑战，一种视觉艺术的革新，更是一种生活态度的象征。</span><br><span class="line">这种别样的玫瑰花，其黑色花瓣宛如漆黑夜空中闪烁的繁星，富有神秘的深度感，给人一种前所未有的视觉冲击力。这种黑色，它不是冷酷、不是绝望，而是充满着独特的魅力和力量。而位于黑色花瓣之中的金黄色花蕊，则犹如星星中的灵魂，默默闪烁，给人带来无尽的遐想，充满活力与生机。</span><br><span class="line">黑色玫瑰的存在，不仅挑战了我们对于玫瑰传统颜色的认知，它更是一种生动的生命象征，象征着那些坚韧、独特、勇敢面对生活的人们。黑色的花瓣中透露出一种坚韧的力量，而金黄的花蕊则是生活中的希望，二者的结合恰好象征了生活中的喜怒哀乐，体现了人生的百态。&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>至此，我们就通过两个 LLM 链和一个顺序链，生成了一篇完美的文案。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>LangChain 为我们提供了好用的 “链”，帮助我们把多个组件像链条一样连接起来。这个“链条” 其实就是一系列组件的调用顺序，这个顺序里还可以包括其他的“链条”。</p>
<p>我们可以使用多种方法调用链，也可以根据开发时的需求选择各种不同的链。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/21695f98ac054cc4962c790d539741cb~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1741&amp;h=1327&amp;s=246218&amp;e=png&amp;b=ffffff" alt=""></p>
<p>除去最常见的 LLMChain 和 SequenceChain 之外，LangChain 中还自带大量其他类型的链，封装了各种各样的功能。你可以看一看这些链的实现细节，并尝试着使用它们。</p>
<p>下一节课，我们会继续介绍另外一种好用的链，RouterChain。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>在<a target="_blank" rel="noopener" href="https://juejin.cn/book/7387702347436130304/section/7396583376915005480" title="https://juejin.cn/book/7387702347436130304/section/7396583376915005480">第 4 课</a>中，我们曾经用提示模板生成过一段鲜花的描述，代码如下：</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for flower, price in zip(flowers, prices):</span><br><span class="line">    # 根据提示准备模型的输入</span><br><span class="line">    input = prompt.format(flower_name=flower, price=price)</span><br><span class="line">    # 获取模型的输出</span><br><span class="line">    output = model(input)    </span><br><span class="line">    # 解析模型的输出</span><br><span class="line">    parsed_output = output_parser.parse(output)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>请你使用 LLMChain 重构提示的 format 和获取模型输出部分，完成相同的功能。</p>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    llm_chain = LLMChain(</span><br><span class="line">        llm=model,</span><br><span class="line">        prompt=prompt)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-4">
<p>上一道题目中，我要求你把提示的 format 和获取模型输出部分整合到 LLMChain 中，其实你还可以更进一步，把 output_parser 也整合到 LLMChain 中，让程序结构进一步简化，请你尝试一下。</p>
</li>
</ol>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    llm_chain = LLMChain(</span><br><span class="line">        llm=model,</span><br><span class="line">        prompt=prompt,</span><br><span class="line">        output_parser=output_parser)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="3">
<li class="lvl-4">
<p>选择一个 LangChain 中的链（我们没用到的类型），尝试使用它解决一个问题，并分享你的用例和代码。</p>
</li>
</ol>
<p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>GitHub 上各种各样的<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain%2Ftree%2Fmaster%2Flibs%2Flangchain%2Flangchain%2Fchains" title="https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/chains">链</a></p>
</li>
<li class="lvl-4">
<p>代码，<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain%2Fblob%2Fmaster%2Flibs%2Flangchain%2Flangchain%2Fchains%2Fllm.py" title="https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/llm.py">LLMChain</a> 的实现细节</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E7%94%A8%E5%B0%91%E6%A0%B7%E6%9C%ACFewShotTemplate%E5%92%8CExampleSelector%E5%88%9B%E5%BB%BA%E5%BA%94%E6%99%AF%E6%96%87%E6%A1%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E7%94%A8%E5%B0%91%E6%A0%B7%E6%9C%ACFewShotTemplate%E5%92%8CExampleSelector%E5%88%9B%E5%BB%BA%E5%BA%94%E6%99%AF%E6%96%87%E6%A1%88/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:05:52" itemprop="dateModified" datetime="2024-11-18T19:05:52+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>上节课我给你留了一个思考题：<strong>在提示模板的构建过程中加入了 partial_variables，也就是输出解析器指定的 format_instructions 之后，为什么能够让模型生成结构化的输出？</strong></p>
<p>当你用 print 语句打印出最终传递给大模型的提示时，一切就变得非常明了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">您是一位专业的鲜花店文案撰写员。</span><br><span class="line">对于售价为 50 元的 玫瑰 ，您能提供一个吸引人的简短描述吗？</span><br><span class="line">The output should be a markdown code snippet formatted in the following schema, including the leading and trailing &quot;```json&quot; and &quot;```&quot;:</span><br><span class="line"></span><br><span class="line">```json</span><br><span class="line">&#123;</span><br><span class="line">        &quot;description&quot;: string  // 鲜花的描述文案</span><br><span class="line">        &quot;reason&quot;: string  // 问什么要这样写这个文案</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>秘密在于，LangChain 的输出解析器偷偷的在提示中加了一段话，也就是 {format_instructions} 中的内容。这段由 LangChain 自动添加的文字，就清楚地指示着我们希望得到什么样的回答以及回答的具体格式。提示指出，模型需要根据一个 schema 来格式化输出文本，<code>这个 schema 从 ```json 开始，到 ``` 结束</code>。</p>
<p>这就是在告诉模型，你就 follow 这个 schema（schema，可以理解为对数据结构的描述）的格式，就行啦！</p>
<p>这就是一个很棒、很典型的<strong>提示工程</strong>。有了这样清晰的提示，智能程度比较高的模型（比如 GPT3.5 及以上版本），肯定能够输出可以被解析的数据结构，如 JSON 格式的数据。</p>
<p>那么这节课我就带着你进一步深究，如何利用 LangChain 中的提示模板，做好提示工程。</p>
<p><img src="https://static001.geekbang.org/resource/image/3b/fe/3b5584552720f22ac10e1ab1430f61fe.jpg?wh=4000x1536" alt=""></p>
<p>上节课我说过，针对大模型的提示工程该如何做，吴恩达老师在他的 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fwww.deeplearning.ai%2Fshort-courses%2Fchatgpt-prompt-engineering-for-developers%2F" title="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/">ChatGPT Prompt Engineering for Developers</a> 公开课中，给出了两个大的原则：第一条原则是写出清晰而具体的指示，第二条原则是给模型思考的时间。</p>
<p>无独有偶，在 Open AI 的官方文档 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fgpt-best-practices%2Fgpt-best-practices" title="https://platform.openai.com/docs/guides/gpt-best-practices/gpt-best-practices">GPT 最佳实践</a>中，也给出了和上面这两大原则一脉相承的 6 大策略。分别是：</p>
<ol>
<li class="lvl-4">
<p>写清晰的指示</p>
</li>
<li class="lvl-4">
<p>给模型提供参考（也就是示例）</p>
</li>
<li class="lvl-4">
<p>将复杂任务拆分成子任务</p>
</li>
<li class="lvl-4">
<p>给 GPT 时间思考</p>
</li>
<li class="lvl-4">
<p>使用外部工具</p>
</li>
<li class="lvl-4">
<p>反复迭代问题</p>
</li>
</ol>
<p>怎么样，这些原则和策略是不是都是大白话？这些原则其实不仅能够指导大语言模型，也完全能够指导你的思维过程，让你处理问题时的思路更为清晰。所以说，大模型的思维过程和我们人类的思维过程，还是蛮相通的。</p>
<h2 id="提示的结构">提示的结构</h2>
<p>当然了，从大原则到实践，还是有一些具体工作需要说明，下面我们先看一个实用的提示框架。</p>
<p><img src="https://static001.geekbang.org/resource/image/b7/16/b77a15cd83b66bba55032d711bcf3c16.png?wh=1920x801" alt=""></p>
<p>在这个提示框架中：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p><strong>指令</strong>（Instuction）告诉模型这个任务大概要做什么、怎么做，比如如何使用提供的外部信息、如何处理查询以及如何构造输出。这通常是一个提示模板中比较固定的部分。一个常见用例是告诉模型 “你是一个有用的 XX 助手”，这会让他更认真地对待自己的角色。</p>
</li>
<li class="lvl-4">
<p><strong>上下文</strong>（Context）则充当模型的额外知识来源。这些信息可以手动插入到提示中，通过矢量数据库检索得来，或通过其他方式（如调用 API、计算器等工具）拉入。一个常见的用例时是把从向量数据库查询到的知识作为上下文传递给模型。</p>
</li>
<li class="lvl-4">
<p><strong>提示输入</strong>（Prompt Input）通常就是具体的问题或者需要大模型做的具体事情，这个部分和 “指令” 部分其实也可以合二为一。但是拆分出来成为一个独立的组件，就更加结构化，便于复用模板。这通常是作为变量，在调用模型之前传递给提示模板，以形成具体的提示。</p>
</li>
<li class="lvl-4">
<p><strong>输出指示器</strong>（Output Indicator）标记​​要生成的文本的开始。这就像我们小时候的数学考卷，先写一个 “解”，就代表你要开始答题了。如果生成 Python 代码，可以使用 “import” 向模型表明它必须开始编写 Python 代码（因为大多数 Python 脚本以 import 开头）。这部分在我们和 ChatGPT 对话时往往是可有可无的，当然 LangChain 中的代理在构建提示模板时，经常性的会用一个 “Thought：”（思考）作为引导词，指示模型开始输出自己的推理（Reasoning）。</p>
</li>
</ul>
<p>下面，就让我们看看如何使用 LangChain 中的各种提示模板做提示工程，将更优质的提示输入大模型。</p>
<h2 id="LangChain-提示模板的类型">LangChain 提示模板的类型</h2>
<p>LangChain 中提供 String（StringPromptTemplate）和 Chat（BaseChatPromptTemplate）两种基本类型的模板，并基于它们构建了不同类型的提示模板：</p>
<p><img src="https://static001.geekbang.org/resource/image/fe/yy/feefbb0a166f53f14f647b88e1025cyy.jpg?wh=2240x812" alt=""></p>
<p>这些模板的导入方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from langchain.prompts.prompt import PromptTemplate</span><br><span class="line">from langchain.prompts import FewShotPromptTemplate</span><br><span class="line">from langchain.prompts.pipeline import PipelinePromptTemplate</span><br><span class="line">from langchain.prompts import ChatPromptTemplate</span><br><span class="line">from langchain.prompts import (</span><br><span class="line">    ChatMessagePromptTemplate,</span><br><span class="line">    SystemMessagePromptTemplate,</span><br><span class="line">    AIMessagePromptTemplate,</span><br><span class="line">    HumanMessagePromptTemplate,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我发现有时候不指定 .prompts，直接从 LangChain 包也能导入模板。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from langchain import PromptTemplate</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面我们通过示例来介绍上面这些模版，前两个我们简单了解就好，其中最典型的 FewShotPromptTemplate 会重点讲。至于 PipelinePrompt 和自定义模板，使用起来比较简单，请你参考 LangChain 文档自己学习。</p>
<h2 id="使用-PromptTemplate">使用 PromptTemplate</h2>
<p>下面通过示例简单说明一下 PromptTemplate 的使用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from langchain import PromptTemplate</span><br><span class="line"></span><br><span class="line">template = &quot;&quot;&quot;\</span><br><span class="line">你是业务咨询顾问。</span><br><span class="line">你给一个销售&#123;product&#125;的电商公司，起一个好的名字？</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">prompt = PromptTemplate.from_template(template)</span><br><span class="line"></span><br><span class="line">print(prompt.format(product=&quot;鲜花&quot;))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你是业务咨询顾问。</span><br><span class="line">你给一个销售鲜花的电商公司，起一个好的名字？</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个程序的主要功能是生成适用于不同场景的提示，对用户定义的一种产品或服务提供公司命名建议。</p>
<p>在这里，<code>&quot;你是业务咨询顾问。你给一个销售&#123;product&#125;的电商公司，起一个好的名字？&quot;</code> 就是原始提示模板，其中 {product} 是占位符。</p>
<p>然后通过 PromptTemplate 的 from_template 方法，我们创建了一个提示模板对象，并通过 prompt.format 方法将模板中的 {product} 替换为 <code>&quot;鲜花&quot;</code>。</p>
<p>这样，就得到了一句具体的提示：<em>你是业务咨询顾问。你给一个销售鲜花的电商公司，起一个好的名字？</em>——这就要求大语言模型，要有的放矢。</p>
<p>在上面这个过程中，LangChain 中的模板的一个方便之处是 from_template 方法可以从传入的字符串中自动提取变量名称（如 product），而无需刻意指定。<strong>上面程序中的 product 自动成为了 format 方法中的一个参数</strong>。</p>
<p>当然，也可以通过提示模板类的构造函数，在创建模板时手工指定 input_variables，示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[&quot;product&quot;, &quot;market&quot;], </span><br><span class="line">    template=&quot;你是业务咨询顾问。对于一个面向&#123;market&#125;市场的，专注于销售&#123;product&#125;的公司，你会推荐哪个名字？&quot;</span><br><span class="line">)</span><br><span class="line">print(prompt.format(product=&quot;鲜花&quot;, market=&quot;高端&quot;))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你是业务咨询顾问。对于一个面向高端市场的，专注于销售鲜花的公司，你会推荐哪个名字？</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面的方式直接生成了提示模板，并没有通过 from_template 方法从字符串模板中创建提示模板。二者效果是一样的。</p>
<h2 id="使用-ChatPromptTemplate">使用 ChatPromptTemplate</h2>
<p>对于 OpenAI 推出的 ChatGPT 这一类的聊天模型，LangChain 也提供了一系列的模板，这些模板的不同之处是它们有对应的角色。</p>
<p>下面代码展示了 OpenAI 的 Chat Model 中的各种消息角色。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import openai</span><br><span class="line">openai.ChatCompletion.create(</span><br><span class="line">  model=&quot;gpt-3.5-turbo&quot;,</span><br><span class="line">  messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;&#125;</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>OpenAI 对传输到 gpt-3.5-turbo 和 GPT-4 的 messsage 格式说明如下：</p>
<p>消息必须是消息对象的数组，其中每个对象都有一个角色（系统、用户或助理）和内容。对话可以短至一条消息，也可以来回多次。</p>
<p>通常，对话首先由系统消息格式化，然后是交替的用户消息和助理消息。</p>
<p>系统消息有助于设置助手的行为。例如，你可以修改助手的个性或提供有关其在整个对话过程中应如何表现的具体说明。但请注意，系统消息是可选的，并且没有系统消息的模型的行为可能类似于使用通用消息，例如 “你是一个有用的助手”。</p>
<p>用户消息提供助理响应的请求或评论。</p>
<p>助理消息存储以前的助理响应，但也可以由你编写以给出所需行为的示例。</p>
<p>LangChain 的 ChatPromptTemplate 这一系列的模板，就是<strong>跟着这一系列角色而设计的</strong>。</p>
<p>下面，我给出一个示例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 导入聊天消息类模板</span><br><span class="line">from langchain.prompts import (</span><br><span class="line">    ChatPromptTemplate,</span><br><span class="line">    SystemMessagePromptTemplate,</span><br><span class="line">    HumanMessagePromptTemplate,</span><br><span class="line">)</span><br><span class="line"># 模板的构建</span><br><span class="line">template=&quot;你是一位专业顾问，负责为专注于&#123;product&#125;的公司起名。&quot;</span><br><span class="line">system_message_prompt = SystemMessagePromptTemplate.from_template(template)</span><br><span class="line">human_template=&quot;公司主打产品是&#123;product_detail&#125;。&quot;</span><br><span class="line">human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)</span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])</span><br><span class="line"></span><br><span class="line"># 格式化提示消息生成提示</span><br><span class="line">prompt = prompt_template.format_prompt(product=&quot;鲜花装饰&quot;, product_detail=&quot;创新的鲜花设计。&quot;).to_messages()</span><br><span class="line"></span><br><span class="line"># 下面调用模型，把提示传入模型，生成结果</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI Key&#x27;</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">chat = ChatOpenAI()</span><br><span class="line">result = chat(prompt)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">content=&#x27;1. 花语创意\n2. 花韵设计\n3. 花艺创新\n4. 花漾装饰\n5. 花语装点\n6. 花翩翩\n7. 花语之美\n8. 花馥馥\n9. 花语时尚\n10. 花之魅力&#x27; </span><br><span class="line">additional_kwargs=&#123;&#125; </span><br><span class="line">example=False</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>好吧，尽管模型成功地完成了任务，但是感觉没有咱 “易速鲜花” 响亮！</p>
<p>讲完上面两种简单易用的提示模板，下面开始介绍今天的重点内容，FewShotPromptTemplate。FewShot，也就是少样本这一概念，是提示工程中非常重要的部分，对应着 OpenAI 提示工程指南中的第 2 条——给模型提供参考（也就是示例）。</p>
<h2 id="FewShot-的思想起源">FewShot 的思想起源</h2>
<p>讲解概念之前，我先分享个事儿哈，帮助你理解。</p>
<p>今天我下楼跑步时，一个老爷爷教孙子学骑车，小孩总掌握不了平衡，蹬一两下就下车。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>爷爷说：“宝贝，你得有毅力！”</p>
</li>
<li class="lvl-4">
<p>孙子说：“爷爷，什么是毅力？”</p>
</li>
<li class="lvl-4">
<p>爷爷说：“你看这个叔叔，绕着楼跑了 10 多圈了，这就是毅力，你也得至少蹬个 10 几趟才能骑起来。”</p>
</li>
</ul>
<p>这老爷爷就是给孙子做了一个 One-Shot 学习。如果他的孙子第一次听说却上来就明白什么是毅力，那就神了，这就叫 Zero-Shot，表明这孩子的语言天赋不是一般的高，从知识积累和当前语境中就能够推知新词的涵义。有时候我们把 Zero-Shot 翻译为 “顿悟”，聪明的大模型，某些情况下也是能够做到的。</p>
<p>Few-Shot（少样本）、One-Shot（单样本）和与之对应的 Zero-Shot（零样本）的概念都起源于机器学习。如何让机器学习模型在极少量甚至没有示例的情况下学习到新的概念或类别，对于许多现实世界的问题是非常有价值的，因为我们往往无法获取到大量的标签化数据。</p>
<p>这几个重要概念并非在某一篇特定的论文中首次提出，而是在机器学习和深度学习的研究中逐渐形成和发展的。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>对于 Few-Shot Learning，一个重要的参考文献是 2016 年 Vinyals, O. 的论文《小样本学习的匹配网络》。</p>
</li>
<li class="lvl-4">
<p>这篇论文提出了一种新的学习模型——匹配网络（Matching Networks），专门针对单样本学习（One-Shot Learning）问题设计，<strong>而</strong> <strong>One-Shot Learning</strong> <strong>可以看作是一种最常见的</strong> <strong>Few-Shot</strong> <strong>学习的情况。</strong></p>
</li>
<li class="lvl-4">
<p>对于 Zero-Shot Learning，一个代表性的参考文献是 Palatucci, M. 在 2009 年提出的《基于语义输出编码的零样本学习（Zero-Shot Learning with semantic output codes）》，这篇论文提出了零次学习（Zero-Shot Learning）的概念，其中的学习系统可以根据类的语义描述来识别之前未见过的类。</p>
</li>
</ul>
<p>在提示工程（Prompt Engineering）中，Few-Shot 和 Zero-Shot 学习的概念也被广泛应用。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>在 Few-Shot 学习设置中，模型会被给予几个示例，以帮助模型理解任务，并生成正确的响应。</p>
</li>
<li class="lvl-4">
<p>在 Zero-Shot 学习设置中，模型只根据任务的描述生成响应，不需要任何示例。</p>
</li>
</ul>
<p>而 OpenAI 在介绍 GPT-3 模型的重要论文《Language models are Few-Shot learners（语言模型是少样本学习者）》中，更是直接指出：GPT-3 模型，作为一个大型的自我监督学习模型，通过提升模型规模，实现了出色的 Few-Shot 学习性能。</p>
<p>这篇论文为大语言模型可以进行 Few-Shot 学习提供了扎实的理论基础。</p>
<p><img src="https://static001.geekbang.org/resource/image/48/bc/481yy45346cc28ec48269c752c3647bc.png?wh=659x786" alt=""></p>
<p>下图就是 OpenAI 的 GPT-3 论文给出的 GPT-3 在翻译任务中，通过 FewShot 提示完成翻译的例子。</p>
<p><img src="https://static001.geekbang.org/resource/image/35/ca/357e9ca0ce2b4699a24e3fe512c047ca.png?wh=1632x1465" alt=""></p>
<p>以上，就是 ZeroShot、OneShot、FewShot 这些重要概念的起源。</p>
<h2 id="使用-FewShotPromptTemplate">使用 FewShotPromptTemplate</h2>
<p>下面，就让我们来通过 LangChain 中的 FewShotPromptTemplate 构建出最合适的鲜花文案。</p>
<p><strong>1. 创建示例样本</strong></p>
<p>首先，创建一些示例，作为提示的样本。其中每个示例都是一个字典，其中键是输入变量，值是这些输入变量的值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 1. 创建一些示例</span><br><span class="line">samples = [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;flower_type&quot;: &quot;玫瑰&quot;,</span><br><span class="line">    &quot;occasion&quot;: &quot;爱情&quot;,</span><br><span class="line">    &quot;ad_copy&quot;: &quot;玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;flower_type&quot;: &quot;康乃馨&quot;,</span><br><span class="line">    &quot;occasion&quot;: &quot;母亲节&quot;,</span><br><span class="line">    &quot;ad_copy&quot;: &quot;康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;flower_type&quot;: &quot;百合&quot;,</span><br><span class="line">    &quot;occasion&quot;: &quot;庆祝&quot;,</span><br><span class="line">    &quot;ad_copy&quot;: &quot;百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;flower_type&quot;: &quot;向日葵&quot;,</span><br><span class="line">    &quot;occasion&quot;: &quot;鼓励&quot;,</span><br><span class="line">    &quot;ad_copy&quot;: &quot;向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>samples 这个列表，它包含了四个字典，每个字典代表了一种花的类型、适合的场合，以及对应的广告文案。 这些示例样本，就是构建 FewShotPrompt 时，作为例子传递给模型的参考信息。</p>
<p><strong>2. 创建提示模板</strong></p>
<p>配置一个提示模板，将一个示例格式化为字符串。这个格式化程序应该是一个 PromptTemplate 对象。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 2. 创建一个提示模板</span><br><span class="line">from langchain.prompts.prompt import PromptTemplate</span><br><span class="line">template=&quot;鲜花类型: &#123;flower_type&#125;\n场合: &#123;occasion&#125;\n文案: &#123;ad_copy&#125;&quot;</span><br><span class="line">prompt_sample = PromptTemplate(input_variables=[&quot;flower_type&quot;, &quot;occasion&quot;, &quot;ad_copy&quot;], </span><br><span class="line">                               template=template)</span><br><span class="line">print(prompt_sample.format(**samples[0]))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>提示模板的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">鲜花类型: 玫瑰</span><br><span class="line">场合: 爱情</span><br><span class="line">文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在这个步骤中，我们创建了一个 PromptTemplate 对象。这个对象是根据指定的输入变量和模板字符串来生成提示的。在这里，输入变量包括 <code>&quot;flower_type&quot;</code>、<code>&quot;occasion&quot;</code>、<code>&quot;ad_copy&quot;</code>，模板是一个字符串，其中包含了用大括号包围的变量名，它们会被对应的变量值替换。</p>
<p>到这里，我们就把字典中的示例格式转换成了提示模板，可以形成一个个具体可用的 LangChain 提示。比如我用 samples[0] 中的数据替换了模板中的变量，生成了一个完整的提示。</p>
<p><strong>3. 创建 FewShotPromptTemplate 对象</strong></p>
<p>然后，通过使用上一步骤中创建的 prompt_sample，以及 samples 列表中的所有示例， 创建一个 FewShotPromptTemplate 对象，生成更复杂的提示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 3. 创建一个FewShotPromptTemplate对象</span><br><span class="line">from langchain.prompts.few_shot import FewShotPromptTemplate</span><br><span class="line">prompt = FewShotPromptTemplate(</span><br><span class="line">    examples=samples,</span><br><span class="line">    example_prompt=prompt_sample,</span><br><span class="line">    suffix=&quot;鲜花类型: &#123;flower_type&#125;\n场合: &#123;occasion&#125;&quot;,</span><br><span class="line">    input_variables=[&quot;flower_type&quot;, &quot;occasion&quot;]</span><br><span class="line">)</span><br><span class="line">print(prompt.format(flower_type=&quot;野玫瑰&quot;, occasion=&quot;爱情&quot;))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">鲜花类型: 玫瑰</span><br><span class="line">场合: 爱情</span><br><span class="line">文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。</span><br><span class="line"></span><br><span class="line">鲜花类型: 康乃馨</span><br><span class="line">场合: 母亲节</span><br><span class="line">文案: 康乃馨代表着母爱的纯洁与伟大，是母亲节赠送给母亲的完美礼物。</span><br><span class="line"></span><br><span class="line">鲜花类型: 百合</span><br><span class="line">场合: 庆祝</span><br><span class="line">文案: 百合象征着纯洁与高雅，是你庆祝特殊时刻的理想选择。</span><br><span class="line"></span><br><span class="line">鲜花类型: 向日葵</span><br><span class="line">场合: 鼓励</span><br><span class="line">文案: 向日葵象征着坚韧和乐观，是你鼓励亲朋好友的最好方式。</span><br><span class="line"></span><br><span class="line">鲜花类型: 野玫瑰</span><br><span class="line">场合: 爱情</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看到，FewShotPromptTemplate 是一个更复杂的提示模板，它包含了多个示例和一个提示。这种模板可以使用多个示例来指导模型生成对应的输出。目前我们创建一个新提示，其中包含了根据指定的花的类型 “野玫瑰” 和场合“爱情”。</p>
<p><strong>4. 调用大模型创建新文案</strong></p>
<p>最后，把这个对象输出给大模型，就可以根据提示，得到我们所需要的文案了！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 4. 把提示传递给大模型</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的Open AI Key&#x27;</span><br><span class="line">from langchain.llms import OpenAI</span><br><span class="line">model = OpenAI(model_name=&#x27;gpt-3.5-turbo-instruct&#x27;)</span><br><span class="line">result = model(prompt.format(flower_type=&quot;野玫瑰&quot;, occasion=&quot;爱情&quot;))</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">文案: 野玫瑰代表着爱情的坚贞，是你向心爱的人表达爱意的最佳礼物。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>好！模型成功地模仿了我们的示例，写出了新文案，从结构到语气都蛮相似的。</p>
<h2 id="使用示例选择器">使用示例选择器</h2>
<p>如果我们的示例很多，那么一次性把所有示例发送给模型是不现实而且低效的。另外，每次都包含太多的 Token 也会浪费流量（OpenAI 是按照 Token 数来收取费用）。</p>
<p>LangChain 给我们提供了示例选择器，来选择最合适的样本。（注意，因为示例选择器使用向量相似度比较的功能，此处需要安装向量数据库，这里我使用的是开源的 Chroma，你也可以选择之前用过的 Qdrant。）</p>
<p>下面，就是使用示例选择器的示例代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 5. 使用示例选择器</span><br><span class="line">from langchain.prompts.example_selector import SemanticSimilarityExampleSelector</span><br><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line"># 初始化示例选择器</span><br><span class="line">example_selector = SemanticSimilarityExampleSelector.from_examples(</span><br><span class="line">    samples,</span><br><span class="line">    OpenAIEmbeddings(),</span><br><span class="line">    Chroma,</span><br><span class="line">    k=1</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建一个使用示例选择器的FewShotPromptTemplate对象</span><br><span class="line">prompt = FewShotPromptTemplate(</span><br><span class="line">    example_selector=example_selector, </span><br><span class="line">    example_prompt=prompt_sample, </span><br><span class="line">    suffix=&quot;鲜花类型: &#123;flower_type&#125;\n场合: &#123;occasion&#125;&quot;, </span><br><span class="line">    input_variables=[&quot;flower_type&quot;, &quot;occasion&quot;]</span><br><span class="line">)</span><br><span class="line">print(prompt.format(flower_type=&quot;红玫瑰&quot;, occasion=&quot;爱情&quot;))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">鲜花类型: 玫瑰</span><br><span class="line">场合: 爱情</span><br><span class="line">文案: 玫瑰，浪漫的象征，是你向心爱的人表达爱意的最佳选择。</span><br><span class="line"></span><br><span class="line">鲜花类型: 红玫瑰</span><br><span class="line">场合: 爱情</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在这个步骤中，它首先创建了一个 SemanticSimilarityExampleSelector 对象，这个对象可以根据语义相似性选择最相关的示例。然后，它创建了一个新的 FewShotPromptTemplate 对象，这个对象使用了上一步创建的选择器来选择最相关的示例生成提示。</p>
<p>然后，我们又用这个模板生成了一个新的提示，因为我们的提示中需要创建的是红玫瑰的文案，所以，示例选择器 example_selector 会根据语义的相似度（余弦相似度）找到最相似的示例，也就是 “玫瑰”，并用这个示例构建了 FewShot 模板。</p>
<p>这样，我们就避免了把过多的无关模板传递给大模型，以节省 Token 的用量。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>好的，到这里，今天这一讲就结束了。我们介绍了提示工程的原理，几种提示模板的用法，以及最重要的 FewShot 的思路。其实说白了，就是给模型一些示例做参考，模型才能明白你要什么。</p>
<p><img src="https://static001.geekbang.org/resource/image/f4/0d/f46817a7ed56c6fef64a6aeee4c1yy0d.png?wh=955x970" alt=""></p>
<p>总的来说，提供示例对于解决某些任务至关重要，通常情况下，FewShot 的方式能够显著提高模型回答的质量。不过，当少样本提示的效果不佳时，这可能表示模型在任务上的学习不足。在这种情况下，我们建议对模型进行微调或尝试更高级的提示技术。</p>
<p>下一节课，我们将在探讨输出解析的同时，讲解另一种备受关注的提示技术，被称为 “思维链提示”（Chain of Thought，简称 CoT）。这种技术因其独特的应用方式和潜在的实用价值而引人注目。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>如果你观察 LangChain 中的 <a target="_blank" rel="noopener" href="http://prompt.py">prompt.py</a> 中的 PromptTemplate 的实现代码，你会发现除了我们使用过的 input_variables、template 等初始化参数之外，还有 template_format、validate_template 等参数。举例来说，template_format 可以指定除了 f-string 之外，其它格式的模板，比如 jinja2。请你查看 LangChain 文档，并尝试使用这些参数。</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">template_format: str = &quot;f-string&quot;</span><br><span class="line">&quot;&quot;&quot;The format of the prompt template. Options are: &#x27;f-string&#x27;, &#x27;jinja2&#x27;.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">validate_template: bool = True</span><br><span class="line">&quot;&quot;&quot;Whether or not to try validating the template.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li class="lvl-4">
<p>请你尝试使用 PipelinePromptTemplate 和自定义 Template。</p>
</li>
<li class="lvl-4">
<p>请你构想一个关于鲜花店运营场景中客户服务对话的少样本学习任务。在这个任务中，模型需要根据提供的示例，学习如何解答客户的各种问题，包括询问花的价格、推荐鲜花、了解鲜花的保养方法等。最好是用 ChatModel 完成这个任务。</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain import PromptTemplate</span><br><span class="line">from langchain.prompts.chat import (</span><br><span class="line">    ChatPromptTemplate,</span><br><span class="line">    SystemMessagePromptTemplate,</span><br><span class="line">    AIMessagePromptTemplate,</span><br><span class="line">    HumanMessagePromptTemplate)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>论文： Open AI 的 GPT-3 模型：<a href="https://link.juejin.cn/?target=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper_files%2Fpaper%2F2020%2Fhash%2F1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">大模型是少样本学习者</a>， Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Agarwal, S. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
</li>
<li class="lvl-4">
<p>论文：<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fabs%2F1606.04080" title="https://arxiv.org/abs/1606.04080">单样本学习的匹配网络</a>，Vinyals, O., Blundell, C., Lillicrap, T., &amp; Wierstra, D. (2016). Matching networks for one shot learning. In Advances in neural information processing systems (pp. 3630-3638).</p>
</li>
<li class="lvl-4">
<p>论文：<a href="https://link.juejin.cn/?target=https%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Fpalatucci.pdf" title="https://www.cs.toronto.edu/~hinton/absps/palatucci.pdf">用语义输出编码做零样本学习</a>，Palatucci, M., Pomerleau, D., Hinton, G. E., &amp; Mitchell, T. M. (2009). Zero-shot learning with semantic output codes. In Advances in neural information processing systems (pp. 1410-1418).</p>
</li>
<li class="lvl-4">
<p>论文：<a href="https://link.juejin.cn/?target=https%3A%2F%2Fdoi.org%2F10.48550%2FarXiv.2202.12837" title="https://doi.org/10.48550/arXiv.2202.12837">对示例角色的重新思考：是什么使得上下文学习有效？</a>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022).</p>
</li>
<li class="lvl-4">
<p>论文：<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2109.01652.pdf" title="https://arxiv.org/pdf/2109.01652.pdf">微调后的语言模型是零样本学习者</a>，Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., &amp; Le, Q. V. (2022). Finetuned Language Models Are Zero-Shot Learners. Proceedings of the International Conference on Learning Representations (ICLR 2022).</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E5%B7%A5%E5%85%B7%E5%92%8C%E5%B7%A5%E5%85%B7%E7%AE%B1%EF%BC%9ALangChain%E4%B8%AD%E7%9A%84Tool%E5%92%8CToolkits%E4%B8%80%E8%A7%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E5%B7%A5%E5%85%B7%E5%92%8C%E5%B7%A5%E5%85%B7%E7%AE%B1%EF%BC%9ALangChain%E4%B8%AD%E7%9A%84Tool%E5%92%8CToolkits%E4%B8%80%E8%A7%88/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:26:50" itemprop="dateModified" datetime="2024-11-18T19:26:50+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>这节课我们来一起看一看 LangChain 中各种强大的工具（Tool），以及如何使用它们。</p>
<p>在之前的几节课中，我们深入讲解了 LangChain 中的代理。未来的 AI Agent，应该就是以 LLM 为核心控制器的代理系统。而<strong>工具，则是代理身上延展出的三头六臂，是代理的武器，代理通过工具来与世界进行交互，控制并改造世界</strong>。</p>
<h2 id="工具是代理的武器">工具是代理的武器</h2>
<p>LangChain 之所以强大，第一是大模型的推理能力强大，第二则是工具的执行能力强大！孙猴子法力再强，没有金箍棒，也降伏不了妖怪。大模型再能思考，没有工具也不行。</p>
<p>工具是代理可以用来与世界交互的功能。这些工具可以是通用实用程序（例如搜索），也可以是其他链，甚至其他的代理。</p>
<p>那么到底什么是工具？在 LangChain 中，工具是如何发挥作用的？</p>
<p>LangChain 通过提供一个统一的框架来集成功能的具体实现。在这个框架中，每个功能都被封装成一个工具。每个工具都有自己的输入和输出，以及处理这些输入和生成输出的方法。</p>
<p>当代理接收到一个任务时，它会根据任务的类型和需求，通过大模型的推理，来选择合适的工具处理这个任务。这个选择过程可以基于各种策略，例如基于工具的性能，或者基于工具处理特定类型任务的能力。</p>
<p>一旦选择了合适的工具，LangChain 就会将任务的输入传递给这个工具，然后工具会处理这些输入并生成输出。这个输出又经过大模型的推理，可以被用作其他工具的输入，或者作为最终结果，被返回给用户。</p>
<p><img src="https://static001.geekbang.org/resource/image/eb/b6/ebcyyaccd79133c03f417c45c225d1b6.png?wh=1456x636" alt=""></p>
<p>通过这种方式，LangChain 大大延展了大模型的功能。大模型的推理，加上工具的调用，都集成在一个系统中，而这个系统可以处理多种类型的任务。这提高了系统的灵活性和可扩展性，也大大简化了开发者的工作。</p>
<h2 id="如何加载工具">如何加载工具</h2>
<p>在程序中，可以使用以下代码片段加载工具。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from langchain.agents import load_tools</span><br><span class="line">tool_names = [...]</span><br><span class="line">tools = load_tools(tool_names)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>某些工具（例如链、代理）可能需要 LLM 来初始化它们。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from langchain.agents import load_tools</span><br><span class="line">tool_names = [...]</span><br><span class="line">llm = ...</span><br><span class="line">tools = load_tools(tool_names, llm=llm)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="LangChain-支持的工具一览">LangChain 支持的工具一览</h2>
<p>下面，我给你列出目前 LangChain 中所支持的工具。</p>
<p><img src="https://static001.geekbang.org/resource/image/e2/5b/e2f8a0318b4f1da7f0e756e87761d95b.jpg?wh=1459x2148" alt=""></p>
<p>当然这个列表随着时间的推移会越来越长，也就意味着 LangChain 的功能会越来越强大。</p>
<h2 id="使用-arXiv-工具开发科研助理">使用 arXiv 工具开发科研助理</h2>
<p>其中有一些工具，比如 SerpAPI，你已经用过了，这里我们再来用一下 arXiv 工具。arXiv 本身就是一个论文研究的利器，里面的论文数量比 AI 顶会还早、还多、还全。那么把它以工具的形式集成到 LangChain 中，能让你在研究学术最新进展时如虎添翼。</p>
<p>arXiv 是一个提供免费访问的预印本库，供研究者在正式出版前上传和分享其研究工作。它成立于 1991 年，最初是作为物理学预印本数据库开始的，但后来扩展到了数学、计算机科学、生物学、经济学等多个领域。</p>
<p>预印本是研究者完成的、但尚未经过同行评议或正式出版的论文。Arxiv 允许研究者上传这些预印本，使其他研究者可以在正式出版之前查看、评论和使用这些工作。这样，研究的发现可以更快地传播和分享，促进学术交流。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API的密钥</span><br><span class="line">import os </span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;Your Key&#x27; </span><br><span class="line"></span><br><span class="line"># 导入库</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.agents import load_tools, initialize_agent, AgentType</span><br><span class="line"></span><br><span class="line"># 初始化模型和工具</span><br><span class="line">llm = ChatOpenAI(temperature=0.0)</span><br><span class="line">tools = load_tools(</span><br><span class="line">    [&quot;arxiv&quot;],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 初始化链</span><br><span class="line">agent_chain = initialize_agent(</span><br><span class="line">    tools,</span><br><span class="line">    llm,</span><br><span class="line">    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,</span><br><span class="line">    verbose=True,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 运行链</span><br><span class="line">agent_chain.run(&quot;介绍一下2005.14165这篇论文的创新点?&quot;)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>首先，我们还是来研究一下 ZERO_SHOT_REACT_DESCRIPTION 这个 Agent 是怎么通过提示来引导模型调用工具的。</p>
<p>“prompts”: [</p>
<p>&quot;Answer the following questions as best you can. You have access to the following tools:\n\n</p>
<p>首先告诉模型，要尽力回答问题，但是可以访问下面的工具。</p>
<p><strong>arxiv:</strong> A wrapper around <a target="_blank" rel="noopener" href="http://Arxiv.org">Arxiv.org</a> Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on <a target="_blank" rel="noopener" href="http://arxiv.org">arxiv.org</a>. Input should be a search query.\n\n</p>
<p>arxiv 工具：一个围绕 <a target="_blank" rel="noopener" href="http://Arxiv.org">Arxiv.org</a> 的封装工具。当你需要回答关于物理学、数学、计算机科学、定量生物学、定量金融、统计学、电气工程和经济学的问题时，来自 <a target="_blank" rel="noopener" href="http://arxiv.org">arxiv.org</a> 上的科学文章非常有用。同时还告诉模型：输入这个工具的内容应该是搜索查询。</p>
<p>Use the following format:\n\n</p>
<p>指导模型输出下面的内容。</p>
<p>Question: the input question you must answer\n （问题：需要回答的问题）</p>
<p>Thought: you should always think about what to do\n （思考：应该总是思考下一步做什么）</p>
<p>Action: the action to take, should be one of [arxiv]\n （行动：从具体工具列表中选择行动——这里只有 arxiv 一个工具）</p>
<p>Action Input: the input to the action\n （行动的输入：输入工具的内容）</p>
<p>Observation: the result of the action\n… （观察：工具返回的结果）</p>
<p>(this Thought/Action/Action Input/Observation can repeat N times)\n （上面 Thought/Action/Action Input/Observation 的过程将重复 N 次）</p>
<p>Thought: I now know the final answer\n （现在我知道最终答案了）</p>
<p>Final Answer: the final answer to the original input question\n\n （原始问题的最终答案）</p>
<p><strong>Begin!</strong>\n\n</p>
<p>现在开始！</p>
<p><strong>Question</strong>: 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models’这篇论文的创新点 \ n</p>
<p>真正的问题在此。</p>
<p>Thought:&quot;</p>
<p>开始思考吧！</p>
<p>然后，我们来看看 Chain 的运行过程。</p>
<p><img src="https://static001.geekbang.org/resource/image/6e/57/6e1195d608d47fbe5b67131c1fe32357.jpg?wh=1041x1519" alt=""></p>
<p>其中，代理的思考过程中的第一个返回结果如下：</p>
<p>“text”: &quot; I need to read the paper to understand the innovation\n （思考：我需要阅读文章才能理解创新点）</p>
<p>Action: arxiv\n （行动：arxiv 工具）</p>
<p>Action Input: ‘Chain-of-Thought Prompting Elicits Reasoning in Large Language Models’&quot;, （行动的输入：论文的标题）</p>
<p>因为在之前的提示中，LangChain 告诉大模型，对于 Arxiv 工具的输入总是以搜索的形式出现，因此尽管我指明了论文的 ID，Arxiv 还是根据这篇论文的关键词搜索到了 3 篇相关论文的信息。</p>
<p>模型对这些信息进行了总结，认为信息已经完善，并给出了最终答案。</p>
<p>Thought: I now know the final answer</p>
<p>想法：我现在知道了最终答案。</p>
<p>Final Answer: The innovation of the paper ‘Chain-of-Thought Prompting Elicits Reasoning in Large Language Models’ is the introduction of a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting, which significantly improves the ability of large language models to perform complex reasoning.&quot;</p>
<p>最终答案：这篇名为《链式思考提示促使大型语言模型进行推理》的论文的创新之处在于，引入了一种简单的方法，即链式思考提示，在提示中提供了一些链式思考的示例，这大大提高了大型语言模型执行复杂推理的能力。</p>
<h2 id="LangChain-中的工具箱一览">LangChain 中的工具箱一览</h2>
<p>下面，我给你列出了目前 LangChain 中所支持的工具箱。每个工具箱中都有一系列工具。</p>
<p><img src="https://static001.geekbang.org/resource/image/c8/27/c87be0638409b278c2657a66f45aa927.jpg?wh=1223x1314" alt=""></p>
<h2 id="使用-Gmail-工具箱开发个人助理">使用 Gmail 工具箱开发个人助理</h2>
<p>刚才，你使用了 arXiv 工具帮助你做了一些科研工作。你当然还希望你的 AI Agent 能够成为你的全能自动助理，你开发出的智能应用应该能帮你检查邮件、写草稿，甚至发邮件、写文档，对吧？</p>
<p>上面这一切的一切，LangChain 当然能够安排上！</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>通过 Gmail 工具箱，你可以通过 LangChain 应用检查邮件、删除垃圾邮件，甚至让它帮你撰写邮件草稿。</p>
</li>
<li class="lvl-4">
<p>通过 Office365 工具箱，你可以让 LangChain 应用帮你读写文档、总结文档，甚至做 PPT。</p>
</li>
<li class="lvl-4">
<p>通过 GitHub 工具箱，你可以指示 LangChain 应用来检查最新的代码，Commit Changes、Merge Branches，甚至尝试让大模型自动回答 Issues 中的问题——反正大模型解决代码问题的能力本来就更强。</p>
</li>
</ul>
<p>这些都不再是梦想。</p>
<p>下面咱们从一个最简单的应用开始。</p>
<p><strong>目标：我要让 AI 应用来访问我的 Gmail 邮件，让他每天早晨检查一次我的邮箱，看看 “易速鲜花” 的客服有没有给我发信息。</strong>（因为我可能正在焦急地等待他们的退款?）</p>
<p>现在开始。</p>
<h3 id="第一步：在-Google-Cloud-中设置你的应用程序接口">第一步：在 Google Cloud 中设置你的应用程序接口</h3>
<p>这个步骤你要跟着 Gmail API 的官方配置<a href="https://link.juejin.cn/?target=https%3A%2F%2Fdevelopers.google.com%2Fgmail%2Fapi%2Fquickstart%2Fpython%3Fhl%3Dzh-cn%23authorize_credentials_for_a_desktop_application" title="https://developers.google.com/gmail/api/quickstart/python?hl=zh-cn#authorize_credentials_for_a_desktop_application">链接</a>完成，这个和 LangChain 无关。蛮复杂的，你需要有点耐心。跟着流程一步步配置就好了。</p>
<p>下面是我在这个设置过程中截取的一部分图片，只是供你参考。详细配置你要 follow Google 的官方说明。</p>
<p><img src="https://static001.geekbang.org/resource/image/8a/21/8a3c72f48c231bd2d886b4d99e9f3321.jpg?wh=1170x854" alt=""></p>
<p><img src="https://static001.geekbang.org/resource/image/38/ab/3822d1effb90c855c133acdecea2eaab.jpg?wh=1930x711" alt=""></p>
<p><img src="https://static001.geekbang.org/resource/image/96/f3/96a788e8a1f7d4f32e3d23eb94cce8f3.jpg?wh=1688x1172" alt=""></p>
<p><img src="https://static001.geekbang.org/resource/image/0f/29/0f746cfa48ba60c0fe98e657cb3yyb29.jpg?wh=1925x810" alt=""></p>
<p>下面这个 OAuth 同意屏幕里面的配置非常重要，你的智能代理能做什么，不能做什么，就看你怎么给权限了！</p>
<p><img src="https://static001.geekbang.org/resource/image/19/2f/195ec3590bb075ecff42911f13d2f22f.jpg?wh=823x1214" alt=""></p>
<p>所有设置都完成之后，在 OAuth 客户段已创建这个页面，你拥有了开发密钥。</p>
<p><img src="https://static001.geekbang.org/resource/image/f6/b0/f6829a70c320161a1002ee3380c5b1b0.jpg?wh=506x509" alt=""></p>
<h3 id="第二步：根据密钥生成开发-Token">第二步：根据密钥生成开发 Token</h3>
<p>在这一步之前，你可能需要安装一些相关的包。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade google-api-python-client </span><br><span class="line">pip install --upgrade google-auth-oauthlib</span><br><span class="line">pip install --upgrade google-auth-httplib2</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>然后，把密钥下载下来，保存为 credentials.json。</p>
<p>运行下面的代码，生成 token.json。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line">import os.path</span><br><span class="line"></span><br><span class="line">from google.auth.transport.requests import Request</span><br><span class="line">from google.oauth2.credentials import Credentials</span><br><span class="line">from google_auth_oauthlib.flow import InstalledAppFlow</span><br><span class="line">from googleapiclient.discovery import build</span><br><span class="line">from googleapiclient.errors import HttpError</span><br><span class="line"></span><br><span class="line"># If modifying these scopes, delete the file token.json.</span><br><span class="line">SCOPES = [&#x27;https://www.googleapis.com/auth/gmail.readonly&#x27;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    &quot;&quot;&quot;Shows basic usage of the Gmail API.</span><br><span class="line">    Lists the user&#x27;s Gmail labels.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    creds = None</span><br><span class="line">    # The file token.json stores the user&#x27;s access and refresh tokens, and is</span><br><span class="line">    # created automatically when the authorization flow completes for the first</span><br><span class="line">    # time.</span><br><span class="line">    if os.path.exists(&#x27;token.json&#x27;):</span><br><span class="line">        creds = Credentials.from_authorized_user_file(&#x27;token.json&#x27;, SCOPES)</span><br><span class="line">    # If there are no (valid) credentials available, let the user log in.</span><br><span class="line">    if not creds or not creds.valid:</span><br><span class="line">        if creds and creds.expired and creds.refresh_token:</span><br><span class="line">            creds.refresh(Request())</span><br><span class="line">        else:</span><br><span class="line">            flow = InstalledAppFlow.from_client_secrets_file(</span><br><span class="line">                &#x27;credentials.json&#x27;, SCOPES)</span><br><span class="line">            creds = flow.run_local_server(port=8088)</span><br><span class="line">        # Save the credentials for the next run</span><br><span class="line">        with open(&#x27;token.json&#x27;, &#x27;w&#x27;) as token:</span><br><span class="line">            token.write(creds.to_json())</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        # Call the Gmail API</span><br><span class="line">        service = build(&#x27;gmail&#x27;, &#x27;v1&#x27;, credentials=creds)</span><br><span class="line">        results = service.users().labels().list(userId=&#x27;me&#x27;).execute()</span><br><span class="line">        labels = results.get(&#x27;labels&#x27;, [])</span><br><span class="line"></span><br><span class="line">        if not labels:</span><br><span class="line">            print(&#x27;No labels found.&#x27;)</span><br><span class="line">            return</span><br><span class="line">        print(&#x27;Labels:&#x27;)</span><br><span class="line">        for label in labels:</span><br><span class="line">            print(label[&#x27;name&#x27;])</span><br><span class="line"></span><br><span class="line">    except HttpError as error:</span><br><span class="line">        # TODO(developer) - Handle errors from gmail API.</span><br><span class="line">        print(f&#x27;An error occurred: &#123;error&#125;&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这是 Google API 网站提供的标准示例代码，里面给了读取权限（gmail.readonly）的 Token，如果你要编写邮件，甚至发送邮件，需要根据需求来调整权限。更多细节可以参阅 Google API 的<a href="https://link.juejin.cn/?target=https%3A%2F%2Fcloud.google.com%2Fcompute%2Fdocs%2Fapis%3Fhl%3Dzh-cn" title="https://cloud.google.com/compute/docs/apis?hl=zh-cn">文档</a>。</p>
<p>这个程序会生成一个 token.json 文件，是有相关权限的开发令牌。这个文件在 LangChain 应用中需要和密钥一起使用。</p>
<p><img src="https://static001.geekbang.org/resource/image/54/78/541c541b377063b49d74ddc53f41d578.jpg?wh=304x55" alt=""></p>
<p>把密钥和 Token 文件都放在程序的同一个目录中，你就可以开始开发应用程序了。</p>
<p><img src="https://static001.geekbang.org/resource/image/f2/b4/f23144b35b44fef8d900d0d50c9da6b4.jpg?wh=147x52" alt=""></p>
<h3 id="第三步：用-LangChain-框架开发-Gmail-App">第三步：用 LangChain 框架开发 Gmail App</h3>
<p>这段代码的核心目的是连接到 Gmail API，查询用户的邮件，并通过 LangChain 的 Agent 框架智能化地调用 API（用语言而不是具体 API），与邮件进行互动。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API的密钥</span><br><span class="line">import os </span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;Your Key&#x27; </span><br><span class="line"></span><br><span class="line"># 导入与Gmail交互所需的工具包</span><br><span class="line">from langchain.agents.agent_toolkits import GmailToolkit</span><br><span class="line"></span><br><span class="line"># 初始化Gmail工具包</span><br><span class="line">toolkit = GmailToolkit()</span><br><span class="line"></span><br><span class="line"># 从gmail工具中导入一些有用的功能</span><br><span class="line">from langchain.tools.gmail.utils import build_resource_service, get_gmail_credentials</span><br><span class="line"></span><br><span class="line"># 获取Gmail API的凭证，并指定相关的权限范围</span><br><span class="line">credentials = get_gmail_credentials(</span><br><span class="line">    token_file=&quot;token.json&quot;,  # Token文件路径</span><br><span class="line">    scopes=[&quot;https://mail.google.com/&quot;],  # 具有完全的邮件访问权限</span><br><span class="line">    client_secrets_file=&quot;credentials.json&quot;,  # 客户端的秘密文件路径</span><br><span class="line">)</span><br><span class="line"># 使用凭证构建API资源服务</span><br><span class="line">api_resource = build_resource_service(credentials=credentials)</span><br><span class="line">toolkit = GmailToolkit(api_resource=api_resource)</span><br><span class="line"></span><br><span class="line"># 获取工具</span><br><span class="line">tools = toolkit.get_tools()</span><br><span class="line">print(tools)</span><br><span class="line"></span><br><span class="line"># 导入与聊天模型相关的包</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.agents import initialize_agent, AgentType</span><br><span class="line"></span><br><span class="line"># 初始化聊天模型</span><br><span class="line">llm = ChatOpenAI(temperature=0, model=&#x27;gpt-4&#x27;)</span><br><span class="line"></span><br><span class="line"># 通过指定的工具和聊天模型初始化agent</span><br><span class="line">agent = initialize_agent(</span><br><span class="line">    tools=toolkit.get_tools(),</span><br><span class="line">    llm=llm,</span><br><span class="line">    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 使用agent运行一些查询或指令</span><br><span class="line">result = agent.run(</span><br><span class="line">    &quot;今天易速鲜花客服给我发邮件了么？最新的邮件是谁发给我的？&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 打印结果</span><br><span class="line">print(result)  </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>代码的核心部分主要是连接到 Gmail API，获取用户的邮件数据，并通过特定的 Agent 查询这些数据。</p>
<p>你的请求是查询今天是否收到了来自 “易速鲜花客服” 的邮件，以及最新邮件的发送者是谁。<strong>这个请求是模糊的，是自然语言格式，具体调用什么 API，由 Agent、Tool 也就是 Gmail API 它俩商量着来。</strong> 这与我们之前所进行的清晰的、具体 API 调用式的应用开发迥然不同。</p>
<p>第一次运行程序，会进行一些确认，并让我 Login 我的 Gmail。</p>
<p><img src="https://static001.geekbang.org/resource/image/0e/37/0e2a7df295caa50512552e05ea3def37.jpg?wh=562x146" alt=""></p>
<p><img src="https://static001.geekbang.org/resource/image/32/51/3208ff117674ebf3f08eac6118393e51.jpg?wh=488x664" alt=""></p>
<p><img src="https://static001.geekbang.org/resource/image/0c/30/0cc81560c4bc412104b5144a474c5530.jpg?wh=546x119" alt=""></p>
<p>之后，我就得到了智能助手的回答！</p>
<p><img src="https://static001.geekbang.org/resource/image/45/cf/455f8cb0138cd3860869e5eee74f8ecf.jpg?wh=1296x605" alt=""></p>
<p>她说：<strong>主人，看起来你没有收到 “易速鲜花” 的邮件耶，还需要我帮你做些什么吗？</strong> 真的很贴心，这样的话，我每天早晨就不需要自己去检查邮件啦！</p>
<p>后来，我又问她，那么谁给我发来了新邮件呢？</p>
<p><img src="https://static001.geekbang.org/resource/image/c9/e4/c95a8e75cdc78a7da4960c8f2yyf8be4.jpg?wh=1291x159" alt=""></p>
<p>她告诉我说，Medium - Programing 给我发了一篇 VS code 的 10 个 tips 的文章，还有 Kubernetes 的点子啥的。</p>
<p>嗯，这是我订阅的内容。下一步，我还可以让她针对这些内容给我总结总结！这也是她的强项！</p>
<h2 id="总结时刻">总结时刻</h2>
<p>学到现在，你应该对 LangChain 的核心价值有了更深的感悟吧。它的价值，在于它将模型运行和交互的复杂性进行了封装和抽象化，为开发者提供了一个更简单、更直观的接口来利用大模型。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p><strong>集成多模型和多策略：</strong> LangChain 提供了一种方法，使得多个模型或策略能够在一个统一的框架下工作。例如，arXiv 是一个单独的工具，它负责处理特定的任务。这种工具可以与其他工具（例如用于处理自然语言查询或者数据库查询的工具）一起作为一个集成的系统存在。这样，你可以轻松地创建一个系统，该系统可以处理多种类型的输入并执行多种任务，而不必为每个任务单独写代码。</p>
</li>
<li class="lvl-4">
<p><strong>更易于交互和维护：</strong> 通过 LangChain，你可以更方便地管理和维护你的工具和模型。LangChain 提供的工具和代理（Agent）抽象使得开发者可以将关注点从底层实现细节转向实现应用的高层逻辑。而且，LangChain 封装了像模型的加载、输入输出的处理、工具的调度等底层任务，使得开发者能够更专注于如何组合这些工具以解决实际问题。</p>
</li>
<li class="lvl-4">
<p><strong>适应性：</strong> LangChain 提供的架构允许你轻松地添加新的工具或模型，或者替换现有的工具或模型。这种灵活性使得你的系统可以很容易地适应新的需求或改变。</p>
</li>
<li class="lvl-4">
<p><strong>可解释性：</strong> LangChain 还提供了对模型决策的可解释性。在你的示例中，LangChain 提供的对话历史和工具选择的记录可以帮助理解系统做出某些决策的原因。</p>
</li>
</ul>
<p>总的来说，尽管直接调用模型可能对于单一任务或简单应用来说足够了，但是当你需要处理更复杂的场景，例如需要协调多个模型或工具，或者需要处理多种类型的输入时，使用像 LangChain 这样的框架可以大大简化你的工作。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>上面 Gmail 的示例中我只是展示了邮件读取功能，你能否让你的 AI 助理帮你写邮件的草稿甚至发送邮件？</p>
</li>
<li class="lvl-4">
<p>你可否尝试使用 GitHub 工具开发一些 App 来自动完成一部分 GitHub 任务，比如查看 Issues、Merge Branches 之类的事儿。</p>
</li>
</ol>
<p>提示：参考此<a href="https://link.juejin.cn/?target=https%3A%2F%2Fdocs.github.com%2Fen%2Fapps%2Fcreating-github-apps%2Fregistering-a-github-app%2Fregistering-a-github-app" title="https://docs.github.com/en/apps/creating-github-apps/registering-a-github-app/registering-a-github-app">链接</a>创建 GitHub App，以及 LangChain 的<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fintegrations%2Ftoolkits%2Fgithub" title="https://python.langchain.com/docs/integrations/toolkits/github">参考文档</a>。</p>
<p><img src="https://static001.geekbang.org/resource/image/1b/2e/1bc0dcd6e05133f934ed926cdcc9eb2e.jpg?wh=1898x851" alt=""></p>
<p><img src="https://static001.geekbang.org/resource/image/e0/ea/e037cf6460826e189811ea2af4bb96ea.jpg?wh=1281x747" alt=""></p>
<p>期待在留言区看到你的分享，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>文档：LangChain 中集成的所有<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fintegrations%2Ftools%2F" title="https://python.langchain.com/docs/integrations/tools/">工具</a></p>
</li>
<li class="lvl-4">
<p>文档：LangChain 中集成的所有<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fintegrations%2Ftoolkits%2F" title="https://python.langchain.com/docs/integrations/toolkits/">工具箱</a></p>
</li>
<li class="lvl-4">
<p>文档：Google Cloud <a href="https://link.juejin.cn/?target=https%3A%2F%2Fcloud.google.com%2Fcompute%2Fdocs%2Fapis%3Fhl%3Dzh-cn" title="https://cloud.google.com/compute/docs/apis?hl=zh-cn">API</a></p>
</li>
<li class="lvl-4">
<p>文档：Github REST <a href="https://link.juejin.cn/?target=https%3A%2F%2Fsupport.github.com%2Ffeatures%2Frest-api" title="https://support.github.com/features/rest-api">API</a></p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%EF%BC%9A%E9%80%9A%E8%BF%87RAG%E5%8A%A9%E5%8A%9B%E9%B2%9C%E8%8A%B1%E8%BF%90%E8%90%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%EF%BC%9A%E9%80%9A%E8%BF%87RAG%E5%8A%A9%E5%8A%9B%E9%B2%9C%E8%8A%B1%E8%BF%90%E8%90%A5/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:27:15" itemprop="dateModified" datetime="2024-11-18T19:27:15+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>在<a target="_blank" rel="noopener" href="https://juejin.cn/book/7387702347436130304/section/7388069933021986856" title="https://juejin.cn/book/7387702347436130304/section/7388069933021986856">第 3 课</a>中，我曾经带着你完成了一个基于本地文档的问答系统。用当下时髦的话说，你实现了一个 RAG 应用。</p>
<p>什么是 RAG？其全称为 Retrieval-Augmented Generation，即检索增强生成，它结合了检索和生成的能力，为文本序列生成任务引入外部知识。RAG 将传统的语言生成模型与大规模的外部知识库相结合，使模型在生成响应或文本时可以动态地从这些知识库中检索相关信息。这种结合方法旨在增强模型的生成能力，使其能够产生更为丰富、准确和有根据的内容，特别是在需要具体细节或外部事实支持的场合。</p>
<p>RAG 的工作原理可以概括为几个步骤。</p>
<ol>
<li class="lvl-4">
<p><strong>检索</strong>：对于给定的输入（问题），模型首先使用检索系统从大型文档集合中查找相关的文档或段落。这个检索系统通常基于密集向量搜索，例如 ChromaDB、Faiss 这样的向量数据库。</p>
</li>
<li class="lvl-4">
<p><strong>上下文编码</strong>：找到相关的文档或段落后，模型将它们与原始输入（问题）一起编码。</p>
</li>
<li class="lvl-4">
<p><strong>生成</strong>：使用编码的上下文信息，模型生成输出（答案）。这通常当然是通过大模型完成的。</p>
</li>
</ol>
<p>RAG 的一个关键特点是，它不仅仅依赖于训练数据中的信息，还可以从大型外部知识库中检索信息。这使得 RAG 模型特别适合处理在训练数据中未出现的问题。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/09158400263a4e488146834041d14c3e~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=4256&amp;h=1472&amp;s=799035&amp;e=jpg&amp;b=e3fbea" alt=""></p>
<p>RAG 类的任务，目前企业实际应用场景中的需求量相当大，也是 LangChain 所关注的一个重点内容。在这节课中，我会对 LangChain 中所有与之相关的工具进行一个梳理，便于你把握 LangChain 在这个领域中都能够做到些什么。</p>
<h2 id="文档加载">文档加载</h2>
<p>RAG 的第一步是文档加载。LangChain 提供了多种类型的文档加载器，以加载各种类型的文档（HTML、PDF、代码），并与该领域的其他主要提供商如 Airbyte 和 <a target="_blank" rel="noopener" href="http://Unstructured.IO">Unstructured.IO</a> 进行了集成。</p>
<p>下面给出常用的文档加载器列表。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6726e90bd5304b189b5570e90c8a9cc6~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1584&amp;h=656&amp;s=374975&amp;e=jpg&amp;b=fafbfe" alt=""></p>
<h2 id="文本转换">文本转换</h2>
<p>加载文档后，下一个步骤是对文本进行转换，而最常见的文本转换就是把长文档分割成更小的块（或者是片，或者是节点），以适合模型的上下文窗口。LangChain 有许多内置的文档转换器，可以轻松地拆分、组合、过滤和以其他方式操作文档。</p>
<h3 id="文本分割器">文本分割器</h3>
<p>把长文本分割成块听起来很简单，其实也存在一些细节。文本分割的质量会影响检索的结果质量。理想情况下，我们希望将语义相关的文本片段保留在一起。</p>
<p>LangChain 中，文本分割器的工作原理如下：</p>
<ol>
<li class="lvl-4">
<p>将文本分成小的、具有语义意义的块（通常是句子）。</p>
</li>
<li class="lvl-4">
<p>开始将这些小块组合成一个更大的块，直到达到一定的大小。</p>
</li>
<li class="lvl-4">
<p>一旦达到该大小，一个块就形成了，可以开始创建新文本块。这个新文本块和刚刚生成的块要有一些重叠，以保持块之间的上下文。</p>
</li>
</ol>
<p>因此，LangChain 提供的各种文本拆分器可以帮助你从下面几个角度设定你的分割策略和参数：</p>
<ol>
<li class="lvl-4">
<p>文本如何分割</p>
</li>
<li class="lvl-4">
<p>块的大小</p>
</li>
<li class="lvl-4">
<p>块之间重叠文本的长度</p>
</li>
</ol>
<p>这些文本分割器的说明和示例如下：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fce9f88936a64a0087ef495f4909fa40~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=2020&amp;h=887&amp;s=511615&amp;e=jpg&amp;b=f9fafd" alt=""></p>
<p>你可能会关心，文本分割在实践，有哪些具体的考量因素，我总结了下面几点。</p>
<p><strong>首先，就是 LLM 的具体限制</strong>。GPT-3.5-turbo 支持的上下文窗口为 4096 个令牌，这意味着输入令牌和生成的输出令牌的总和不能超过 4096，否则会出错。为了保证不超过这个限制，我们可以预留约 2000 个令牌作为输入提示，留下约 2000 个令牌作为返回的消息。这样，如果你提取出了五个相关信息块，那么每个片的大小不应超过 400 个令牌。</p>
<p><strong>此外，文本分割策略的选择和任务类型相关。</strong></p>
<ul class="lvl-0">
<li class="lvl-4">
<p>需要细致查看文本的任务，最好使用较小的分块。例如，拼写检查、语法检查和文本分析可能需要识别文本中的单个单词或字符。垃圾邮件识别、查找剽窃和情感分析类任务，以及搜索引擎优化、主题建模中常用的关键字提取任务也属于这类细致任务。</p>
</li>
<li class="lvl-4">
<p>需要全面了解文本的任务，则使用较大的分块。例如，机器翻译、文本摘要和问答任务需要理解文本的整体含义。而自然语言推理、问答和机器翻译需要识别文本中不同部分之间的关系。还有创意写作，都属于这种粗放型的任务。</p>
</li>
</ul>
<p><strong>最后，你也要考虑所分割的文本的性质</strong>。例如，如果文本结构很强，如代码或 HTML，你可能想使用较大的块，如果文本结构较弱，如小说或新闻文章，你可能想使用较小的块。</p>
<p>你可以反复试验不同大小的块和块与块之间重叠窗口的大小，找到最适合你特定问题的解决方案。</p>
<h3 id="其他形式的文本转换">其他形式的文本转换</h3>
<p>除拆分文本之外，LangChain 中还集成了各种工具对文档执行的其他类型的转换。下面让我们对其进行逐点分析。</p>
<ol>
<li class="lvl-4">
<p>过滤冗余的文档：使用 EmbeddingsRedundantFilter 工具可以识别相似的文档并过滤掉冗余信息。这意味着如果你有多份高度相似或几乎相同的文档，这个功能可以帮助识别并删除这些多余的副本，从而节省存储空间并提高检索效率。</p>
</li>
<li class="lvl-4">
<p>翻译文档：通过与工具 doctran 进行集成，可以将文档从一种语言翻译成另一种语言。</p>
</li>
<li class="lvl-4">
<p>提取元数据：通过与工具 doctran 进行集成，可以从文档内容中提取关键信息（如日期、作者、关键字等），并将其存储为元数据。元数据是描述文档属性或内容的数据，这有助于更有效地管理、分类和检索文档。</p>
</li>
<li class="lvl-4">
<p>转换对话格式：通过与工具 doctran 进行集成，可以将对话式的文档内容转化为问答（Q/A）格式，从而更容易地提取和查询特定的信息或回答。这在处理如访谈、对话或其他交互式内容时非常有用。</p>
</li>
</ol>
<p>所以说，文档转换不仅限于简单的文本拆分，还可以包含附加的操作，这些操作的目的都是更好地准备和优化文档，以供后续生成更好的索引和检索功能。</p>
<h2 id="文本嵌入">文本嵌入</h2>
<p>文本块形成之后，我们就通过 LLM 来做嵌入（Embeddings），将文本转换为数值表示，使得计算机可以更容易地处理和比较文本。OpenAI、Cohere、Hugging Face 中都有能做文本嵌入的模型。</p>
<p>Embeddings 会创建一段文本的向量表示，让我们可以在向量空间中思考文本，并执行语义搜索之类的操作，在向量空间中查找最相似的文本片段。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/71d94e4b9c354d3f8f398692d8272c6d~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1505&amp;h=527&amp;s=116887&amp;e=png&amp;b=ffffff" alt=""></p>
<p>LangChain 中的 Embeddings 类是设计用于与文本嵌入模型交互的类。这个类为所有这些提供者提供标准接口。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 初始化Embedding类</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">embeddings_model = OpenAIEmbeddings()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>它提供两种方法：</p>
<ol>
<li class="lvl-4">
<p>第一种是 embed_documents 方法，为文档创建嵌入。这个方法接收多个文本作为输入，意味着你可以一次性将多个文档转换为它们的向量表示。</p>
</li>
<li class="lvl-4">
<p>第二种是 embed_query 方法，为查询创建嵌入。这个方法只接收一个文本作为输入，通常是用户的搜索查询。</p>
</li>
</ol>
<p><strong>为什么需要两种方法</strong>？虽然看起来这两种方法都是为了文本嵌入，但是 LangChain 将它们分开了。原因是一些嵌入提供者对于文档和查询使用的是不同的嵌入方法。文档是要被搜索的内容，而查询是实际的搜索请求。这两者可能因为其性质和目的，而需要不同的处理或优化。</p>
<p>embed_documents 方法的示例代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embeddings = embeddings_model.embed_documents(</span><br><span class="line">    [</span><br><span class="line">        &quot;您好，有什么需要帮忙的吗？&quot;,</span><br><span class="line">        &quot;哦，你好！昨天我订的花几天送达&quot;,</span><br><span class="line">        &quot;请您提供一些订单号？&quot;,</span><br><span class="line">        &quot;12345678&quot;,</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">len(embeddings), len(embeddings[0])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(4, 1536)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>embed_query 方法的示例代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">embedded_query = embeddings_model.embed_query(&quot;刚才对话中的订单号是多少?&quot;)</span><br><span class="line">embedded_query[:3]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[-0.0029746221837547455, -0.007710168602107487, 0.00923260021751183]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="存储嵌入">存储嵌入</h2>
<p>计算嵌入可能是一个时间消耗大的过程。为了加速这一过程，我们可以将计算出的嵌入存储或临时缓存，这样在下次需要它们时，就可以直接读取，无需重新计算。</p>
<h3 id="缓存存储">缓存存储</h3>
<p>CacheBackedEmbeddings 是一个支持缓存的嵌入式包装器，它可以将嵌入缓存在键值存储中。具体操作是：对文本进行哈希处理，并将此哈希值用作缓存的键。</p>
<p>要初始化一个 CacheBackedEmbeddings，主要的方式是使用 from_bytes_store。其需要以下参数：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>underlying_embedder：实际计算嵌入的嵌入器。</p>
</li>
<li class="lvl-4">
<p>document_embedding_cache：用于存储文档嵌入的缓存。</p>
</li>
<li class="lvl-4">
<p>namespace（可选）：用于文档缓存的命名空间，避免与其他缓存发生冲突。</p>
</li>
</ul>
<p><strong>不同的缓存策略如下：</strong></p>
<ol>
<li class="lvl-4">
<p>InMemoryStore：在内存中缓存嵌入。主要用于单元测试或原型设计。如果需要长期存储嵌入，请勿使用此缓存。</p>
</li>
<li class="lvl-4">
<p>LocalFileStore：在本地文件系统中存储嵌入。适用于那些不想依赖外部数据库或存储解决方案的情况。</p>
</li>
<li class="lvl-4">
<p>RedisStore：在 Redis 数据库中缓存嵌入。当需要一个高速且可扩展的缓存解决方案时，这是一个很好的选择。</p>
</li>
</ol>
<p>在内存中缓存嵌入的示例代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 导入内存存储库，该库允许我们在RAM中临时存储数据</span><br><span class="line">from langchain.storage import InMemoryStore</span><br><span class="line"></span><br><span class="line"># 创建一个InMemoryStore的实例</span><br><span class="line">store = InMemoryStore()</span><br><span class="line"></span><br><span class="line"># 导入与嵌入相关的库。OpenAIEmbeddings是用于生成嵌入的工具，而CacheBackedEmbeddings允许我们缓存这些嵌入</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings</span><br><span class="line"></span><br><span class="line"># 创建一个OpenAIEmbeddings的实例，这将用于实际计算文档的嵌入</span><br><span class="line">underlying_embeddings = OpenAIEmbeddings()</span><br><span class="line"></span><br><span class="line"># 创建一个CacheBackedEmbeddings的实例。</span><br><span class="line"># 这将为underlying_embeddings提供缓存功能，嵌入会被存储在上面创建的InMemoryStore中。</span><br><span class="line"># 我们还为缓存指定了一个命名空间，以确保不同的嵌入模型之间不会出现冲突。</span><br><span class="line">embedder = CacheBackedEmbeddings.from_bytes_store(</span><br><span class="line">    underlying_embeddings,  # 实际生成嵌入的工具</span><br><span class="line">    store,  # 嵌入的缓存位置</span><br><span class="line">    namespace=underlying_embeddings.model  # 嵌入缓存的命名空间</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 使用embedder为两段文本生成嵌入。</span><br><span class="line"># 结果，即嵌入向量，将被存储在上面定义的内存存储中。</span><br><span class="line">embeddings = embedder.embed_documents([&quot;你好&quot;, &quot;智能鲜花客服&quot;])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>解释下这段代码。首先我们在内存中设置了一个存储空间，然后初始化了一个嵌入工具，该工具将实际生成嵌入。之后，这个嵌入工具被包装在一个缓存工具中，用于为两段文本生成嵌入。</p>
<p>至于其他两种缓存器，嵌入的使用方式也不复杂，你可以参考 LangChain 文档自行学习。</p>
<h3 id="向量数据库（向量存储）">向量数据库（向量存储）</h3>
<p>更常见的存储向量的方式是通过向量数据库（Vector Store）来保存它们。LangChain 支持非常多种向量数据库，其中有很多是开源的，也有很多是商用的。比如 Elasticsearch、Faiss、Chroma 和 Qdrant 等等。</p>
<p>因为选择实在是太多了，我也给你列出来了一个表。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c753cd7e596c49b0b6dad97388522758~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=2911&amp;h=7734&amp;s=3008623&amp;e=jpg&amp;b=fafbfe" alt=""></p>
<p>那么问题来了，面对这么多种类的向量数据库，应该如何选择呢？</p>
<p>这就涉及到许多技术和业务层面的考量，你应该<strong>根据具体需求进行选型</strong>。</p>
<ol>
<li class="lvl-4">
<p>数据规模和速度需求：考虑你的数据量大小以及查询速度的要求。一些向量数据库在处理大规模数据时更加出色，而另一些在低延迟查询中表现更好。</p>
</li>
<li class="lvl-4">
<p>持久性和可靠性：根据你的应用场景，确定你是否需要数据的高可用性、备份和故障转移功能。</p>
</li>
<li class="lvl-4">
<p>易用性和社区支持：考虑向量数据库的学习曲线、文档的完整性以及社区的活跃度。</p>
</li>
<li class="lvl-4">
<p>成本：考虑总体拥有成本，包括许可、硬件、运营和维护成本。</p>
</li>
<li class="lvl-4">
<p>特性：考虑你是否需要特定的功能，例如多模态搜索等。</p>
</li>
<li class="lvl-4">
<p>安全性：确保向量数据库符合你的安全和合规要求。</p>
</li>
</ol>
<p>在进行向量数据库的评测时，进行<strong>性能基准测试</strong>是了解向量数据库实际表现的关键。这可以帮助你评估查询速度、写入速度、并发性能等。</p>
<p>没有 “最好” 的向量数据库，只有 “最适合” 的向量数据库。在你的需求上做些研究和测试，确保你选择的向量数据库满足你的业务和技术要求就好。</p>
<h2 id="数据检索">数据检索</h2>
<p>在 LangChain 中，Retriever，也就是检索器，是数据检索模块的核心入口，它通过非结构化查询返回相关的文档。</p>
<h3 id="向量存储检索器">向量存储检索器</h3>
<p>向量存储检索器是最常见的，它主要支持向量检索。当然 LangChain 也有支持其他类型存储格式的检索器。</p>
<p>下面实现一个端到端的数据检索功能，我们通过 VectorstoreIndexCreator 来创建索引，并在索引的 query 方法中，通过 vectorstore 类的 as_retriever 方法，把向量数据库（Vector Store）直接作为检索器，来完成检索任务。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI的API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;Your OpenAI Key&#x27;</span><br><span class="line"></span><br><span class="line"># 导入文档加载器模块，并使用TextLoader来加载文本文件</span><br><span class="line">from langchain.document_loaders import TextLoader</span><br><span class="line">loader = TextLoader(&#x27;LangChainSamples/OneFlower/易速鲜花花语大全.txt&#x27;, encoding=&#x27;utf8&#x27;)</span><br><span class="line"></span><br><span class="line"># 使用VectorstoreIndexCreator来从加载器创建索引</span><br><span class="line">from langchain.indexes import VectorstoreIndexCreator</span><br><span class="line">index = VectorstoreIndexCreator().from_loaders([loader])</span><br><span class="line"></span><br><span class="line"># 定义查询字符串, 使用创建的索引执行查询</span><br><span class="line">query = &quot;玫瑰花的花语是什么？&quot;</span><br><span class="line">result = index.query(query)</span><br><span class="line">print(result) # 打印查询结果</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">玫瑰花的花语是爱情、热情、美丽。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>你可能会觉得，这个数据检索过程太简单了。这就要归功于 LangChain 的强大封装能力。如果我们审视一下位于 <a target="_blank" rel="noopener" href="http://vectorstore.py">vectorstore.py</a> 中的 VectorstoreIndexCreator 类的代码，你就会发现，它其中封装了 vectorstore、embedding 以及 text_splitter，甚至 document loader（如果你使用 from_documents 方法的话）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class VectorstoreIndexCreator(BaseModel):</span><br><span class="line">    &quot;&quot;&quot;Logic for creating indexes.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    vectorstore_cls: Type[VectorStore] = Chroma</span><br><span class="line">    embedding: Embeddings = Field(default_factory=OpenAIEmbeddings)</span><br><span class="line">    text_splitter: TextSplitter = Field(default_factory=_get_default_text_splitter)</span><br><span class="line">    vectorstore_kwargs: dict = Field(default_factory=dict)</span><br><span class="line"></span><br><span class="line">    class Config:</span><br><span class="line">        &quot;&quot;&quot;Configuration for this pydantic object.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        extra = Extra.forbid</span><br><span class="line">        arbitrary_types_allowed = True</span><br><span class="line"></span><br><span class="line">    def from_loaders(self, loaders: List[BaseLoader]) -&gt; VectorStoreIndexWrapper:</span><br><span class="line">        &quot;&quot;&quot;Create a vectorstore index from loaders.&quot;&quot;&quot;</span><br><span class="line">        docs = []</span><br><span class="line">        for loader in loaders:</span><br><span class="line">            docs.extend(loader.load())</span><br><span class="line">        return self.from_documents(docs)</span><br><span class="line"></span><br><span class="line">    def from_documents(self, documents: List[Document]) -&gt; VectorStoreIndexWrapper:</span><br><span class="line">        &quot;&quot;&quot;Create a vectorstore index from documents.&quot;&quot;&quot;</span><br><span class="line">        sub_docs = self.text_splitter.split_documents(documents)</span><br><span class="line">        vectorstore = self.vectorstore_cls.from_documents(</span><br><span class="line">            sub_docs, self.embedding, **self.vectorstore_kwargs</span><br><span class="line">        )</span><br><span class="line">        return VectorStoreIndexWrapper(vectorstore=vectorstore)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>因此，上面的检索功能就相当于我们第 3 课中讲过的一系列工具的整合。而我们也可以用下面的代码，来显式地指定索引创建器的 vectorstore、embedding 以及 text_splitter，并把它们替换成你所需要的工具，比如另外一种向量数据库或者别的 Embedding 模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from langchain.text_splitter import CharacterTextSplitter</span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)</span><br><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">index_creator = VectorstoreIndexCreator(</span><br><span class="line">    vectorstore_cls=Chroma,</span><br><span class="line">    embedding=OpenAIEmbeddings(),</span><br><span class="line">    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那么，下一个问题是 index.query(query)，又是如何完成具体的检索及文本生成任务的呢？我们此处既没有看到大模型，又没有看到 LangChain 的文档检索工具（比如我们在第 3 课中见过的 QARetrival 链）。</p>
<p>秘密仍然存在于源码中，在 VectorStoreIndexWrapper 类的 query 方法中，可以看到，在调用方法的同时，RetrievalQA 链被启动，以完成检索功能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class VectorStoreIndexWrapper(BaseModel):</span><br><span class="line">    &quot;&quot;&quot;Wrapper around a vectorstore for easy access.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    vectorstore: VectorStore</span><br><span class="line"></span><br><span class="line">    class Config:</span><br><span class="line">        &quot;&quot;&quot;Configuration for this pydantic object.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        extra = Extra.forbid</span><br><span class="line">        arbitrary_types_allowed = True</span><br><span class="line"></span><br><span class="line">    def query(</span><br><span class="line">        self,</span><br><span class="line">        question: str,</span><br><span class="line">        llm: Optional[BaseLanguageModel] = None,</span><br><span class="line">        retriever_kwargs: Optional[Dict[str, Any]] = None,</span><br><span class="line">        **kwargs: Any</span><br><span class="line">    ) -&gt; str:</span><br><span class="line">        &quot;&quot;&quot;Query the vectorstore.&quot;&quot;&quot;</span><br><span class="line">        llm = llm or OpenAI(temperature=0)</span><br><span class="line">        retriever_kwargs = retriever_kwargs or &#123;&#125;</span><br><span class="line">        chain = RetrievalQA.from_chain_type(</span><br><span class="line">            llm, retriever=self.vectorstore.as_retriever(**retriever_kwargs), **kwargs</span><br><span class="line">        )</span><br><span class="line">        return chain.run(question)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面我们用到的向量存储检索器，是向量存储类的轻量级包装器，使其符合检索器接口。它使用向量存储中的搜索方法（例如相似性搜索和 MMR）来查询向量存储中的文本。</p>
<h3 id="各种类型的检索器">各种类型的检索器</h3>
<p>除向量存储检索器之外，LangChain 中还提供很多种其他的检索工具。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c0e6f920df3649f7b1d3cceb938e3d58~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1761&amp;h=1210&amp;s=737309&amp;e=jpg&amp;b=f9fafd" alt=""></p>
<p>这些检索工具，各有其功能特点，你可以查找它们的文档说明，并尝试使用。</p>
<h2 id="索引">索引</h2>
<p>在本节课的最后，我们来看看 LangChain 中的索引（Index）。简单的说，索引是一种高效地管理和定位文档信息的方法，确保每个文档具有唯一标识并便于检索。</p>
<p>尽管在<a target="_blank" rel="noopener" href="https://juejin.cn/book/7387702347436130304/section/7388069933021986856" title="https://juejin.cn/book/7387702347436130304/section/7388069933021986856">第 3 课</a>的示例中，我们并没有显式的使用到索引就完成了一个 RAG 任务，但在复杂的信息检索任务中，有效地管理和索引文档是关键的一步。LangChain 提供的索引 API 为开发者带来了一个高效且直观的解决方案。具体来说，它的优势包括：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>避免重复内容：确保你的向量存储中不会有冗余数据。</p>
</li>
<li class="lvl-4">
<p>只更新更改的内容：能检测哪些内容已更新，避免不必要的重写。</p>
</li>
<li class="lvl-4">
<p>省时省钱：不对未更改的内容重新计算嵌入，从而减少了计算资源的消耗。</p>
</li>
<li class="lvl-4">
<p>优化搜索结果：减少重复和不相关的数据，从而提高搜索的准确性。</p>
</li>
</ul>
<p>LangChain 利用了记录管理器（RecordManager）来跟踪哪些文档已经被写入向量存储。</p>
<p>在进行索引时，API 会对每个文档进行哈希处理，确保每个文档都有一个唯一的标识。这个哈希值不仅仅基于文档的内容，还考虑了文档的元数据。</p>
<p>一旦哈希完成，以下信息会被保存在记录管理器中：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>文档哈希：基于文档内容和元数据计算出的唯一标识。</p>
</li>
<li class="lvl-4">
<p>写入时间：记录文档何时被添加到向量存储中。</p>
</li>
<li class="lvl-4">
<p>源 ID：这是一个元数据字段，表示文档的原始来源。</p>
</li>
</ul>
<p>这种方法确保了即使文档经历了多次转换或处理，也能够精确地跟踪它的状态和来源，确保文档数据被正确管理和索引。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>这节课的内容非常多，而且我给出了很多表格供你查询之用，信息量很大。同时，你可以复习<a target="_blank" rel="noopener" href="https://juejin.cn/book/7387702347436130304/section/7388069933021986856" title="https://juejin.cn/book/7387702347436130304/section/7388069933021986856">第 3 课</a>的内容，我希望你对 RAG 的流程有个更深的理解。</p>
<p>通过检索增强生成来存储和搜索非结构化数据的最常见方法是，给这些非结构化的数据做嵌入并存储生成的嵌入向量，然后在查询时给要查询的文本也做嵌入，并检索与嵌入查询 “最相似” 的嵌入向量。向量数据库则负责存储嵌入数据，并为你执行向量的搜索。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7a1ac21492e64b8792a6ca803c763bdb~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=4096&amp;h=1701&amp;s=712894&amp;e=jpg&amp;b=eefdf3" alt=""></p>
<p>你看，RAG 实际上是为非结构化数据创建了一个 “地图”。当用户有查询请求时，该查询同样被嵌入，然后你的应用程序会在这个“地图” 中寻找与之最匹配的位置，从而快速准确地检索信息。</p>
<p>在我们的鲜花运营场景中，RAG 当然可以在很多方面发挥巨大的作用。你的鲜花有各种各样的品种、颜色和花语，这些数据往往是自然的、松散的，也就是非结构化的。使用 RAG，你可以通过嵌入向量，把库存的鲜花与相关的非结构化信息（如花语、颜色、产地等）关联起来。当客户或者员工想要查询某种鲜花的信息时，系统可以快速地提供准确的答案。</p>
<p>此外，RAG 还可以应用于订单管理。每个订单，无论是客户的姓名、地址、购买的鲜花种类，还是订单状态，都可以被视为非结构化数据。通过 RAG，我们可以轻松地嵌入并检索这些订单，为客户提供实时的订单更新、跟踪和查询服务。</p>
<p>当然，对于订单这样的信息，更常见的情况仍是把它们组织成结构化的数据，存储在数据库中（至少也是 CSV 或者 Excel 表中），以便高效、精准地查询。那么，LLM 能否帮助我们查询数据库表中的条目呢？在下一课中，我将为你揭晓答案。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>请你尝试使用一种文本分割器来给你的文档分块。</p>
</li>
<li class="lvl-4">
<p>请你尝试使用一种新的向量数据库来存储你的文本嵌入。</p>
</li>
<li class="lvl-4">
<p>请你尝试使用一种新的检索器来提取信息。</p>
</li>
</ol>
<p>期待在留言区看到你的实践成果，如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>Github：<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fpsychic-api%2Fdoctran%2Ftree%2Fmain" title="https://github.com/psychic-api/doctran/tree/main">doctran</a>，辅助 LangChain 进行文本转换</p>
</li>
<li class="lvl-4">
<p>文档：LangChain 中 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fdata_connection%2Findexing" title="https://python.langchain.com/docs/modules/data_connection/indexing">Indexing</a> 的说明</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E7%94%A8%E6%80%9D%E7%BB%B4%E9%93%BE%E5%92%8C%E6%80%9D%E7%BB%B4%E6%A0%91%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%80%9D%E8%80%83%E8%B4%A8%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E7%94%A8%E6%80%9D%E7%BB%B4%E9%93%BE%E5%92%8C%E6%80%9D%E7%BB%B4%E6%A0%91%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%80%9D%E8%80%83%E8%B4%A8%E9%87%8F/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:06:52" itemprop="dateModified" datetime="2024-11-18T19:06:52+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>我在上一节的结尾时说了，你可以尝试用思维链也就是 CoT（Chain of Thought）的概念来引导模型的推理，让模型生成更详实、更完备的文案，今天我们就一起看一看 CoT 的使用。</p>
<h2 id="什么是-Chain-of-Thought">什么是 Chain of Thought</h2>
<p>CoT 这个概念来源于学术界，是谷歌大脑的 Jason Wei 等人于 2022 年在论文《<a href="https://link.juejin.cn/?target=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper_files%2Fpaper%2F2022%2Ffile%2F9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" title="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>（自我一致性提升了语言模型中的思维链推理能力）》中提出来的概念。它提出，如果生成一系列的中间推理步骤，就能够显著提高大型语言模型进行复杂推理的能力。</p>
<h3 id="Few-Shot-CoT">Few-Shot CoT</h3>
<p>Few-Shot CoT 简单的在提示中提供了一些链式思考示例（Chain-of-Thought Prompting），足够大的语言模型的推理能力就能够被增强。简单说，就是给出一两个示例，然后在示例中写清楚推导的过程。</p>
<p><img src="https://static001.geekbang.org/resource/image/f2/a0/f27cec109dff8947d85507b34ce240a0.png?wh=940x473" alt=""></p>
<p>论文中给出了一个大模型通过思维链做数学题的示例。图左和图右，大模型都读入了 OneShot 示例，但是图左只给出了答案，而图右则在 OneShot 示例中给出了解题的具体思路。结果，只给出了答案的模型推理错误，而给出解题思路后，同一个模型生成了正确的答案。</p>
<p>在三种大型语言模型的实验中，CoT 在一系列的算术、常识和符号推理任务中都提高了性能。在 GSM8K 数学问题基准测试中，通过 CoT 指导后，大模型的表现可以达到当时最先进的准确性。</p>
<p>CoT 从概念上非常容易理解，从应用上非常容易操作。虽然简单，但这种思想可以给我们的开发过程带来很多启发。</p>
<p>比如，假设我们正在开发一个 AI 花店助手，它的任务是帮助用户选择他们想要的花，并生成一个销售列表。在这个过程中，我们可以使用 CoT 来引导 AI 的推理过程。</p>
<p>? 整体指导：你需要跟着下面的步骤一步步的推理。</p>
<ol>
<li class="lvl-4">
<p>问题理解：首先，AI 需要理解用户的需求。例如，用户可能会说：“今天要参加朋友的生日 Party，想送束花祝福她。” 我们可以给 AI 一个提示模板，里面包含示例：“<em><strong>遇到 XX 问题，我先看自己有</strong><strong><strong>没有</strong></strong><strong>相关知识，有的话，就提供答案；没有，就调用工具搜索，有了知识后再试图解决。</strong></em>”—— 这就是给了 AI 一个思维链的示例。</p>
</li>
<li class="lvl-4">
<p>信息搜索：接下来，AI 需要搜索相关信息。例如，它可能需要查找哪些花最适合生日派对。</p>
</li>
<li class="lvl-4">
<p>决策制定：基于收集到的信息，AI 需要制定一个决策。我们可以通过思维链让他详细思考决策的流程，先做什么后做什么。例如，我们可以给它一个示例：“<em><strong>遇到生日派对送花的情况，我先考虑用户的需求，然后查看鲜花的库存，最后决定推荐一些玫瑰和百合，因为这些花通常适合生日派对。</strong></em>”—— 那么有了生日派对这个场景做示例，大模型就能把类似的思维流程运用到其它场景。</p>
</li>
<li class="lvl-4">
<p>生成销售列表：最后，AI 使用 OutputParser 生成一个销售列表，包括推荐的花和价格。</p>
</li>
</ol>
<p>在这个过程中，整体上，思维链引导 AI 从理解问题，到搜索信息，再到制定决策，最后生成销售列表。这种方法不仅使 AI 的推理过程更加清晰，也使得生成的销售列表更加符合用户的需求。具体到每一个步骤，也可以通过思维链来设计更为详细的提示模板，来引导模型每一步的思考都遵循清晰准确的逻辑。</p>
<p>其实 LangChain 的核心组件 Agent 的本质就是进行好的提示工程，并大量地使用预置的 FewShot 和 CoT 模板。这个在之后的课程学习中我们会理解得越来越透彻。</p>
<h3 id="Zero-Shot-CoT">Zero-Shot CoT</h3>
<p>下面的这两个 CoT 提示模板的例子，来自于 Google Research 和东京大学的论文《<a href="https://link.juejin.cn/?target=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper_files%2Fpaper%2F2022%2Ffile%2F8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf" title="https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf">大语言模型是零样本推理者</a>》。</p>
<p>图中的（d）示例非常非常有意思，在 Zero-Shot CoT 中，你只要简单地告诉模型 “<strong>让我们<strong><strong>一步步的思考</strong></strong>（Let’s think step by step）</strong>”，模型就能够给出更好的答案！</p>
<p><img src="https://static001.geekbang.org/resource/image/09/8b/09a48be47b3e0e9ec0ae7ebd483d868b.png?wh=944x508" alt=""></p>
<p>哈哈哈，这样太神奇了吧，这不由得让我联想起最简单的提示工程，角色设定——模型回答之前，先告诉它 “你是一个很有经验的 XX 专家”，模型应该就会在开始胡说八道之前三思。</p>
<p>简单总结一下：Few-Shot CoT，指的就是在带有示例的提示过程中，加入思考的步骤，从而引导模型给出更好的结果。而 Zero-Shot CoT，就是直接告诉模型要一步一步地思考，慢慢地推理。</p>
<h2 id="Chain-of-Thought-实战">Chain of Thought 实战</h2>
<p>现在，就让我带着你完成一次 Chain of Thought 的 LangChain 应用开发实战。</p>
<p><strong>项目需求</strong>：在这个示例中，你正在开发一个 AI 运营助手，我们要展示 AI 如何根据用户的需求推理和生成答案。然后，AI 根据当前的用户请求进行推理，提供了具体的花卉建议并解释了为什么选择这些建议。</p>
<p>在这个过程中，AI 需要理解客户的需求之后，按部就班的思考，然后给出最符合逻辑的回答。</p>
<h3 id="CoT-的模板设计">CoT 的模板设计</h3>
<p>针对这个聊天机器人的需求，我设计了下面这样的思维链模板。</p>
<p>作为一个为花店电商公司工作的 AI 助手，我的目标是帮助客户根据他们的喜好做出明智的决定。</p>
<p>我会按部就班的思考，先理解客户的需求，然后考虑各种鲜花的涵义，最后根据这个需求，给出我的推荐。</p>
<p>同时，我也会向客户解释我这样推荐的原因。</p>
<p><strong>示例 1：</strong></p>
<p>人类：我想找一种象征爱情的花。</p>
<p>AI：首先，我理解你正在寻找一种可以象征爱情的花。在许多文化中，红玫瑰被视为爱情的象征，这是因为它们的红色通常与热情和浓烈的感情联系在一起。因此，考虑到这一点，我会推荐红玫瑰。红玫瑰不仅能够象征爱情，同时也可以传达出强烈的感情，这是你在寻找的。</p>
<p><strong>示例 2：</strong></p>
<p>人类：我想要一些独特和奇特的花。</p>
<p>AI：从你的需求中，我理解你想要的是独一无二和引人注目的花朵。兰花是一种非常独特并且颜色鲜艳的花，它们在世界上的许多地方都被视为奢侈品和美的象征。因此，我建议你考虑兰花。选择兰花可以满足你对独特和奇特的要求，而且，兰花的美丽和它们所代表的力量和奢侈也可能会吸引你。</p>
<p>AI 的模板开始于对其角色的阐述，并给出了一些先前的对话示例（Few-Shot Learning）来帮助 AI 理解如何处理这种类型的请求。这些示例展示了 AI 如何根据思维链进行思考，给出深思熟虑之后的答案。</p>
<h3 id="程序的完整框架">程序的完整框架</h3>
<p>程序的完整代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"># 设置环境变量和API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI API Key&#x27;</span><br><span class="line"></span><br><span class="line"># 创建聊天模型</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">llm = ChatOpenAI(temperature=0)</span><br><span class="line"></span><br><span class="line"># 设定 AI 的角色和目标</span><br><span class="line">role_template = &quot;你是一个为花店电商公司工作的AI助手, 你的目标是帮助客户根据他们的喜好做出明智的决定&quot;</span><br><span class="line"></span><br><span class="line"># CoT 的关键部分，AI 解释推理过程，并加入一些先前的对话示例（Few-Shot Learning）</span><br><span class="line">cot_template = &quot;&quot;&quot;</span><br><span class="line">作为一个为花店电商公司工作的AI助手，我的目标是帮助客户根据他们的喜好做出明智的决定。 </span><br><span class="line"></span><br><span class="line">我会按部就班的思考，先理解客户的需求，然后考虑各种鲜花的涵义，最后根据这个需求，给出我的推荐。</span><br><span class="line">同时，我也会向客户解释我这样推荐的原因。</span><br><span class="line"></span><br><span class="line">示例 1:</span><br><span class="line">  人类：我想找一种象征爱情的花。</span><br><span class="line">  AI：首先，我理解你正在寻找一种可以象征爱情的花。在许多文化中，红玫瑰被视为爱情的象征，这是因为它们的红色通常与热情和浓烈的感情联系在一起。因此，考虑到这一点，我会推荐红玫瑰。红玫瑰不仅能够象征爱情，同时也可以传达出强烈的感情，这是你在寻找的。</span><br><span class="line"></span><br><span class="line">示例 2:</span><br><span class="line">  人类：我想要一些独特和奇特的花。</span><br><span class="line">  AI：从你的需求中，我理解你想要的是独一无二和引人注目的花朵。兰花是一种非常独特并且颜色鲜艳的花，它们在世界上的许多地方都被视为奢侈品和美的象征。因此，我建议你考虑兰花。选择兰花可以满足你对独特和奇特的要求，而且，兰花的美丽和它们所代表的力量和奢侈也可能会吸引你。</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate</span><br><span class="line">system_prompt_role = SystemMessagePromptTemplate.from_template(role_template)</span><br><span class="line">system_prompt_cot = SystemMessagePromptTemplate.from_template(cot_template)</span><br><span class="line"></span><br><span class="line"># 用户的询问</span><br><span class="line">human_template = &quot;&#123;human_input&#125;&quot;</span><br><span class="line">human_prompt = HumanMessagePromptTemplate.from_template(human_template)</span><br><span class="line"></span><br><span class="line"># 将以上所有信息结合为一个聊天提示</span><br><span class="line">chat_prompt = ChatPromptTemplate.from_messages([system_prompt_role, system_prompt_cot, human_prompt])</span><br><span class="line"></span><br><span class="line">prompt = chat_prompt.format_prompt(human_input=&quot;我想为我的女朋友购买一些花。她喜欢粉色和紫色。你有什么建议吗?&quot;).to_messages()</span><br><span class="line"></span><br><span class="line"># 接收用户的询问，返回回答结果</span><br><span class="line">response = llm(prompt)</span><br><span class="line">print(response)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>程序中，首先设置环境变量 OpenAI 的 API 密钥，以便能够使用 OpenAI 的 GPT-4 模型。然后创建聊天模型：通过调用 ChatOpenAI 类，创建了一个聊天模型。设置 temperature=0 可以让模型生成更确定性的回答，即输出更倾向于最可能的结果。</p>
<p>接着定义了 AI 的角色和目标，该 AI 为花店电商公司的助手，其目标是根据客户的喜好来提供购买建议。紧接着，定义 CoT 模板，其中包括了 AI 的角色和目标描述、思考链条以及遵循思考链条的一些示例，显示了 AI 如何理解问题，并给出建议。</p>
<p>之后，我使用了 PromptTemplate 的 from_template 方法，来生成相应的询问模板。其中包括用于指导模型的 SystemMessagePromptTemplate 和用于传递人类问题的 HumanMessagePromptTemplate。</p>
<p>然后，我使用了 ChatPromptTemplate.from_messages 方法，整合上述定义的角色，CoT 模板和用户询问，生成聊天提示。</p>
<p>最后，将生成的聊天提示输入模型中，获得模型的回答，并打印出来。</p>
<p>在 Few-Shot CoT 提示的指引之下，模型针对我们的问题，从问题中的具体需求出发，返回了不错的建议。</p>
<p><em><strong>现在，根据你的需求：你正在寻找你的女朋友喜欢的粉色和紫色的花。</strong></em></p>
<p><em><strong>首先，我从理解你的需求出发，只会推荐粉色或紫色，或者两者的组合的花。这些可能包括粉色的玫瑰，紫色的兰花，或者是粉色和紫色的花的混合花束。玫瑰是象征爱情和亲情的经典符号，而兰花象征着美丽和力量。这两种花都蕴含很棒的内涵。当然了，无论你选择哪种花卉，重要的是表达出你对她的爱和关心。记得附上一张温馨的贺卡，写下你的真挚祝福。</strong></em></p>
<h2 id="Tree-of-Thought">Tree of Thought</h2>
<p>CoT 这种思想，为大模型带来了更好的答案，然而，对于需要探索或预判战略的复杂任务来说，传统或简单的提示技巧是不够的。基于 CoT 的思想，Yao 和 Long 等人几乎在同一时间在论文《<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.10601.pdf" title="https://arxiv.org/pdf/2305.10601.pdf">思维之树：使用大型语言模型进行深思熟虑的问题解决</a>》和《<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.08291.pdf" title="https://arxiv.org/pdf/2305.08291.pdf">大型语言模型指导的思维之树</a>》中，进一步提出了思维树（Tree of Thoughts，ToT）框架，该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。</p>
<p>ToT 是一种解决复杂问题的框架，它在需要多步骤推理的任务中，引导语言模型搜索一棵由连贯的语言序列（解决问题的中间步骤）组成的思维树，而不是简单地生成一个答案。ToT 框架的核心思想是：让模型生成和评估其思维的能力，并将其与搜索算法（如广度优先搜索和深度优先搜索）结合起来，进行系统性地探索和验证。</p>
<p><img src="https://static001.geekbang.org/resource/image/6e/a0/6eec83ffe1a5f37d245520535d65f8a0.png?wh=1083x550" alt=""></p>
<p>ToT 框架为每个任务定义具体的思维步骤和每个步骤的候选项数量。例如，要解决一个数学推理任务，先把它分解为 3 个思维步骤，并为每个步骤提出多个方案，并保留最优的 5 个候选方案。然后在多条思维路径中搜寻最优的解决方案。</p>
<p>这种方法的优势在于，模型可以通过观察和评估其自身的思维过程，更好地解决问题，而不仅仅是基于输入生成输出。这对于需要深度推理的复杂任务非常有用。此外，通过引入强化学习、集束搜索等技术，可以进一步提高搜索策略的性能，并让模型在解决新问题或面临未知情况时有更好的表现。</p>
<p>下面我们应用 ToT 的思想，给出一个鲜花运营方面的示例。</p>
<p>假设一个顾客在鲜花网站上询问：“我想为我的妻子购买一束鲜花，但我不确定应该选择哪种鲜花。她喜欢淡雅的颜色和花香。”</p>
<p>AI（使用 ToT 框架）：</p>
<p><strong>思维步骤 1</strong>：理解顾客的需求。</p>
<p>顾客想为妻子购买鲜花。</p>
<p>顾客的妻子喜欢淡雅的颜色和花香。</p>
<p><strong>思维步骤 2</strong>：考虑可能的鲜花选择。</p>
<p>候选 1：百合，因为它有淡雅的颜色和花香。</p>
<p>候选 2：玫瑰，选择淡粉色或白色，它们通常有花香。</p>
<p>候选 3：紫罗兰，它有淡雅的颜色和花香。</p>
<p>候选 4：桔梗，它的颜色淡雅但不一定有花香。</p>
<p>候选 5：康乃馨，选择淡色系列，它们有淡雅的花香。</p>
<p><strong>思维步骤 3</strong>：根据顾客的需求筛选最佳选择。</p>
<p>百合和紫罗兰都符合顾客的需求，因为它们都有淡雅的颜色和花香。</p>
<p>淡粉色或白色的玫瑰也是一个不错的选择。</p>
<p>桔梗可能不是最佳选择，因为它可能没有花香。</p>
<p>康乃馨是一个可考虑的选择。</p>
<p><strong>思维步骤 4</strong>：给出建议。</p>
<p>“考虑到您妻子喜欢淡雅的颜色和花香，我建议您可以选择百合或紫罗兰。淡粉色或白色的玫瑰也是一个很好的选择。希望这些建议能帮助您做出决策！”</p>
<p>这个例子，可以作为 FewShot 示例之一，传递给模型，让他学着实现 ToT。</p>
<p>通过在具体的步骤中产生多条思考路径，ToT 框架为解决复杂问题提供了一种新的方法，这种方法结合了语言模型的生成能力、搜索算法以及强化学习，以达到更好的效果。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>这节课我们介绍了 Chain of Thought（CoT，即 “思维链”）和 Tree of Thoughts（ToT，即 “思维树”）这两个非常有趣的概念，并探讨了如何利用它们引导大型语言模型进行更深入的推理。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>CoT 的核心思想是通过生成一系列中间推理步骤来增强模型的推理能力。在 Few-Shot CoT 和 Zero-Shot CoT 两种应用方法中，前者通过提供链式思考示例传递给模型，后者则直接告诉模型进行要按部就班的推理。</p>
</li>
<li class="lvl-4">
<p>ToT 进一步扩展了 CoT 的思想，通过搜索由连贯的语言序列组成的思维树来解决复杂问题。我通过一个鲜花选择的实例，展示了如何在实际应用中使用 ToT 框架。</p>
</li>
</ul>
<p>有朋友在 GitHub 上开了一个 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts" title="https://github.com/kyegomez/tree-of-thoughts">Repo</a>，专门给大家介绍 ToT 的应用方法和实例，他们还给出了几个非常简单的通用 ToT 提示语，就像下面这样。</p>
<p>请你模拟三位出色、逻辑性强的专家合作回答一个问题。每个人都详细地解释他们的思考过程，考虑到其他人之前的解释，并公开承认错误。在每一步，只要可能，每位专家都会在其他人的思考基础上进行完善和建设，并承认他们的贡献。他们继续，直到对问题有一个明确的答案。为了清晰起见，您的整个回应应该是一个 Markdown 表格。   问题是…</p>
<p><img src="https://static001.geekbang.org/resource/image/d7/3c/d719e10a2b045f5a70993b6135ef503c.png?wh=1920x1357" alt=""></p>
<p>如果你有兴趣，可以去这个 Repo 里面看一看。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>我们的 CoT 实战示例中使用的是 Few-Shot CoT 提示，请你把它换为 Zero-Shot CoT，跑一下程序，看看结果。</p>
</li>
<li class="lvl-4">
<p>请你设计一个你工作场景中的任务需求，然后用 ToT 让大语言模型帮你解决问题。</p>
</li>
</ol>
<p>期待在留言区看到你的分享，我们一起交流探讨，共创一个好的学习氛围。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>论文，自我一致性提升了语言模型中的思维链推理能力，<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2205.11916.pdf" title="https://arxiv.org/pdf/2205.11916.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>，Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., &amp; Zhou, D. (2023). Self-Consistency Improves Chain of Thought Reasoning in Language Models. Proceedings of the International Conference on Learning Representations (ICLR). arXiv preprint arXiv:2203.11171.</p>
</li>
<li class="lvl-4">
<p>论文，大语言模型是零样本推理者，<a href="https://link.juejin.cn/?target=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper_files%2Fpaper%2F2022%2Ffile%2F8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf" title="https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf">Large Language Models are Zero-Shot Reasoners</a>，Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., &amp; Iwasawa, Y. (2023). Large Language Models are Zero-Shot Reasoners. arXiv preprint arXiv:2205.11916v4.</p>
</li>
<li class="lvl-4">
<p>论文，思维之树：使用大型语言模型进行深思熟虑的问题解决，<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fabs%2F2305.10601" title="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a>，Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., &amp; Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv preprint arXiv:2305.10601.</p>
</li>
<li class="lvl-4">
<p>论文，大型语言模型指导的思维之树，<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fabs%2F2305.08291" title="https://arxiv.org/abs/2305.08291">Large Language Model Guided Tree-of-Thought</a>，Long, J. (2023). Large Language Model Guided Tree-of-Thought. arXiv preprint arXiv:2305.08291.</p>
</li>
<li class="lvl-4">
<p>GitHub 链接，<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fkyegomez%2Ftree-of-thoughts" title="https://github.com/kyegomez/tree-of-thoughts">tree-of-thoughts</a>，把 ToT 算法导入你的大模型应用，目前 3.3K 颗星</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%A8%A1%E5%9E%8BIO%EF%BC%9A%E8%BE%93%E5%85%A5%E6%8F%90%E7%A4%BA%E3%80%81%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B%E3%80%81%E8%A7%A3%E6%9E%90%E8%BE%93%E5%87%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E6%A8%A1%E5%9E%8BIO%EF%BC%9A%E8%BE%93%E5%85%A5%E6%8F%90%E7%A4%BA%E3%80%81%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B%E3%80%81%E8%A7%A3%E6%9E%90%E8%BE%93%E5%87%BA/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:04:15" itemprop="dateModified" datetime="2024-11-18T19:04:15+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>从这节课开始，我们将对 LangChain 中的六大核心组件一一进行详细的剖析。</p>
<p>模型，位于 LangChain 框架的最底层，它是基于语言模型构建的应用的<strong>核心元素</strong>，因为所谓 LangChain 应用开发，就是以 LangChain 作为框架，通过 API 调用大模型来解决具体问题的过程。</p>
<p>可以说，整个 LangChain 框架的逻辑都是由 LLM 这个发动机来驱动的。没有模型，LangChain 这个框架也就失去了它存在的意义。那么这节课我们就详细讲讲模型，最后你会收获一个能够自动生成鲜花文案的应用程序。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/487e3a24b3324a279309b90adce319ec~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1965&amp;h=1363&amp;s=498342&amp;e=png&amp;b=ffffff" alt=""></p>
<h2 id="Model-I-O">Model I/O</h2>
<p>我们可以把对模型的使用过程拆解成三块，分别是<strong>输入提示</strong>（对应图中的 Format）、<strong>调用模型</strong>（对应图中的 Predict）和<strong>输出解析</strong>（对应图中的 Parse）。这三块形成了一个整体，因此在 LangChain 中这个过程被统称为 <strong>Model I/O</strong>（Input/Output）。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ffb488f5f695467bb994245df8d947c1~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=4000&amp;h=1536&amp;s=298807&amp;e=png&amp;b=eefcf1" alt=""></p>
<p>在模型 I/O 的每个环节，LangChain 都为咱们提供了模板和工具，快捷地形成调用各种语言模型的接口。</p>
<ol>
<li class="lvl-4">
<p><strong>提示模板</strong>：使用模型的第一个环节是把提示信息输入到模型中，你可以创建 LangChain 模板，根据实际需求动态选择不同的输入，针对特定的任务和应用调整输入。</p>
</li>
<li class="lvl-4">
<p><strong>语言模型</strong>：LangChain 允许你通过通用接口来调用语言模型。这意味着无论你要使用的是哪种语言模型，都可以通过同一种方式进行调用，这样就提高了灵活性和便利性。</p>
</li>
<li class="lvl-4">
<p><strong>输出解析</strong>：LangChain 还提供了从模型输出中提取信息的功能。通过输出解析器，你可以精确地从模型的输出中获取需要的信息，而不需要处理冗余或不相关的数据，更重要的是还可以把大模型给回的非结构化文本，转换成程序可以处理的结构化数据。</p>
</li>
</ol>
<p>下面我们用示例的方式来深挖一下这三个环节。先来看看 LangChain 中提示模板的构建。</p>
<h2 id="提示模板">提示模板</h2>
<p>语言模型是个无穷无尽的宝藏，人类的知识和智慧，好像都封装在了这个 “魔盒” 里面了。但是，怎样才能解锁其中的奥秘，那可就是仁者见仁智者见智了。所以，现在 “提示工程” 这个词特别流行，所谓 Prompt Engineering，就是专门研究对大语言模型的提示构建。</p>
<p>我的观点是，使用大模型的场景千差万别，因此肯定不存在那么一两个神奇的模板，能够骗过所有模型，让它总能给你最想要的回答。然而，好的提示（其实也就是好的问题或指示啦），肯定能够让你在调用语言模型的时候事半功倍。</p>
<p>那其中的具体原则，不外乎吴恩达老师在他的<a href="https://link.juejin.cn/?target=https%3A%2F%2Flearn.deeplearning.ai%2Flogin%3Fredirect_course%3Dchatgpt-prompt-eng" title="https://learn.deeplearning.ai/login?redirect_course=chatgpt-prompt-eng">提示工程课程</a>中所说的：</p>
<ol>
<li class="lvl-4">
<p>给予模型清晰明确的指示</p>
</li>
<li class="lvl-4">
<p>让模型慢慢地思考</p>
</li>
</ol>
<p>说起来很简单，对吧？是的，道理总是简单，但是如何具体实践这些原则，又是个大问题。让我从创建一个简单的 LangChain 提示模板开始。</p>
<p>这里，我们希望为销售的每一种鲜花生成一段简介文案，那么每当你的员工或者顾客想了解某种鲜花时，调用该模板就会生成适合的文字。</p>
<p>这个提示模板的生成方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 导入LangChain中的提示模板</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line"># 创建原始模板</span><br><span class="line">template = &quot;&quot;&quot;您是一位专业的鲜花店文案撰写员。\n</span><br><span class="line">对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 根据原始模板创建LangChain提示模板</span><br><span class="line">prompt = PromptTemplate.from_template(template) </span><br><span class="line"># 打印LangChain提示模板的内容</span><br><span class="line">print(prompt)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>提示模板的具体内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input_variables=[&#x27;flower_name&#x27;, &#x27;price&#x27;] </span><br><span class="line">output_parser=None partial_variables=&#123;&#125; </span><br><span class="line">template=&#x27;/\n您是一位专业的鲜花店文案撰写员。</span><br><span class="line">\n对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？\n&#x27;</span><br><span class="line">template_format=&#x27;f-string&#x27; </span><br><span class="line">validate_template=True</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在这里，所谓 “模板” 就是一段描述某种鲜花的文本格式，它是一个 f-string，其中有两个变量 {flower_name} 和 {price} 表示花的名称和价格，这两个值是模板里面的占位符，在实际使用模板生成提示时会被具体的值替换。</p>
<p>代码中的 from_template 是一个类方法，它允许我们直接从一个字符串模板中创建一个 PromptTemplate 对象。打印出这个 PromptTemplate 对象，你可以看到这个对象中的信息包括输入的变量（在这个例子中就是 <code>flower_name</code> 和 <code>price</code>）、输出解析器（这个例子中没有指定）、模板的格式（这个例子中为<code>'f-string'</code>）、是否验证模板（这个例子中设置为 <code>True</code>）。</p>
<p>因此 PromptTemplate 的 from_template 方法就是将一个原始的模板字符串转化为一个更丰富、更方便操作的 PromptTemplate 对象，这个对象就是 LangChain 中的提示模板。LangChain 提供了多个类和函数，也<strong>为各种应用场景设计了很多内置模板，使构建和使用提示变得容易</strong>。我们下节课还会对提示工程的基本原理和 LangChain 中的各种提示模板做更深入的讲解。</p>
<p>下面，我们将会使用这个刚刚构建好的提示模板来生成提示，并把提示输入到大语言模型中。</p>
<h2 id="语言模型"><strong>语言模型</strong></h2>
<p>LangChain 中支持的模型有三大类。</p>
<ol>
<li class="lvl-4">
<p>大语言模型（LLM） ，也叫 Text Model，这些模型将文本字符串作为输入，并返回文本字符串作为输出。Open AI 的 text-davinci-003、Facebook 的 LLaMA、ANTHROPIC 的 Claude，都是典型的 LLM。</p>
</li>
<li class="lvl-4">
<p>聊天模型（Chat Model），主要代表 Open AI 的 ChatGPT 系列模型。这些模型通常由语言模型支持，但它们的 API 更加结构化。具体来说，这些模型将聊天消息列表作为输入，并返回聊天消息。</p>
</li>
<li class="lvl-4">
<p>文本嵌入模型（Embedding Model），这些模型将文本作为输入并返回浮点数列表，也就是 Embedding。而文本嵌入模型如 OpenAI 的 text-embedding-ada-002，我们之前已经见过了。文本嵌入模型负责把文档存入向量数据库，和我们这里探讨的提示工程关系不大。</p>
</li>
</ol>
<p>然后，我们将调用语言模型，让模型帮我们写文案，并且返回文案的结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API Key</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的Open AI API Key&#x27;</span><br><span class="line"></span><br><span class="line"># 导入LangChain中的OpenAI模型接口</span><br><span class="line">from langchain_openai import OpenAI</span><br><span class="line"># 创建模型实例</span><br><span class="line">model = OpenAI(model_name=&#x27;gpt-3.5-turbo-instruct&#x27;)</span><br><span class="line"># 输入提示</span><br><span class="line">input = prompt.format(flower_name=[&quot;玫瑰&quot;], price=&#x27;50&#x27;)</span><br><span class="line"># 得到模型的输出</span><br><span class="line">output = model.invoke(input)</span><br><span class="line"># 打印输出内容</span><br><span class="line">print(output)  </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>input = prompt.format(flower_name=[&quot;玫瑰&quot;], price='50')</code> 这行代码的作用是将模板实例化，此时将 <code>&#123;flower_name&#125;</code> 替换为 <code>&quot;玫瑰&quot;</code>，<code>&#123;price&#125;</code> 替换为 <code>'50'</code>，形成了具体的提示：“您是一位专业的鲜花店文案撰写员。对于售价为 50 元的玫瑰，您能提供一个吸引人的简短描述吗？”</p>
<p>接收到这个输入，调用模型之后，得到的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">让你心动！50元就可以拥有这支充满浪漫气息的玫瑰花束，让TA感受你的真心爱意。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>复用提示模板，我们可以同时生成多个鲜花的文案。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># 导入LangChain中的提示模板</span><br><span class="line">from langchain import PromptTemplate</span><br><span class="line"># 创建原始模板</span><br><span class="line">template = &quot;&quot;&quot;您是一位专业的鲜花店文案撰写员。\n</span><br><span class="line">对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 根据原始模板创建LangChain提示模板</span><br><span class="line">prompt = PromptTemplate.from_template(template) </span><br><span class="line"># 打印LangChain提示模板的内容</span><br><span class="line">print(prompt)</span><br><span class="line"></span><br><span class="line"># 设置OpenAI API Key</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的Open AI API Key&#x27;</span><br><span class="line"></span><br><span class="line"># 导入LangChain中的OpenAI模型接口</span><br><span class="line">from langchain import OpenAI</span><br><span class="line"># 创建模型实例</span><br><span class="line">model = OpenAI(model_name=&#x27;gpt-3.5-turbo-instruct&#x27;)</span><br><span class="line"></span><br><span class="line"># 多种花的列表</span><br><span class="line">flowers = [&quot;玫瑰&quot;, &quot;百合&quot;, &quot;康乃馨&quot;]</span><br><span class="line">prices = [&quot;50&quot;, &quot;30&quot;, &quot;20&quot;]</span><br><span class="line"></span><br><span class="line"># 生成多种花的文案</span><br><span class="line">for flower, price in zip(flowers, prices):</span><br><span class="line">    # 使用提示模板生成输入</span><br><span class="line">    input_prompt = prompt.format(flower_name=flower, price=price)</span><br><span class="line"></span><br><span class="line">    # 得到模型的输出</span><br><span class="line">    output = model.invoke(input_prompt)</span><br><span class="line"></span><br><span class="line">    # 打印输出内容</span><br><span class="line">    print(output)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>模型的输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这支玫瑰，深邃的红色，传递着浓浓的深情与浪漫，令人回味无穷！</span><br><span class="line">百合：美丽的花朵，多彩的爱恋！30元让你拥有它！</span><br><span class="line">康乃馨—20元，象征爱的祝福，送给你最真挚的祝福。</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>你也许会问我，在这个过程中，使用 LangChain 的意义究竟何在呢？我直接调用 Open AI 的 API，不是完全可以实现相同的功能吗？</p>
<p>的确如此，让我们来看看直接使用 Open AI API 来完成上述功能的代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import openai # 导入OpenAI</span><br><span class="line">openai.api_key = &#x27;Your-OpenAI-API-Key&#x27; # API Key</span><br><span class="line"></span><br><span class="line">prompt_text = &quot;您是一位专业的鲜花店文案撰写员。对于售价为&#123;&#125;元的&#123;&#125;，您能提供一个吸引人的简短描述吗？&quot; # 设置提示</span><br><span class="line"></span><br><span class="line">flowers = [&quot;玫瑰&quot;, &quot;百合&quot;, &quot;康乃馨&quot;]</span><br><span class="line">prices = [&quot;50&quot;, &quot;30&quot;, &quot;20&quot;]</span><br><span class="line"></span><br><span class="line"># 循环调用Text模型的Completion方法，生成文案</span><br><span class="line">for flower, price in zip(flowers, prices):</span><br><span class="line">    prompt = prompt_text.format(price, flower)</span><br><span class="line">    response = openai.completions.create(</span><br><span class="line">        engine=&quot;gpt-3.5-turbo-instruct&quot;,</span><br><span class="line">        prompt=prompt,</span><br><span class="line">        max_tokens=100</span><br><span class="line">    )</span><br><span class="line">    print(response.choices[0].text.strip()) # 输出文案</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面的代码是直接使用 Open AI 和带有 {} 占位符的提示语，同时生成了三种鲜花的文案。看起来也是相当简洁。</p>
<p>不过，如果你深入思考一下，你就会发现 LangChain 的优势所在。<strong>我们只需要定义一次模板，就可以用它来生成各种不同的提示</strong>。对比单纯使用 f-string 来格式化文本，这种方法更加简洁，也更容易维护。而 LangChain 在提示模板中，还整合了 output_parser、template_format 以及是否需要 validate_template 等功能。</p>
<p>更重要的是，使用 LangChain 提示模板，我们还可以很方便地把程序切换到不同的模型，而不需要修改任何提示相关的代码。</p>
<p>下面，我们用完全相同的提示模板来生成提示，并发送给 HuggingFaceHub 中的开源模型来创建文案。（注意：需要注册 HUGGINGFACEHUB_API_TOKEN）</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b336202a59124ff69d1ea0184e5321b3~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1704&amp;h=854&amp;s=302708&amp;e=png&amp;b=fffafa" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 导入LangChain中的提示模板</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line"># 创建原始模板</span><br><span class="line">template = &quot;&quot;&quot;You are a flower shop assitiant。\n</span><br><span class="line">For &#123;price&#125; of &#123;flower_name&#125; ，can you write something for me？</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># 根据原始模板创建LangChain提示模板</span><br><span class="line">prompt = PromptTemplate.from_template(template) </span><br><span class="line"># 打印LangChain提示模板的内容</span><br><span class="line">print(prompt)</span><br><span class="line">import os</span><br><span class="line">os.environ[&#x27;HUGGINGFACEHUB_API_TOKEN&#x27;] = &#x27;你的HuggingFace API Token&#x27;</span><br><span class="line"># 导入LangChain中的OpenAI模型接口</span><br><span class="line">from langchain_community.llms import HuggingFaceHub</span><br><span class="line"># 创建模型实例</span><br><span class="line">model= HuggingFaceHub(repo_id=&quot;google/flan-t5-large&quot;)</span><br><span class="line"># 输入提示</span><br><span class="line">input = prompt.format(flower_name=[&quot;rose&quot;], price=&#x27;50&#x27;)</span><br><span class="line"># 得到模型的输出</span><br><span class="line">output = model(input)</span><br><span class="line"># 打印输出内容</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i love you</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>真是一分钱一分货，当我使用较早期的开源模型 T5，得到了很粗糙的文案 “i love you”（哦，还要注意 T5 还没有支持中文的能力，我把提示文字换成英文句子，结构其实都没变）。</p>
<p>当然，这里我想要向你传递的信息是：你可以重用模板，重用程序结构，通过 LangChain 框架调用任何模型。如果你熟悉机器学习的训练流程的话，这 LangChain 是不是让你联想到 PyTorch 和 TensorFlow 这样的框架——<strong>模型可以自由选择、自主训练，而调用模型的框架往往是有章法、而且可复用的</strong>。</p>
<p>因此，使用 LangChain 和提示模板的好处是：</p>
<ol>
<li class="lvl-4">
<p>代码的可读性：使用模板的话，提示文本更易于阅读和理解，特别是对于复杂的提示或多变量的情况。</p>
</li>
<li class="lvl-4">
<p>可复用性：模板可以在多个地方被复用，让你的代码更简洁，不需要在每个需要生成提示的地方重新构造提示字符串。</p>
</li>
<li class="lvl-4">
<p>维护：如果你在后续需要修改提示，使用模板的话，只需要修改模板就可以了，而不需要在代码中查找所有使用到该提示的地方进行修改。</p>
</li>
<li class="lvl-4">
<p>变量处理：如果你的提示中涉及到多个变量，模板可以自动处理变量的插入，不需要手动拼接字符串。</p>
</li>
<li class="lvl-4">
<p>参数化：模板可以根据不同的参数生成不同的提示，这对于个性化生成文本非常有用。</p>
</li>
</ol>
<p>那我们就接着介绍模型 I/O 的最后一步，输出解析。</p>
<h2 id="输出解析"><strong>输出解析</strong></h2>
<p>LangChain 提供的解析模型输出的功能，使你能够更容易地从模型输出中获取结构化的信息，这将大大加快基于语言模型进行应用开发的效率。</p>
<p>为什么这么说呢？请你思考一下刚才的例子，你只是让模型生成了一个文案。这段文字是一段字符串，正是你所需要的。但是，在开发具体应用的过程中，很明显<strong>我们不仅仅需要文字，更多情况下我们需要的是程序能够直接处理的、结构化的数据</strong>。</p>
<p>比如说，在这个文案中，如果你希望模型返回两个字段：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>description：鲜花的说明文本</p>
</li>
<li class="lvl-4">
<p>reason：解释一下为何要这样写上面的文案</p>
</li>
</ul>
<p>那么，模型可能返回的一种结果是：</p>
<p><strong>A</strong>：“文案是：让你心动！50 元就可以拥有这支充满浪漫气息的玫瑰花束，让 TA 感受你的真心爱意。为什么这样说呢？因为爱情是无价的，50 元对应热恋中的情侣也会觉得值得。”</p>
<p>上面的回答并不是我们在处理数据时所需要的，我们需要的是一个类似于下面的 Python 字典。</p>
<p><strong>B</strong>：{description: “让你心动！50 元就可以拥有这支充满浪漫气息的玫瑰花束，让 TA 感受你的真心爱意。” ; reason: “因为爱情是无价的，50 元对应热恋中的情侣也会觉得值得。”}</p>
<p>那么从 A 的笼统言语，到 B 这种结构清晰的数据结构，如何自动实现？这就需要 LangChain 中的输出解析器上场了。</p>
<p>下面，我们就通过 LangChain 的输出解析器来重构程序，让模型有能力生成结构化的回应，同时对其进行解析，直接将解析好的数据存入 CSV 文档。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"># 导入OpenAI Key</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的OpenAI API Key&#x27;</span><br><span class="line"></span><br><span class="line"># 导入LangChain中的提示模板</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line"># 创建原始提示模板</span><br><span class="line">prompt_template = &quot;&quot;&quot;您是一位专业的鲜花店文案撰写员。</span><br><span class="line">对于售价为 &#123;price&#125; 元的 &#123;flower_name&#125; ，您能提供一个吸引人的简短描述吗？</span><br><span class="line">&#123;format_instructions&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 通过LangChain调用模型</span><br><span class="line">from langchain_openai import OpenAI</span><br><span class="line"># 创建模型实例</span><br><span class="line">model = OpenAI(model_name=&#x27;gpt-3.5-turbo-instruct&#x27;)</span><br><span class="line"></span><br><span class="line"># 导入结构化输出解析器和ResponseSchema</span><br><span class="line">from langchain.output_parsers import StructuredOutputParser, ResponseSchema</span><br><span class="line"># 定义我们想要接收的响应模式</span><br><span class="line">response_schemas = [</span><br><span class="line">    ResponseSchema(name=&quot;description&quot;, description=&quot;鲜花的描述文案&quot;),</span><br><span class="line">    ResponseSchema(name=&quot;reason&quot;, description=&quot;问什么要这样写这个文案&quot;)</span><br><span class="line">]</span><br><span class="line"># 创建输出解析器</span><br><span class="line">output_parser = StructuredOutputParser.from_response_schemas(response_schemas)</span><br><span class="line"></span><br><span class="line"># 获取格式指示</span><br><span class="line">format_instructions = output_parser.get_format_instructions()</span><br><span class="line"># 根据原始模板创建提示，同时在提示中加入输出解析器的说明</span><br><span class="line">prompt = PromptTemplate.from_template(prompt_template, </span><br><span class="line">                partial_variables=&#123;&quot;format_instructions&quot;: format_instructions&#125;) </span><br><span class="line"></span><br><span class="line"># 数据准备</span><br><span class="line">flowers = [&quot;玫瑰&quot;, &quot;百合&quot;, &quot;康乃馨&quot;]</span><br><span class="line">prices = [&quot;50&quot;, &quot;30&quot;, &quot;20&quot;]</span><br><span class="line"></span><br><span class="line"># 创建一个空的DataFrame用于存储结果</span><br><span class="line">import pandas as pd</span><br><span class="line">df = pd.DataFrame(columns=[&quot;flower&quot;, &quot;price&quot;, &quot;description&quot;, &quot;reason&quot;]) # 先声明列名</span><br><span class="line"></span><br><span class="line">for flower, price in zip(flowers, prices):</span><br><span class="line">    # 根据提示准备模型的输入</span><br><span class="line">    input = prompt.format(flower_name=flower, price=price)</span><br><span class="line"></span><br><span class="line">    # 获取模型的输出</span><br><span class="line">    output = model.invoke(input)</span><br><span class="line">    </span><br><span class="line">    # 解析模型的输出（这是一个字典结构）</span><br><span class="line">    parsed_output = output_parser.parse(output)</span><br><span class="line"></span><br><span class="line">    # 在解析后的输出中添加“flower”和“price”</span><br><span class="line">    parsed_output[&#x27;flower&#x27;] = flower</span><br><span class="line">    parsed_output[&#x27;price&#x27;] = price</span><br><span class="line"></span><br><span class="line">    # 将解析后的输出添加到DataFrame中</span><br><span class="line">    df.loc[len(df)] = parsed_output  </span><br><span class="line"></span><br><span class="line"># 打印字典</span><br><span class="line">print(df.to_dict(orient=&#x27;records&#x27;))</span><br><span class="line"></span><br><span class="line"># 保存DataFrame到CSV文件</span><br><span class="line">df.to_csv(&quot;flowers_with_descriptions.csv&quot;, index=False)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&#x27;flower&#x27;: &#x27;玫瑰&#x27;, &#x27;price&#x27;: &#x27;50&#x27;, &#x27;description&#x27;: &#x27;Luxuriate in the beauty of this 50 yuan rose, with its deep red petals and delicate aroma.&#x27;, &#x27;reason&#x27;: &#x27;This description emphasizes the elegance and beauty of the rose, which will be sure to draw attention.&#x27;&#125;, </span><br><span class="line">&#123;&#x27;flower&#x27;: &#x27;百合&#x27;, &#x27;price&#x27;: &#x27;30&#x27;, &#x27;description&#x27;: &#x27;30元的百合，象征着坚定的爱情，带给你的是温暖而持久的情感！&#x27;, &#x27;reason&#x27;: &#x27;百合是象征爱情的花，写出这样的描述能让顾客更容易感受到百合所带来的爱意。&#x27;&#125;, </span><br><span class="line">&#123;&#x27;flower&#x27;: &#x27;康乃馨&#x27;, &#x27;price&#x27;: &#x27;20&#x27;, &#x27;description&#x27;: &#x27;This beautiful carnation is the perfect way to show your love and appreciation. Its vibrant pink color is sure to brighten up any room!&#x27;, &#x27;reason&#x27;: &#x27;The description is short, clear and appealing, emphasizing the beauty and color of the carnation while also invoking a sense of love and appreciation.&#x27;&#125;]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这段代码中，首先定义输出结构，我们希望模型生成的答案包含两部分：鲜花的描述文案（description）和撰写这个文案的原因（reason）。所以我们定义了一个名为 response_schemas 的列表，其中包含两个 ResponseSchema 对象，分别对应这两部分的输出。</p>
<p>根据这个列表，我通过 StructuredOutputParser.from_response_schemas 方法创建了一个输出解析器。</p>
<p>然后，我们通过输出解析器对象的 get_format_instructions() 方法获取输出的格式说明（format_instructions），再根据原始的字符串模板和输出解析器格式说明创建新的提示模板（这个模板就整合了输出解析结构信息）。再通过新的模板生成模型的输入，得到模型的输出。此时模型的输出结构将尽最大可能遵循我们的指示，以便于输出解析器进行解析。</p>
<p>对于每一个鲜花和价格组合，我们都用 output_parser.parse(output) 把模型输出的文案解析成之前定义好的数据格式，也就是一个 Python 字典，这个字典中包含了 description 和 reason 这两个字段的值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parsed_output</span><br><span class="line">&#123;&#x27;description&#x27;: &#x27;This 50-yuan rose is... feelings.&#x27;, &#x27;reason&#x27;: &#x27;The description is s...y emotion.&#x27;&#125;</span><br><span class="line">len(): 2</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>最后，把所有信息整合到一个 pandas DataFrame 对象中（需要安装 Pandas 库）。这个 DataFrame 对象中包含了 flower、price、description 和 reason 这四个字段的值。其中，description 和 reason 是由 output_parser 从模型的输出中解析出来的，flower 和 price 是我们自己添加的。</p>
<p>我们可以打印出 DataFrame 的内容，也方便地在程序中处理它，比如保存为下面的 CSV 文件。因为此时数据不再是模糊的、无结构的文本，而是结构清晰的有格式的数据。<strong>输出解析器在这个过程中的功劳很大</strong>。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8eb5d4eb72c2439397fba4d0fa5450c4~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1032&amp;h=97&amp;s=6999&amp;e=png&amp;b=faf9f9" alt=""></p>
<p>到这里，我们今天的任务也就顺利完成了。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>这样，你就从头到尾利用大模型开发出来了一个能够自动生成鲜花文案的应用程序！怎么样，是不是感觉和我们平时所做的基于 SQL 和数据库表以及固定业务逻辑的应用开发很不一样？</p>
<p>你看，每一次运行都有不同的结果，而我们完全不知道大模型下一次会给我们带来怎样的新东西。因此，基于大模型构建的应用可以说充满了创造力。</p>
<p>总结一下使用 LangChain 框架的好处，你会发现它有这样几个优势。</p>
<ol>
<li class="lvl-4">
<p>模板管理：在大型项目中，可能会有许多不同的提示模板，使用 LangChain 可以帮助你更好地管理这些模板，保持代码的清晰和整洁。</p>
</li>
<li class="lvl-4">
<p>变量提取和检查：LangChain 可以自动提取模板中的变量并进行检查，确保你没有忘记填充任何变量。</p>
</li>
<li class="lvl-4">
<p>模型切换：如果你想尝试使用不同的模型，只需要更改模型的名称就可以了，无需修改代码。</p>
</li>
<li class="lvl-4">
<p>输出解析：LangChain 的提示模板可以嵌入对输出格式的定义，以便在后续处理过程中比较方便地处理已经被格式化了的输出。</p>
</li>
</ol>
<p>在下节课中，我们将继续深入探索 LangChain 中的提示模板，看一看如何通过高质量的提示工程让模型创造出更为精准、更高质量的输出。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>请你用自己的理解，简述 LangChain 调用大语言模型来做应用开发的优势。</p>
</li>
<li class="lvl-4">
<p>在上面的示例中，format_instructions，也就是输出格式是怎样用 output_parser 构建出来的，又是怎样传递到提示模板中的？</p>
</li>
<li class="lvl-4">
<p>加入了 partial_variables，也就是输出解析器指定的 format_instructions 之后的提示，为什么能够让模型生成结构化的输出？你可以打印出这个提示，一探究竟。</p>
</li>
<li class="lvl-4">
<p>使用输出解析器后，调用模型时有没有可能仍然得不到所希望的输出？也就是说，模型有没有可能仍然返回格式不够完美的输出？</p>
</li>
</ol>
<p>题目较多，可以选择性思考，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>吴恩达老师的<a href="https://link.juejin.cn/?target=https%3A%2F%2Flearn.deeplearning.ai%2Flogin%3Fredirect_course%3Dchatgpt-prompt-eng" title="https://learn.deeplearning.ai/login?redirect_course=chatgpt-prompt-eng">提示工程课程</a>，吴老师也有 LangChain 的简单介绍课程呦！网上也有这些课程的中文翻译版！</p>
</li>
<li class="lvl-4">
<p>LangChain 官方文档中，关于模型 I/O 的资料<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fmodel_io%2F" title="https://python.langchain.com/docs/modules/model_io/">在此</a>。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E7%94%A8LangChain%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E2%80%9C%E6%98%93%E9%80%9F%E9%B2%9C%E8%8A%B1%E2%80%9D%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E7%94%A8LangChain%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8E%E2%80%9C%E6%98%93%E9%80%9F%E9%B2%9C%E8%8A%B1%E2%80%9D%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93%E7%9A%84%E6%99%BA%E8%83%BD%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:03:48" itemprop="dateModified" datetime="2024-11-18T19:03:48+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>在深入讲解 LangChain 的每一个具体组件之前，我想带着你从头完成一个很实用、很有意义的实战项目。目的就是让你直观感受一下 LangChain 作为一个基于大语言模型的应用开发框架，功能到底有多么强大。好的，现在就开始！</p>
<h2 id="项目及实现框架">项目及实现框架</h2>
<p>我们先来整体了解一下这个项目。</p>
<p><strong>项目名称</strong>：“易速鲜花” 内部员工知识库问答系统。</p>
<p><strong>项目介绍</strong>：“易速鲜花” 作为一个大型在线鲜花销售平台，有自己的业务流程和规范，也拥有针对员工的 SOP 手册。新员工入职培训时，会分享相关的信息。但是，这些信息分散于内部网和 HR 部门目录各处，有时不便查询；有时因为文档过于冗长，员工无法第一时间找到想要的内容；有时公司政策已更新，但是员工手头的文档还是旧版内容。</p>
<p>基于上述需求，我们将开发一套基于各种内部知识手册的 “Doc-QA” 系统。这个系统将充分利用 LangChain 框架，处理从员工手册中产生的各种问题。这个问答系统能够理解员工的问题，并基于最新的员工手册，给出精准的答案。</p>
<p><strong>开发框架</strong>：下面这张图片描述了通过 LangChain 框架实现一个知识库文档系统的整体框架。</p>
<p><img src="https://static001.geekbang.org/resource/image/c6/bf/c66995f1bf8575fb8fyye6293200eabf.jpg?wh=1393x697" alt=""></p>
<p>整个框架分为这样三个部分。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>数据源（Data Sources）：数据可以有很多种，包括 PDF 在内的非结构化的数据（Unstructured Data）、SQL 在内的结构化的数据（Structured Data），以及 Python、Java 之类的代码（Code）。在这个示例中，我们聚焦于对非结构化数据的处理。</p>
</li>
<li class="lvl-4">
<p>大模型应用（Application，即 LLM App）：以大模型为逻辑引擎，生成我们所需要的回答。</p>
</li>
<li class="lvl-4">
<p>用例（Use-Cases）：大模型生成的回答可以构建出 QA / 聊天机器人等系统。</p>
</li>
</ul>
<p><strong>核心实现机制：</strong> 这个项目的核心实现机制是下图所示的数据处理管道（Pipeline）。</p>
<p><img src="https://static001.geekbang.org/resource/image/73/87/73a46eecd42038961db9067e75de3387.jpg?wh=2509x799" alt=""></p>
<p>在这个管道的每一步中，LangChain 都为我们提供了相关工具，让你轻松实现基于文档的问答功能。</p>
<p>具体流程分为下面 5 步。</p>
<ol>
<li class="lvl-4">
<p>Loading：文档加载器把 Documents <strong>加载</strong>为以 LangChain 能够读取的形式。</p>
</li>
<li class="lvl-4">
<p>Splitting：文本分割器把 Documents <strong>切分</strong>为指定大小的分割，我把它们称为 “文档块” 或者“文档片”。</p>
</li>
<li class="lvl-4">
<p>Storage：将上一步中分割好的 “文档块” 以“嵌入”（Embedding）的形式<strong>存储</strong>到向量数据库（Vector DB）中，形成一个个的 “嵌入片”。</p>
</li>
<li class="lvl-4">
<p>Retrieval：应用程序从存储中<strong>检索</strong>分割后的文档（例如通过比较余弦相似度，找到与输入问题类似的嵌入片）。</p>
</li>
<li class="lvl-4">
<p>Output：把问题和相似的嵌入片传递给语言模型（LLM），使用包含问题和检索到的分割的提示<strong>生成答案</strong>。</p>
</li>
</ol>
<p>上面 5 个环节的介绍都非常简单，有些概念（如嵌入、向量存储）是第一次出现，理解起来需要一些背景知识，别着急，我们接下来具体讲解这 5 步。</p>
<h2 id="数据的准备和载入">数据的准备和载入</h2>
<p>“易速鲜花” 的内部资料包括 pdf、word 和 txt 格式的各种文件，我已经放在<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fhuangjia2019%2Flangchain%2Ftree%2Fmain%2F02_%25E6%2596%2587%25E6%25A1%25A3QA%25E7%25B3%25BB%25E7%25BB%259F" title="https://github.com/huangjia2019/langchain/tree/main/02_%E6%96%87%E6%A1%A3QA%E7%B3%BB%E7%BB%9F">这里</a>供你下载。</p>
<p><img src="https://static001.geekbang.org/resource/image/b6/ff/b69956a706112266df404eee953459ff.jpg?wh=402x224" alt=""></p>
<p>其中一个文档的示例如下：</p>
<p><img src="https://static001.geekbang.org/resource/image/93/24/931a55af4f0a3842a640d95c2c4bf224.jpg?wh=1630x1073" alt=""></p>
<p>我们首先用 LangChain 中的 document_loaders 来加载各种格式的文本文件。（这些文件我把它放在 OneFlower 这个目录中了，如果你创建自己的文件夹，就要调整一下代码中的目录。）</p>
<p>在这一步中，我们从 pdf、word 和 txt 文件中加载文本，然后将这些文本存储在一个列表中。（注意：可能需要安装 PyPDF、Docx2txt 等库）</p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;你的Open AI API Key&#x27;</span><br><span class="line"></span><br><span class="line"># 1.Load 导入Document Loaders</span><br><span class="line">from langchain.document_loaders import PyPDFLoader</span><br><span class="line">from langchain.document_loaders import Docx2txtLoader</span><br><span class="line">from langchain.document_loaders import TextLoader</span><br><span class="line"></span><br><span class="line"># 加载Documents</span><br><span class="line">base_dir = &#x27;.\OneFlower&#x27; # 文档的存放目录</span><br><span class="line">documents = []</span><br><span class="line">for file in os.listdir(base_dir): </span><br><span class="line">    # 构建完整的文件路径</span><br><span class="line">    file_path = os.path.join(base_dir, file)</span><br><span class="line">    if file.endswith(&#x27;.pdf&#x27;):</span><br><span class="line">        loader = PyPDFLoader(file_path)</span><br><span class="line">        documents.extend(loader.load())</span><br><span class="line">    elif file.endswith(&#x27;.docx&#x27;): </span><br><span class="line">        loader = Docx2txtLoader(file_path)</span><br><span class="line">        documents.extend(loader.load())</span><br><span class="line">    elif file.endswith(&#x27;.txt&#x27;):</span><br><span class="line">        loader = TextLoader(file_path)</span><br><span class="line">        documents.extend(loader.load())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里我们首先导入了 OpenAI 的 API Key。因为后面我们需要利用 Open AI 的两种不同模型做以下两件事：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>用 OpenAI 的 Embedding 模型为文档做嵌入。</p>
</li>
<li class="lvl-4">
<p>调用 OpenAI 的 GPT 模型来生成问答系统中的回答。</p>
</li>
</ul>
<p>当然了，LangChain 所支持的大模型绝不仅仅是 Open AI 而已，你完全可以遵循这个框架，把 Embedding 模型和负责生成回答的语言模型都替换为其他的开源模型。</p>
<p>在运行上面的程序时，除了要导入正确的 Open AI Key 之外，还要注意的是工具包的安装。使用 LangChain 时，根据具体的任务，往往需要各种不同的工具包（比如上面的代码需要 PyPDF 和 Docx2txt 工具）。它们安装起来都非常简单，如果程序报错缺少某个包，只要通过 <code>pip install</code> 安装相关包即可。</p>
<h2 id="文本的分割">文本的分割</h2>
<p>接下来需要将加载的文本分割成更小的块，以便进行嵌入和向量存储。这个步骤中，我们使用 LangChain 中的 RecursiveCharacterTextSplitter 来分割文本。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 2.Split 将Documents切分成块以便后续进行嵌入和向量存储</span><br><span class="line">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)</span><br><span class="line">chunked_documents = text_splitter.split_documents(documents)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在，我们的文档被切成了一个个 200 字符左右的文档块。这一步，是为把它们存储进下面的向量数据库做准备。</p>
<h2 id="向量数据库存储">向量数据库存储</h2>
<p>紧接着，我们将这些分割后的文本转换成嵌入的形式，并将其存储在一个向量数据库中。在这个例子中，我们使用了 OpenAIEmbeddings 来生成嵌入，然后使用 Qdrant 这个向量数据库来存储嵌入（这里需要 pip install qdrant-client）。</p>
<p>如果文本的 “嵌入” 这个概念对你来说有些陌生的话，你可以看一下下面的说明。</p>
<p>词嵌入（Word Embedding）是自然语言处理和机器学习中的一个概念，它将文字或词语转换为一系列数字，通常是一个向量。简单地说，词嵌入就是一个为每个词分配的数字列表。这些数字不是随机的，而是捕获了这个词的含义和它在文本中的上下文。因此，语义上相似或相关的词在这个数字空间中会比较接近。</p>
<p>举个例子，通过某种词嵌入技术，我们可能会得到： “国王” -&gt; [1.2, 0.5, 3.1, …] “皇帝” -&gt; [1.3, 0.6, 2.9, …] “苹果” -&gt; [0.9, -1.2, 0.3, …]</p>
<p>从这些向量中，我们可以看到 “国王” 和“皇帝”这两个词的向量在某种程度上是相似的，而与 “苹果” 这个词相比，它们的向量则相差很大，因为这两个概念在语义上是不同的。</p>
<p>词嵌入的优点是，它提供了一种将文本数据转化为计算机可以理解和处理的形式，同时保留了词语之间的语义关系。这在许多自然语言处理任务中都是非常有用的，比如文本分类、机器翻译和情感分析等。</p>
<p>你也可以对照下面的讲解学习一下向量数据库这个概念，它最近因为大语言模型的流行变得非常火爆。</p>
<p>向量数据库，也称为矢量数据库或者向量搜索引擎，是一种专门用于存储和搜索向量形式的数据的数据库。在众多的机器学习和人工智能应用中，尤其是自然语言处理和图像识别这类涉及大量非结构化数据的领域，将数据转化为高维度的向量是常见的处理方式。这些向量可能拥有数百甚至数千个维度，是对复杂的非结构化数据如文本、图像的一种数学表述，从而使这些数据能被机器理解和处理。然而，传统的关系型数据库在存储和查询如此高维度和复杂性的向量数据时，往往面临着效率和性能的问题。因此，向量数据库被设计出来以解决这一问题，它具备高效存储和处理高维向量数据的能力，从而更好地支持涉及非结构化数据处理的人工智能应用。</p>
<p><img src="https://static001.geekbang.org/resource/image/e3/16/e3c7e244b15f9527a4eb811e550a8f16.png?wh=2989x1805" alt=""></p>
<p>向量数据库有很多种，比如 Pinecone、Chroma 和 Qdrant，有些是收费的，有些则是开源的。</p>
<p>LangChain 中支持很多向量数据库，这里我们选择的是开源向量数据库 Qdrant。（注意，需要安装 qdrant-client）</p>
<p>具体实现代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 3.Store 将分割嵌入并存储在矢量数据库Qdrant中</span><br><span class="line">from langchain.vectorstores import Qdrant</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">vectorstore = Qdrant.from_documents(</span><br><span class="line">    documents=chunked_documents, # 以分块的文档</span><br><span class="line">    embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入</span><br><span class="line">    location=&quot;:memory:&quot;,  # in-memory 存储</span><br><span class="line">    collection_name=&quot;my_documents&quot;,) # 指定collection_name</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>目前，易速鲜花的所有内部文档，都以 “文档块嵌入片” 的格式被存储在向量数据库里面了。那么，我们只需要查询这个向量数据库，就可以找到大体上相关的信息了。</p>
<h2 id="相关信息的获取">相关信息的获取</h2>
<p>当内部文档存储到向量数据库之后，我们需要根据问题和任务来提取最相关的信息。此时，信息提取的基本方式就是把问题也转换为向量，然后去和向量数据库中的各个向量进行比较，提取最接近的信息。</p>
<p>向量之间的比较通常基于向量的距离或者相似度。在高维空间中，常用的向量距离或相似度计算方法有欧氏距离和余弦相似度。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p><strong>欧氏距离</strong>：这是最直接的距离度量方式，就像在二维平面上测量两点之间的直线距离那样。在高维空间中，两个向量的欧氏距离就是各个对应维度差的平方和的平方根。</p>
</li>
<li class="lvl-4">
<p><strong>余弦相似度</strong>：在很多情况下，我们更关心向量的方向而不是它的大小。例如在文本处理中，一个词的向量可能会因为文本长度的不同，而在大小上有很大的差距，但方向更能反映其语义。余弦相似度就是度量向量之间方向的相似性，它的值范围在 - 1 到 1 之间，值越接近 1，表示两个向量的方向越相似。</p>
</li>
</ul>
<p>这两种方法都被广泛应用于各种机器学习和人工智能任务中，选择哪一种方法取决于具体的应用场景。</p>
<p><img src="https://static001.geekbang.org/resource/image/32/7a/32db77431433da86d9f818037752bd7a.png?wh=1600x1320" alt=""></p>
<p>当然这里你肯定会问了，那么到底什么时候选择欧式距离，什么时候选择余弦相似度呢？</p>
<p>简单来说，关心数量等大小差异时用欧氏距离，关心文本等语义差异时用余弦相似度。</p>
<p>具体来说，欧氏距离度量的是绝对距离，它能很好地反映出向量的绝对差异。当我们关心数据的绝对大小，例如在物品推荐系统中，用户的购买量可能反映他们的偏好强度，此时可以考虑使用欧氏距离。同样，在数据集中各个向量的大小相似，且数据分布大致均匀时，使用欧氏距离也比较适合。</p>
<p>余弦相似度度量的是方向的相似性，它更关心的是两个向量的角度差异，而不是它们的大小差异。在处理文本数据或者其他高维稀疏数据的时候，余弦相似度特别有用。比如在信息检索和文本分类等任务中，文本数据往往被表示为高维的词向量，词向量的方向更能反映其语义相似性，此时可以使用余弦相似度。</p>
<p>在这里，我们正在处理的是文本数据，目标是建立一个问答系统，需要从语义上理解和比较问题可能的答案。因此，我建议使用余弦相似度作为度量标准。通过比较问题和答案向量在语义空间中的方向，可以找到与提出的问题最匹配的答案。</p>
<p>在这一步的代码部分，我们会创建一个聊天模型。然后需要创建一个 RetrievalQA 链，它是一个检索式问答模型，用于生成问题的答案。</p>
<p>在 RetrievalQA 链中有下面两大重要组成部分。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>LLM 是大模型，负责回答问题。</p>
</li>
<li class="lvl-4">
<p>retriever（vectorstore.as_retriever()）负责根据问题检索相关的文档，找到具体的 “嵌入片”。这些“嵌入片” 对应的 “文档块” 就会作为知识信息，和问题一起传递进入大模型。本地文档中检索而得的知识很重要，因为<strong>从互联网信息中训练而来的大模型不可能拥有 “易速鲜花” 作为一个私营企业的内部知识</strong>。</p>
</li>
</ul>
<p>具体代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 4. Retrieval 准备模型和Retrieval链</span><br><span class="line">import logging # 导入Logging工具</span><br><span class="line">from langchain.chat_models import ChatOpenAI # ChatOpenAI模型</span><br><span class="line">from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetriever工具</span><br><span class="line">from langchain.chains import RetrievalQA # RetrievalQA链</span><br><span class="line"></span><br><span class="line"># 设置Logging</span><br><span class="line">logging.basicConfig()</span><br><span class="line">logging.getLogger(&#x27;langchain.retrievers.multi_query&#x27;).setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line"># 实例化一个大模型工具 - OpenAI的GPT-3.5</span><br><span class="line">llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;, temperature=0)</span><br><span class="line"></span><br><span class="line"># 实例化一个MultiQueryRetriever</span><br><span class="line">retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)</span><br><span class="line"></span><br><span class="line"># 实例化一个RetrievalQA链</span><br><span class="line">qa_chain = RetrievalQA.from_chain_type(llm,retriever=retriever_from_llm)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>现在我们已经为后续的步骤做好了准备，下一步就是接收来自系统用户的具体问题，并根据问题检索信息，生成回答。</p>
<h2 id="生成回答并展示">生成回答并展示</h2>
<p>这一步是问答系统应用的主要 UI 交互部分，这里会创建一个 Flask 应用（需要安装 Flask 包）来接收用户的问题，并生成相应的答案，最后通过 index.html 对答案进行渲染和呈现。</p>
<p>在这个步骤中，我们使用了之前创建的 RetrievalQA 链来获取相关的文档和生成答案。然后，将这些信息返回给用户，显示在网页上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 5. Output 问答系统的UI实现</span><br><span class="line">from flask import Flask, request, render_template</span><br><span class="line">app = Flask(__name__) # Flask APP</span><br><span class="line"></span><br><span class="line">@app.route(&#x27;/&#x27;, methods=[&#x27;GET&#x27;, &#x27;POST&#x27;])</span><br><span class="line">def home():</span><br><span class="line">    if request.method == &#x27;POST&#x27;:</span><br><span class="line"></span><br><span class="line">        # 接收用户输入作为问题</span><br><span class="line">        question = request.form.get(&#x27;question&#x27;)        </span><br><span class="line">        </span><br><span class="line">        # RetrievalQA链 - 读入问题，生成答案</span><br><span class="line">        result = qa_chain(&#123;&quot;query&quot;: question&#125;)</span><br><span class="line">        </span><br><span class="line">        # 把大模型的回答结果返回网页进行渲染</span><br><span class="line">        return render_template(&#x27;index.html&#x27;, result=result)</span><br><span class="line">    </span><br><span class="line">    return render_template(&#x27;index.html&#x27;)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    app.run(host=&#x27;0.0.0.0&#x27;,debug=True,port=5000)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>相关 HTML 网页的关键代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;body&gt;</span><br><span class="line">    &lt;div&gt;</span><br><span class="line">        &lt;div&gt;</span><br><span class="line">            &lt;h1&gt;易速鲜花内部问答系统&lt;/h1&gt;</span><br><span class="line">            &lt;img src=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;flower.png&#x27;) &#125;&#125;&quot; alt=&quot;flower logo&quot; width=&quot;200&quot;&gt; </span><br><span class="line">        &lt;/div&gt;</span><br><span class="line">        &lt;form method=&quot;POST&quot;&gt;</span><br><span class="line">            &lt;label for=&quot;question&quot;&gt;Enter your question:&lt;/label&gt;&lt;br&gt;</span><br><span class="line">            &lt;input type=&quot;text&quot; name=&quot;question&quot;&gt;&lt;br&gt;</span><br><span class="line">            &lt;input type=&quot;submit&quot; value=&quot;Submit&quot;&gt;</span><br><span class="line">        &lt;/form&gt;</span><br><span class="line">        &#123;% if result is defined %&#125;</span><br><span class="line">            &lt;h2&gt;Answer&lt;/h2&gt;</span><br><span class="line">            &lt;p&gt;&#123;&#123; result.result &#125;&#125;&lt;/p&gt;</span><br><span class="line">        &#123;% endif %&#125;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个项目的目录结构如下：</p>
<p><img src="https://static001.geekbang.org/resource/image/21/3e/2110cd73ddb8677f9b188d41c589c73e.png?wh=710x465" alt=""></p>
<p>运行程序之后，我们跑起一个网页 <a href="https://link.juejin.cn/?target=http%3A%2F%2F127.0.0.1%3A5000%2F" title="http://127.0.0.1:5000/">http://127.0.0.1:5000/</a>。与网页进行互动时，可以发现，问答系统完美生成了专属于异速鲜花内部资料的回答。</p>
<p><img src="https://static001.geekbang.org/resource/image/46/17/46b5b08c5f022f2c4c5975436b3e2d17.png?wh=567x338" alt=""></p>
<h2 id="总结时刻">总结时刻</h2>
<p>来回顾一下上面的流程。正如下图所示，我们先把本地知识切片后做 Embedding，存储到向量数据库中，然后把用户的输入和从向量数据库中检索到的本地知识传递给大模型，最终生成所想要的回答。</p>
<p><img src="https://static001.geekbang.org/resource/image/24/af/249c631211275e40f3e72d05dda976af.jpg?wh=2523x1058" alt=""></p>
<p>怎么样，你是不是觉得整个流程特别简单易懂？</p>
<p>对了，LangChain+LLM 的配置就是使原本复杂的东西变得特别简单，特别易于操作。而这个任务，在大模型和 LangChain 出现之前，要实现起来可不是这么简单的。</p>
<p>如果这个示例让你了解到了 LangChain 的威力，那么这节课的目标也就完成了。除了流程图的回顾，我也为你准备了一个详细版的脑图，你可以对照着复习。</p>
<p><img src="https://static001.geekbang.org/resource/image/78/c2/78a4b0435639b4db8c4e024d830a2ac2.jpg?wh=2482x1434" alt=""></p>
<p>在后面几节课中，我们即将对 LangChain 的模型、链、内存、代理等组件进行详细拆解，我会带着你实现更多任务，开发出更奇妙的应用。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>请你用自己的话简述一下这个基于文档的 QA（问答）系统的实现流程？</p>
</li>
<li class="lvl-4">
<p>LangChain 支持很多种向量数据库，你能否用另一种常用的向量数据库 Chroma 来实现这个任务？</p>
</li>
<li class="lvl-4">
<p>LangChain 支持很多种大语言模型，你能否用 HuggingFace 网站提供的开源模型 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2Fgoogle%2Fflan-t5-xl" title="https://huggingface.co/google/flan-t5-xl">google/flan-t5-x1</a> 代替 GPT-3.5 完成这个任务？</p>
</li>
</ol>
<p>题目较多，可以选择性尝试，期待在留言区看到你的分享。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>LangChain 官方文档对 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fdocs.langchain.com%2Fdocs%2Fuse-cases%2Fqa-docs" title="https://docs.langchain.com/docs/use-cases/qa-docs">Document QA 系统</a>设计及实现的详细说明</p>
</li>
<li class="lvl-4">
<p>HuggingFace 官网上的<a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2Ftasks%2Fdocument-question-answering" title="https://huggingface.co/tasks/document-question-answering">文档问答</a>资源</p>
</li>
<li class="lvl-4">
<p>论文<a href="https://link.juejin.cn/?target=https%3A%2F%2Farxiv.org%2Fabs%2F2010.10439" title="https://arxiv.org/abs/2010.10439">开放式表格与文本问题回答</a>，Chen, W., Chang, M.-W., Schlinger, E., Wang, W., &amp; Cohen, W. W. (2021). Open Question Answering over Tables and Text. ICLR 2021.</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BD%BF%E7%94%A8OpenAI%20API%E8%BF%98%E6%98%AF%E5%BE%AE%E8%B0%83%E5%BC%80%E6%BA%90Llama2%E3%80%81ChatGLM%EF%BC%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E8%B0%83%E7%94%A8%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BD%BF%E7%94%A8OpenAI%20API%E8%BF%98%E6%98%AF%E5%BE%AE%E8%B0%83%E5%BC%80%E6%BA%90Llama2%E3%80%81ChatGLM%EF%BC%9F/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:07:45" itemprop="dateModified" datetime="2024-11-18T19:07:45+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>之前，我们花了两节课的内容讲透了提示工程的原理以及 LangChain 中的具体使用方式。今天，我们来着重讨论 Model I/O 中的第二个子模块，LLM。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aa2e629d7bbd42e791b73ead6dabc98b~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=4000&amp;h=1536&amp;s=504459&amp;e=jpg&amp;b=eefcf1" alt=""></p>
<p>让我们带着下面的问题来开始这一节课的学习。大语言模型，不止 ChatGPT 一种。调用 OpenAI 的 API，当然方便且高效，不过，如果我就是想用其他的模型（比如说开源的 Llama2 或者 ChatGLM），该怎么做？再进一步，如果我就是想在本机上从头训练出来一个新模型，然后在 LangChain 中使用自己的模型，又该怎么做？</p>
<p>关于大模型的微调（或称精调）、预训练、重新训练、乃至从头训练，这是一个相当大的话题，不仅仅需要足够的知识和经验，还需要大量的语料数据、GPU 硬件和强大的工程能力。别说一节课了，我想两三个专栏也不一定能讲全讲透。不过，我可以提纲挈领地把大模型的训练流程和使用方法给你缕一缕。这样你就能体验到，在 LangChain 中使用自己微调的模型是完全没问题的。</p>
<h2 id="大语言模型发展史">大语言模型发展史</h2>
<p>说到语言模型，我们不妨先从其发展史中去了解一些关键信息。</p>
<p>Google 2018 年的论文名篇 Attention is all you need，提出了 Transformer 架构，也给这一次 AI 的腾飞点了火。Transformer 是几乎所有预训练模型的核心底层架构。基于 Transformer 预训练所得的大规模语言模型也被叫做 “基础模型”（Foundation Model 或 Base Model）。</p>
<p>在这个过程中，模型学习了词汇、语法、句子结构以及上下文信息等丰富的语言知识。这种在大量数据上学到的知识，为后续的下游任务（如情感分析、文本分类、命名实体识别、问答系统等）提供了一个通用的、丰富的语言表示基础，为解决许多复杂的 NLP 问题提供了可能。</p>
<p>在预训练模型出现的早期，BERT 毫无疑问是最具代表性的，也是影响力最大的模型。BERT 通过同时学习文本的前向和后向上下文信息，实现对句子结构的深入理解。BERT 之后，各种大型预训练模型如雨后春笋般地涌现，自然语言处理（NLP）领域进入了一个新时代。这些模型推动了 NLP 技术的快速发展，解决了许多以前难以应对的问题，比如翻译、文本总结、聊天对话等等，提供了强大的工具。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/de0a990946884334bb7725e30bd73407~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=10666&amp;h=4300&amp;s=876799&amp;e=jpg&amp;b=ffffff" alt=""></p>
<p>当然，现今的预训练模型的趋势是参数越来越多，模型也越来越大，训练一次的费用可达几百万美元。这样大的开销和资源的耗费，只有世界顶级大厂才能够负担得起，普通的学术组织和高等院校很难在这个领域继续引领科技突破，这种现象开始被普通研究人员所诟病。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0ef96d0e60864b6a8642d1a5ffbe73e4~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=10666&amp;h=6000&amp;s=954543&amp;e=jpg&amp;b=ffffff" alt=""></p>
<h2 id="预训练-微调的模式">预训练 + 微调的模式</h2>
<p>不过，话虽如此，大型预训练模型的确是工程师的福音。因为，经过预训练的大模型中所习得的语义信息和所蕴含的语言知识，能够非常容易地向下游任务迁移。NLP 应用人员可以对模型的头部或者部分参数根据自己的需要进行适应性的调整，这通常涉及在相对较小的有标注数据集上进行有监督学习，让模型适应特定任务的需求。</p>
<p>这就是对预训练模型的微调（Fine-tuning）。微调过程相比于从头训练一个模型要快得多，且需要的数据量也要少得多，这使得作为工程师的我们能够更高效地开发和部署各种 NLP 解决方案。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/77de71b3192c4214a06eb6fae70365db~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=10666&amp;h=5061&amp;s=840274&amp;e=jpg&amp;b=ffffff" alt=""></p>
<p>图中的 “具体任务”，其实也可以更换为 “具体领域”。那么总结来说：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p><strong>预训练</strong>：在大规模无标注文本数据上进行模型的训练，目标是让模型学习自然语言的基础表达、上下文信息和语义知识，为后续任务提供一个通用的、丰富的语言表示基础。</p>
</li>
<li class="lvl-4">
<p><strong>微调</strong>：在预训练模型的基础上，可以根据特定的下游任务对模型进行微调。现在你经常会听到各行各业的人说：<em>我们的优势就是领域知识嘛！我们比不过国内外大模型，我们可以拿开源模型做垂直领域嘛！做垂类模型！</em>—— 啥叫垂类？指的其实就是根据领域数据微调开源模型这件事儿。</p>
</li>
</ul>
<p>这种预训练 + 微调的大模型应用模式优势明显。首先，预训练模型能够将大量的通用语言知识迁移到各种下游任务上，作为应用人员，我们不需要自己寻找语料库，从头开始训练大模型，这减少了训练时间和数据需求；其次，微调过程可以快速地根据特定任务进行优化，简化了模型部署的难度；最后，预训练 + 微调的架构具有很强的可扩展性，可以方便地应用于各种自然语言处理任务，大大提高了 NLP 技术在实际应用中的可用性和普及程度，给我们带来了巨大的便利。</p>
<p>好，下面咱们开始一步步地使用开源模型。今天我要带你玩的模型主要是 Meta（Facebook）推出的 Llama2。当然你可以去 Llama 的官网下载模型，然后通过 Llama 官方 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fllama" title="https://github.com/facebookresearch/llama">GitHub</a> 中提供的方法来调用它。但是，我还是会推荐你从 HuggingFace 下载并导入模型。因为啊，前天百川，昨天千问，今天流行 Llama，明天不就流行别的了嘛。模型总在变，但是 HuggingFace 一直在那里，支持着各种开源模型。我们学东西，尽量选择学一次能够复用的知识。</p>
<h2 id="用-HuggingFace-跑开源模型">用 HuggingFace 跑开源模型</h2>
<h3 id="注册并安装-HuggingFace">注册并安装 HuggingFace</h3>
<p>第一步，还是要登录 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2F" title="https://huggingface.co/">HuggingFace</a> 网站，并拿到专属于你的 Token。（如果你做了前面几节课的实战案例，那么你应该已经有这个 API Token 了）</p>
<p>第二步，用 <code>pip install transformers</code> 安装 HuggingFace Library。详见<a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Finstallation" title="https://huggingface.co/docs/transformers/installation">这里</a>。</p>
<p>第三步，在命令行中运行 <code>huggingface-cli login</code>，设置你的 API Token。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dd94f10a986f4df0920bebaf8100fc1b~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1475&amp;h=668&amp;s=501685&amp;e=png&amp;b=fefcfc" alt=""></p>
<p>当然，也可以在程序中设置你的 API Token，但是这不如在命令行中设置来得安全。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 导入HuggingFace API Token</span><br><span class="line">import os</span><br><span class="line">os.environ[&#x27;HUGGINGFACEHUB_API_TOKEN&#x27;] = &#x27;你的HuggingFace API Token&#x27;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="申请使用-Meta-的-Llama2-模型">申请使用 Meta 的 Llama2 模型</h3>
<p>在 HuggingFace 的 Model 中，找到 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FLlama-2-7b" title="https://huggingface.co/meta-llama/Llama-2-7b">meta-llama/Llama-2-7b</a>。注意，各种各样版本的 Llama2 模型多如牛毛，我们这里用的是最小的 7B 版。此外，还有 13b\70b\chat 版以及各种各样的非 Meta 官方版。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/136e9035944547739db7296b63910ead~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=3028&amp;h=710&amp;s=375013&amp;e=png&amp;b=fef8f7" alt=""></p>
<p>选择 meta-llama/Llama-2-7b 这个模型后，你能够看到这个模型的基本信息。如果你是第一次用 Llama，你需要申请 Access，因为我已经申请过了，所以屏幕中间有句话：“You have been granted access to this model”。从申请到批准，大概是几分钟的事儿。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/018cde81ef93445293b5949b529c4a4f~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=3029&amp;h=1662&amp;s=392474&amp;e=png&amp;b=fffdfd" alt=""></p>
<h3 id="通过-HuggingFace-调用-Llama">通过 HuggingFace 调用 Llama</h3>
<p>好，万事俱备，现在我们可以使用 HuggingFace 的 Transformers 库来调用 Llama 啦！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 导入必要的库</span><br><span class="line">from transformers import AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"># 加载预训练模型的分词器</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Llama-2-7b-chat-hf&quot;)</span><br><span class="line"></span><br><span class="line"># 加载预训练的模型</span><br><span class="line"># 使用 device_map 参数将模型自动加载到可用的硬件设备上，例如GPU</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">          &quot;meta-llama/Llama-2-7b-chat-hf&quot;, </span><br><span class="line">          device_map = &#x27;auto&#x27;)  </span><br><span class="line"></span><br><span class="line"># 定义一个提示，希望模型基于此提示生成故事</span><br><span class="line">prompt = &quot;请给我讲个玫瑰的爱情故事?&quot;</span><br><span class="line"></span><br><span class="line"># 使用分词器将提示转化为模型可以理解的格式，并将其移动到GPU上</span><br><span class="line">inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)</span><br><span class="line"></span><br><span class="line"># 使用模型生成文本，设置最大生成令牌数为2000</span><br><span class="line">outputs = model.generate(inputs[&quot;input_ids&quot;], max_new_tokens=2000)</span><br><span class="line"></span><br><span class="line"># 将生成的令牌解码成文本，并跳过任何特殊的令牌，例如[CLS], [SEP]等</span><br><span class="line">response = tokenizer.decode(outputs[0], skip_special_tokens=True)</span><br><span class="line"></span><br><span class="line"># 打印生成的响应</span><br><span class="line">print(response)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这段程序是一个很典型的 HuggingFace 的 Transformers 库的用例，该库提供了大量预训练的模型和相关的工具。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>导入 AutoTokenizer：这是一个用于自动加载预训练模型的相关分词器的工具。分词器负责将文本转化为模型可以理解的数字格式。</p>
</li>
<li class="lvl-4">
<p>导入 AutoModelForCausalLM：这是用于加载因果语言模型（用于文本生成）的工具。</p>
</li>
<li class="lvl-4">
<p>使用 from_pretrained 方法来加载预训练的分词器和模型。其中，<code>device_map = 'auto'</code> 是为了自动地将模型加载到可用的设备上，例如 GPU。</p>
</li>
<li class="lvl-4">
<p>然后，给定一个提示（prompt）：<code>&quot;请给我讲个玫瑰的爱情故事?&quot;</code>，并使用分词器将该提示转换为模型可以接受的格式，<code>return_tensors=&quot;pt&quot;</code> 表示返回 PyTorch 张量。语句中的 <code>.to(&quot;cuda&quot;)</code> 是 GPU 设备格式转换，因为我在 GPU 上跑程序，不用这个的话会报错，如果你使用 CPU，可以试一下删掉它。</p>
</li>
<li class="lvl-4">
<p>最后使用模型的 <code>.generate()</code> 方法生成响应。<code>max_new_tokens=2000</code> 限制生成的文本的长度。使用分词器的 <code>.decode()</code> 方法将输出的数字转化回文本，并且跳过任何特殊的标记。</p>
</li>
</ul>
<p>因为是在本地进行推理，耗时时间比较久。在我的机器上，大概需要 30s～2min 产生结果。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7236954938cb4330883275f0dfff4c5f~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=1653&amp;h=651&amp;s=207641&amp;e=png&amp;b=fefdfd" alt=""></p>
<p>这样的回答肯定不能直接用做商业文案，而且，我的意思是玫瑰花相关的故事，它明显把玫瑰理解成一个女孩的名字了。所以，开源模型，尤其是 7B 的小模型和 Open AI 的 ChatGPT 还是有一定差距的。</p>
<h2 id="LangChain-和-HuggingFace-的接口">LangChain 和 HuggingFace 的接口</h2>
<p>讲了半天，LangChain 未出场。下面让我们看一看，如何把 HuggingFace 里面的模型接入 LangChain。</p>
<h3 id="通过-HuggingFace-Hub">通过 HuggingFace Hub</h3>
<p>第一种集成方式，是通过 HuggingFace Hub。HuggingFace Hub 是一个开源模型中心化存储库，主要用于分享、协作和存储预训练模型、数据集以及相关组件。</p>
<p>我们给出一个 HuggingFace Hub 和 LangChain 集成的代码示例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># 导入HuggingFace API Token</span><br><span class="line">import os</span><br><span class="line">os.environ[&#x27;HUGGINGFACEHUB_API_TOKEN&#x27;] = &#x27;你的HuggingFace API Token&#x27;</span><br><span class="line"></span><br><span class="line"># 导入必要的库</span><br><span class="line">from langchain import PromptTemplate, HuggingFaceHub, LLMChain</span><br><span class="line"></span><br><span class="line"># 初始化HF LLM</span><br><span class="line">llm = HuggingFaceHub(</span><br><span class="line">    repo_id=&quot;google/flan-t5-small&quot;,</span><br><span class="line">    #repo_id=&quot;meta-llama/Llama-2-7b-chat-hf&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建简单的question-answering提示模板</span><br><span class="line">template = &quot;&quot;&quot;Question: &#123;question&#125;</span><br><span class="line">              Answer: &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 创建Prompt          </span><br><span class="line">prompt = PromptTemplate(template=template, input_variables=[&quot;question&quot;])</span><br><span class="line"></span><br><span class="line"># 调用LLM Chain --- 我们以后会详细讲LLM Chain</span><br><span class="line">llm_chain = LLMChain(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    llm=llm</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 准备问题</span><br><span class="line">question = &quot;Rose is which type of flower?&quot;</span><br><span class="line"></span><br><span class="line"># 调用模型并返回结果</span><br><span class="line">print(llm_chain.run(question))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>可以看出，这个集成过程非常简单，只需要在 HuggingFaceHub 类的 repo_id 中指定模型名称，就可以直接下载并使用模型，模型会自动下载到 HuggingFace 的 Cache 目录，并不需要手工下载。</p>
<p>初始化 LLM，创建提示模板，生成提示的过程，你已经很熟悉了。这段代码中有一个新内容是我通过 llm_chain 来调用了 LLM。这段代码也不难理解，有关 Chain 的概念我们以后还会详述。</p>
<p>不过，我尝试使用 meta-llama/Llama-2-7b-chat-hf 这个模型时，出现了错误，因此我只好用比较旧的模型做测试。我随便选择了 google/flan-t5-small，问了它一个很简单的问题，想看看它是否知道玫瑰是哪一种花。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e211313253dd4ab882989b31e9e6a60a~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=547&amp;h=149&amp;s=13256&amp;e=png&amp;a=1&amp;b=ffffff" alt=""></p>
<p>模型告诉我，玫瑰是花。对，答案只有一个字，flower。这… 不得不说，2023 年之前的模型，和 2023 年之后的模型，水平没得比。以前的模型能说话就不错了。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ce02bf5715db48fd862fba0f1407a29b~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=578&amp;h=92&amp;s=6429&amp;e=png&amp;b=f7f7f8" alt=""></p>
<h3 id="通过-HuggingFace-Pipeline">通过 HuggingFace Pipeline</h3>
<p>既然 HuggingFace Hub 还不能完成 Llama-2 的测试，让我们来尝试另外一种方法，HuggingFace Pipeline。HuggingFace 的 Pipeline 是一种高级工具，它简化了多种常见自然语言处理（NLP）任务的使用流程，使得用户不需要深入了解模型细节，也能够很容易地利用预训练模型来做任务。</p>
<p>让我来看看下面的示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># 指定预训练模型的名称</span><br><span class="line">model = &quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><br><span class="line"></span><br><span class="line"># 从预训练模型中加载词汇器</span><br><span class="line">from transformers import AutoTokenizer</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model)</span><br><span class="line"></span><br><span class="line"># 创建一个文本生成的管道</span><br><span class="line">import transformers</span><br><span class="line">import torch</span><br><span class="line">pipeline = transformers.pipeline(</span><br><span class="line">    &quot;text-generation&quot;,</span><br><span class="line">    model=model,</span><br><span class="line">    torch_dtype=torch.float16,</span><br><span class="line">    device_map=&quot;auto&quot;,</span><br><span class="line">    max_length = 1000</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建HuggingFacePipeline实例</span><br><span class="line">from langchain import HuggingFacePipeline</span><br><span class="line">llm = HuggingFacePipeline(pipeline = pipeline, </span><br><span class="line">                          model_kwargs = &#123;&#x27;temperature&#x27;:0&#125;)</span><br><span class="line"></span><br><span class="line"># 定义输入模板，该模板用于生成花束的描述</span><br><span class="line">template = &quot;&quot;&quot;</span><br><span class="line">              为以下的花束生成一个详细且吸引人的描述：</span><br><span class="line">              花束的详细信息：</span><br><span class="line">              ```&#123;flower_details&#125;```</span><br><span class="line">           &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># 使用模板创建提示</span><br><span class="line">from langchain import PromptTemplate,  LLMChain</span><br><span class="line">prompt = PromptTemplate(template=template, </span><br><span class="line">                     input_variables=[&quot;flower_details&quot;])</span><br><span class="line"></span><br><span class="line"># 创建LLMChain实例</span><br><span class="line">from langchain import PromptTemplate</span><br><span class="line">llm_chain = LLMChain(prompt=prompt, llm=llm)</span><br><span class="line"></span><br><span class="line"># 需要生成描述的花束的详细信息</span><br><span class="line">flower_details = &quot;12支红玫瑰，搭配白色满天星和绿叶，包装在浪漫的红色纸中。&quot;</span><br><span class="line"></span><br><span class="line"># 打印生成的花束描述</span><br><span class="line">print(llm_chain.run(flower_details))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里简单介绍一下代码中使用到的 transformers pipeline 的配置参数。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9bbf2c4cc7064061b3637b82b296150c~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=2490&amp;h=1034&amp;s=375096&amp;e=jpg&amp;b=fafbfe" alt=""></p>
<p>生成的结果之一如下：</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/07a642bb691a4ec5afd14cbd3967fde6~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=6396&amp;h=3569&amp;s=1095318&amp;e=jpg&amp;b=fbf8f8" alt=""></p>
<p>此结果不敢恭维。但是，后续的测试告诉我，这很有可能是 7B 这个模型太小，尽管有形成中文的相应能力，但是能力不够强大，也就导致了这样的结果。</p>
<p>至此，通过 HuggingFace 接口调用各种开源模型的尝试成功结束。下面，我们进行最后一个测试，看看 LangChain 到底能否直接调用本地模型。</p>
<h2 id="用-LangChain-调用自定义语言模型">用 LangChain 调用自定义语言模型</h2>
<p>最后，我们来尝试回答这节课开头提出的问题，假设你就是想训练属于自己的模型。而且出于商业秘密的原因，不想开源它，不想上传到 HuggingFace，就是要在本机运行模型。此时应该如何利用 LangChain 的功能？</p>
<p>我们可以创建一个 LLM 的衍生类，自己定义模型。而 LLM 这个基类，则位于 langchain.llms.base 中，通过 from langchain.llms.base import LLM 语句导入。</p>
<p>这个自定义的 LLM 类只需要实现一个方法：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>_call 方法：用于接收输入字符串并返回响应字符串。</p>
</li>
</ul>
<p>以及一个可选方法：</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>_identifying_params 方法：用于帮助打印此类的属性。</p>
</li>
</ul>
<p>下面，让我们先从 HuggingFace 的<a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2FTheBloke%2FLlama-2-7B-Chat-GGML%2Ftree%2Fmain" title="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main">这里</a>，下载一个 llama-2-7b-chat.ggmlv3.q4_K_S.bin 模型，并保存在本地。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0d64e046d393422792a57d00d40d2d71~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=3879&amp;h=1976&amp;s=572980&amp;e=png&amp;b=fffdfd" alt=""></p>
<p>你可能会质疑我，不是说自己训练，自己微调，不再用 HuggingFace 了吗？</p>
<p>不好意思，容许我解释一下。自己训练一个能用的模型没那么容易。这个模型，它并不是原始的 Llama 模型，而是 TheBloke 这位老兄用他的手段为我们量化过的新模型，你也可以理解成，他已经为我们压缩或者说微调了 Llama 模型。</p>
<p>量化是 AI 模型大小和性能优化的常用技术，它将模型的权重简化到较少的位数，以减少模型的大小和计算需求，让大模型甚至能够在 CPU 上面运行。当你看到模型的后缀有 GGML 或者 GPTQ，就说明模型已经被量化过，其中 GPTQ 是一种仅适用于 GPU 的特定格式。GGML 专为 CPU 和 Apple M 系列设计，但也可以加速 GPU 上的某些层。llama-cpp-python 这个包就是为了实现 GGML 而制作的。</p>
<p>所以，这里你就假设，咱们下载下来的 llama-2-7b-chat.ggmlv3.q4_K_S.bin 这个模型，就是你自己微调过的。将来你真的微调了 Llama2、ChatGLM、百川或者千问的开源版，甚至是自己从头训练了一个 mini-ChatGPT，你也可以保存为 you_own_model.bin 的格式，就按照下面的方式加载到 LangChain 之中。</p>
<p>然后，为了使用 llama-2-7b-chat.ggmlv3.q4_K_S.bin 这个模型，你需要安装 pip install llama-cpp-python 这个包。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 导入需要的库</span><br><span class="line">from llama_cpp import Llama</span><br><span class="line">from typing import Optional, List, Mapping, Any</span><br><span class="line">from langchain.llms.base import LLM</span><br><span class="line"></span><br><span class="line"># 模型的名称和路径常量</span><br><span class="line">MODEL_NAME = &#x27;llama-2-7b-chat.ggmlv3.q4_K_S.bin&#x27;</span><br><span class="line">MODEL_PATH = &#x27;/home/huangj/03_Llama/&#x27;</span><br><span class="line"></span><br><span class="line"># 自定义的LLM类，继承自基础LLM类</span><br><span class="line">class CustomLLM(LLM):</span><br><span class="line">    model_name = MODEL_NAME</span><br><span class="line"></span><br><span class="line">    # 该方法使用Llama库调用模型生成回复</span><br><span class="line">    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:</span><br><span class="line">        prompt_length = len(prompt) + 5</span><br><span class="line">        # 初始化Llama模型，指定模型路径和线程数</span><br><span class="line">        llm = Llama(model_path=MODEL_PATH+MODEL_NAME, n_threads=4)</span><br><span class="line">        # 使用Llama模型生成回复</span><br><span class="line">        response = llm(f&quot;Q: &#123;prompt&#125; A: &quot;, max_tokens=256)</span><br><span class="line">        </span><br><span class="line">        # 从返回的回复中提取文本部分</span><br><span class="line">        output = response[&#x27;choices&#x27;][0][&#x27;text&#x27;].replace(&#x27;A: &#x27;, &#x27;&#x27;).strip()</span><br><span class="line"></span><br><span class="line">        # 返回生成的回复，同时剔除了问题部分和额外字符</span><br><span class="line">        return output[prompt_length:]</span><br><span class="line"></span><br><span class="line">    # 返回模型的标识参数，这里只是返回模型的名称</span><br><span class="line">    @property</span><br><span class="line">    def _identifying_params(self) -&gt; Mapping[str, Any]:</span><br><span class="line">        return &#123;&quot;name_of_model&quot;: self.model_name&#125;</span><br><span class="line"></span><br><span class="line">    # 返回模型的类型，这里是&quot;custom&quot;</span><br><span class="line">    @property</span><br><span class="line">    def _llm_type(self) -&gt; str:</span><br><span class="line">        return &quot;custom&quot;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 初始化自定义LLM类</span><br><span class="line">llm = CustomLLM()</span><br><span class="line"></span><br><span class="line"># 使用自定义LLM生成一个回复</span><br><span class="line">result = llm(&quot;昨天有一个客户抱怨他买了花给女朋友之后，两天花就枯了，你说作为客服我应该怎么解释？&quot;)</span><br><span class="line"></span><br><span class="line"># 打印生成的回复</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>代码中需要解释的内容不多，基本上就是 CustomLLM 类的构建和使用，类内部通过 Llama 类来实现大模型的推理功能，然后直接返回模型的回答。</p>
<p><img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a36e095e365f45cd9ab84d36fcfc510f~tplv-k3u1fbpfcp-jj-mark:2268:0:0:0:q75.awebp#?w=6369&amp;h=1322&amp;s=561228&amp;e=jpg&amp;b=fbfafa" alt=""></p>
<p>似乎 Llama 经过量化之后，虽然仍读得懂中文，但是不会讲中文了。</p>
<p>翻译成中文，他的回答是这样的。</p>
<p><em>当客户抱怨他们为女朋友买的花在两天内就枯萎了，我会以礼貌和专业的方式这样解释：</em></p>
<p><em>“感谢您把这个问题告诉我们。对于给您带来的任何不便，我深感抱歉。有可能这些花没有被正确地存储或照料，这可能影响了它们的生命期。我们始终以提供高质量的产品为荣，但有时可能会出现意外的问题。请您知道，我们非常重视您的满意度并随时为您提供帮助。您希望我为您提供替换或退款吗？”</em></p>
<p>看上去，除了中文能力不大灵光之外，Llama2 的英文表现真的非常完美，和 GPT3.5 差距不是很大，要知道：</p>
<ol>
<li class="lvl-4">
<p>这可是开源模型，而且是允许商业的免费模型。</p>
</li>
<li class="lvl-4">
<p>这是在本机 CPU 的环境下运行的，模型的推理速度还是可以接受的。</p>
</li>
<li class="lvl-4">
<p>这仅仅是 Llama 的最小版本，也就是 7B 的量化版，就达到了这么好的效果。</p>
</li>
</ol>
<p>基于上述三点原因，我给 Llama2 打 98.5 分。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>今天的课程到此就结束了，相信你学到了很多新东西吧。的确，进入大模型开发这个领域，就好像打开了通往新世界的一扇门，有太多的新知识，等待着你去探索。</p>
<p>现在，你已经知道大模型训练涉及在大量数据上使用深度学习算法，通常需要大量计算资源和时间。训练后，模型可能不完全适合特定任务，因此需要微调，即在特定数据集上继续训练，以使模型更适应该任务。为了减小部署模型的大小和加快推理速度，模型还会经过量化，即将模型参数从高精度格式减少到较低精度。</p>
<p>如果你想继续深入学习大模型，那么有几个工具你不得不接着研究。</p>
<ul class="lvl-0">
<li class="lvl-4">
<p>PyTorch 是一个流行的深度学习框架，常用于模型的训练和微调。</p>
</li>
<li class="lvl-4">
<p>HuggingFace 是一个开源社区，提供了大量预训练模型和微调工具，尤其是 NLP 任务。</p>
</li>
<li class="lvl-4">
<p>LangChain 则擅长于利用大语言模型的推理功能，开发新的工具或应用，完成特定的任务。</p>
</li>
</ul>
<p>这些工具和库在 AI 模型的全生命周期中起到关键作用，使研究者和开发者更容易开发和部署高效的 AI 系统。</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>现在请你再回答一下，什么时候应该使用 OpenAI 的 API？什么时候应该使用开源模型？或者自己开发 / 微调的模型？ 提示：的确，文中没有给出这个问题的答案。因为这个问题并没有标准答案。</p>
</li>
<li class="lvl-4">
<p>请你使用 HuggingFace 的 Transformers 库，下载新的模型进行推理，比较它们的性能。</p>
</li>
<li class="lvl-4">
<p>请你在 LangChain 中，使用 HuggingFaceHub 和 HuggingFace Pipeline 这两种接口，调用当前最流行的大语言模型。 提示：HuggingFace Model 页面，有模型下载量的当月排序，当月下载最多的模型就是最流行的模型。</p>
</li>
</ol>
<p>期待在留言区看到你的分享，我们一起交流探讨，共创一个良好的学习氛围。如果你觉得内容对你有帮助，也欢迎分享给有需要的朋友！最后如果你学有余力，可以进一步学习下面的延伸阅读。</p>
<h2 id="延伸阅读">延伸阅读</h2>
<ol>
<li class="lvl-4">
<p>Llama2，开源的可商用类 ChatGPT 模型，<a href="https://link.juejin.cn/?target=https%3A%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fllama-2-open-foundation-and-fine-tuned-chat-models%2F" title="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Facebook 链接</a>、<a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fllama" title="https://github.com/facebookresearch/llama">GitHub 链接</a></p>
</li>
<li class="lvl-4">
<p>HuggingFace <a href="https://link.juejin.cn/?target=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Findex" title="https://huggingface.co/docs/transformers/index">Transformer</a> 文档</p>
</li>
<li class="lvl-4">
<p>PyTorch 官方<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpytorch.org%2Ftutorials%2F" title="https://pytorch.org/tutorials/">教程</a>、<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Findex.html" title="https://pytorch.org/docs/stable/index.html">文档</a></p>
</li>
<li class="lvl-4">
<p><a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2FPanQiWei%2FAutoGPTQ" title="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ</a> 基于 GPTQ 算法的大模型量化工具包</p>
</li>
<li class="lvl-4">
<p><a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fggerganov%2Fllama.cpp" title="https://github.com/ggerganov/llama.cpp">Llama CPP</a> 支持 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fgithub.com%2Fggerganov%2Fggml" title="https://github.com/ggerganov/ggml">GGML</a>，目标是在 MacBook（或类似的非 GPU 的普通家用硬件环境）上使用 4 位整数量化运行 Llama 模型</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/62/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/62/">62</a><span class="page-number current">63</span><a class="page-number" href="/page/64/">64</a><span class="space">&hellip;</span><a class="page-number" href="/page/66/">66</a><a class="extend next" rel="next" href="/page/64/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">听</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
        
          <span class="site-state-item-count">653</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">听</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
