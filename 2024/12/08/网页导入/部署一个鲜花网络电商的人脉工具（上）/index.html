<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
<link rel="stylesheet" href="/css/custom.css">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="你好，我是黄佳，欢迎来到 LangChain 实战课！ 从今天开始，我要用 4 节课的篇幅，带着你设计两个有趣而又实用的应用程序。设计这两个应用程序的目的，是为了让你能够把 LangChain 中的各个组件灵活地组合起来，融会贯通，并以此作为启发，在你熟悉的业务场景中，利用 LangChain 和 LLM 的能力，开发出更多、更强大的效率工具。 第一个应用程序，是用 LangChain 创建出一个">
<meta property="og:type" content="article">
<meta property="og:title" content="O3xiaoyuhe">
<meta property="og:url" content="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E9%B2%9C%E8%8A%B1%E7%BD%91%E7%BB%9C%E7%94%B5%E5%95%86%E7%9A%84%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%8A%EF%BC%89/index.html">
<meta property="og:site_name" content="O3xiaoyuhe">
<meta property="og:description" content="你好，我是黄佳，欢迎来到 LangChain 实战课！ 从今天开始，我要用 4 节课的篇幅，带着你设计两个有趣而又实用的应用程序。设计这两个应用程序的目的，是为了让你能够把 LangChain 中的各个组件灵活地组合起来，融会贯通，并以此作为启发，在你熟悉的业务场景中，利用 LangChain 和 LLM 的能力，开发出更多、更强大的效率工具。 第一个应用程序，是用 LangChain 创建出一个">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/00/5f/0049810d3cfe1aee633d29722ded8e5f.png?wh=1860x1612">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/5d/0c/5dab492f802d34086975616d06708e0c.jpg?wh=152x270">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/52/f1/520688ef98a70c3d3651420bcc26bef1.jpg?wh=1805x1292">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/a2/d8/a22043515a4b9686c58ccedcda2075d8.jpg?wh=1336x670">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/86/88/86238bae7452cde52f23e4d6ea3a1688.jpg?wh=1909x1398">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/38/be/38643a041b2f24ed9e8405a485580cbe.jpg?wh=1275x1473">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/2a/c0/2a78e4b1cc0734f1c5ce171a05f153c0.jpg?wh=1275x465">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/06/5e/06da407d1f6d93eeaefa6e9f450cdf5e.jpg?wh=1837x1631">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/ab/6f/ab67de9be2b4f26be53c0e8af713f16f.jpg?wh=819x326">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/92/38/92ea6832ea69c8a1342a62180b7da538.jpg?wh=3842x1944">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/e6/94/e696b6dae6332a486763e29e9f310594.jpg?wh=1318x665">
<meta property="og:image" content="https://static001.geekbang.org/resource/image/16/10/16aaa4d952428bb1960ecfee9df6df10.jpg?wh=1310x217">
<meta property="article:published_time" content="2024-12-08T12:29:01.382Z">
<meta property="article:modified_time" content="2024-11-18T11:35:22.006Z">
<meta property="article:author" content="听">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://static001.geekbang.org/resource/image/00/5f/0049810d3cfe1aee633d29722ded8e5f.png?wh=1860x1612">

<link rel="canonical" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E9%B2%9C%E8%8A%B1%E7%BD%91%E7%BB%9C%E7%94%B5%E5%95%86%E7%9A%84%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%8A%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title> | O3xiaoyuhe</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">O3xiaoyuhe</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E9%B2%9C%E8%8A%B1%E7%BD%91%E7%BB%9C%E7%94%B5%E5%95%86%E7%9A%84%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%8A%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="听">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="O3xiaoyuhe">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-08 20:29:01" itemprop="dateCreated datePublished" datetime="2024-12-08T20:29:01+08:00">2024-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-11-18 19:35:22" itemprop="dateModified" datetime="2024-11-18T19:35:22+08:00">2024-11-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>你好，我是黄佳，欢迎来到 LangChain 实战课！</p>
<p>从今天开始，我要用 4 节课的篇幅，带着你设计两个有趣而又实用的应用程序。设计这两个应用程序的目的，是为了让你能够把 LangChain 中的各个组件灵活地组合起来，融会贯通，并以此作为启发，在你熟悉的业务场景中，利用 LangChain 和 LLM 的能力，开发出更多、更强大的效率工具。</p>
<p>第一个应用程序，是用 LangChain 创建出一个专属于 “易速鲜花” 的网络人脉工具。光这么说，有些模糊，这个人脉工具长啥样？有些啥具体功能？</p>
<p>动手之前，让我先给你把这个所谓 “人脉” 工具的能力和细节说清楚。</p>
<h2 id="“人脉工具”-项目说明">“人脉工具” 项目说明</h2>
<p><strong>项目背景</strong>：易速鲜花电商网络自从创建以来，通过微信、抖音、小红书等自媒体宣传推广，短期内获得了广泛流量展示。目前，营销部门希望以此为契机，再接再厉，继续扩大品牌影响力。经过调研，发现很多用户会通过微博热搜推荐的新闻来购买鲜花赠送给明星、达人等，因此各部门一致认为应该联络相关微博大 V，共同推广，带动品牌成长。</p>
<p>然而，发掘并选择适合于 “鲜花推广” 的微博大 V 有一定难度。营销部门员工表示，这个任务比找微信、抖音和小红书达人要难得多。他们都希望技术部门能够给出一个 “人脉搜索工具” 来协助完成这一目标。</p>
<p><strong>项目目标：</strong> 帮助市场营销部门的员工找到微博上适合做鲜花推广的大 V，并给出具体的联络方案。</p>
<h2 id="项目的技术实现细节">项目的技术实现细节</h2>
<p>这个项目的具体技术实现细节，这里简述如下。</p>
<p><strong>第一步：</strong> 通过 LangChain 的搜索工具，以模糊搜索的方式，帮助运营人员找到微博中有可能对相关鲜花推广感兴趣的大 V（比如喜欢玫瑰花的大 V），并返回 UID。</p>
<p><strong>第二步：</strong> 根据微博 UID，通过爬虫工具拿到相关大 V 的微博公开信息，并以 JSON 格式返回大 V 的数据。</p>
<p><img src="https://static001.geekbang.org/resource/image/00/5f/0049810d3cfe1aee633d29722ded8e5f.png?wh=1860x1612" alt=""></p>
<p><strong>第三步：</strong> 通过 LangChain 调用 LLM，通过 LLM 的总结整理以及生成功能，根据大 V 的个人信息，写一篇热情洋溢的介绍型文章，谋求与该大 V 的合作。</p>
<p><strong>第四步：</strong> 把 LangChain 输出解析功能加入进来，让 LLM 生成可以嵌入提示模板的格式化数据结构。</p>
<p><strong>第五步：</strong> 添加 HTML、CSS，并用 Flask 创建一个 App，在网络上部署及发布这个鲜花电商人脉工具，供市场营销部门的人员使用。</p>
<p>在上面的 5 个步骤中，我们使用到很多 LangChain 技术，包括<strong>提示工程、模型、链、代理、输出解析</strong>等。</p>
<p>这节课我们先来实现项目的前两个部分。</p>
<h2 id="第一步：找到大-V">第一步：找到大 V</h2>
<p>因为咱们的项目需要用到很多工具，所以我创建了一个项目目录，叫做 socializer_v0（项目每完成一步，我就创建一个新目录，并把版本号加 1）。当第一个步骤 “找到大 V” 实现之后，项目中的文档结构如下。</p>
<p><img src="https://static001.geekbang.org/resource/image/5d/0c/5dab492f802d34086975616d06708e0c.jpg?wh=152x270" alt=""></p>
<p>这里，主程序是 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>。意思就是派程序来作为智能代理，找到喜欢鲜花的微博大 V。</p>
<h2 id="主程序-findbigV-py">主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a></h2>
<p>主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a> 在第一步完成之后，是这样的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;&#x27;</span><br><span class="line">os.environ[&quot;SERPAPI_API_KEY&quot;] = &#x27;&#x27;</span><br><span class="line"></span><br><span class="line"># 导入所取的库</span><br><span class="line">import re</span><br><span class="line">from agents.weibo_agent import lookup_V</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    # 拿到UID</span><br><span class="line">    response_UID = lookup_V(flower_type = &quot;牡丹&quot; )</span><br><span class="line">    print(response_UID)</span><br><span class="line"></span><br><span class="line">    # 抽取UID里面的数字</span><br><span class="line">    UID = re.findall(r&#x27;\d+&#x27;, response_UID)[0]</span><br><span class="line">    print(&quot;这位鲜花大V的微博ID是&quot;, UID)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里，我们要搜到的，是一个热爱鲜花的大 V 的微博 UID，而不是 URL。</p>
<p><img src="https://static001.geekbang.org/resource/image/52/f1/520688ef98a70c3d3651420bcc26bef1.jpg?wh=1805x1292" alt=""></p>
<p>比如，上面这位喜欢牡丹花的大 V，他的 UID 是 6053338099。这些都是公开的信息。</p>
<p>为什么我们希望得到 UID 呢？因为我们可以通过这个 ID，爬取他个人主页里的更多介绍信息，有利于进一步了解他。</p>
<h3 id="微博-Agent：查找大-V-的-ID">微博 Agent：查找大 V 的 ID</h3>
<p>下面，我们就来看看，文件 agents\weibo_agent.py 中的 lookup_V 函数是如何实现这个搜寻 UID 的功能的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 导入一个搜索UID的工具</span><br><span class="line">from tools.search_tool import get_UID</span><br><span class="line"></span><br><span class="line"># 导入所需的库</span><br><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.agents import initialize_agent, Tool</span><br><span class="line">from langchain.agents import AgentType</span><br><span class="line"></span><br><span class="line"># 通过LangChain代理找到UID的函数</span><br><span class="line">def lookup_V(flower_type: str) :</span><br><span class="line">    # 初始化大模型</span><br><span class="line">    llm = ChatOpenAI(temperature=0, model_name=&quot;gpt-3.5-turbo&quot;)</span><br><span class="line"></span><br><span class="line">    # 寻找UID的模板</span><br><span class="line">    template = &quot;&quot;&quot;given the &#123;flower&#125; I want you to get a related 微博 UID.</span><br><span class="line">                  Your answer should contain only a UID.</span><br><span class="line">                  The URL always starts with https://weibo.com/u/</span><br><span class="line">                  for example, if https://weibo.com/u/1669879400 is her 微博, then 1669879400 is her UID</span><br><span class="line">                  This is only the example don&#x27;t give me this, but the actual UID&quot;&quot;&quot;</span><br><span class="line">    # 完整的提示模板</span><br><span class="line">    prompt_template = PromptTemplate(</span><br><span class="line">        input_variables=[&quot;flower&quot;], template=template</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 代理的工具</span><br><span class="line">    tools = [</span><br><span class="line">        Tool(</span><br><span class="line">            name=&quot;Crawl Google for 微博 page&quot;,</span><br><span class="line">            func=get_UID,</span><br><span class="line">            description=&quot;useful for when you need get the 微博 UID&quot;,</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    # 初始化代理</span><br><span class="line">    agent = initialize_agent(</span><br><span class="line">        tools, </span><br><span class="line">        llm, </span><br><span class="line">        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, </span><br><span class="line">        verbose=True</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 返回找到的UID</span><br><span class="line">    ID = agent.run(prompt_template.format_prompt(flower=flower_type))</span><br><span class="line"></span><br><span class="line">    return ID</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这段代码的目的，是为了通过提供的花的类型（flower type）来查找与之相关的微博 UID。其中使用了 LangChain 中的代理和工具。</p>
<p>这里有两点需要特别说明：</p>
<ol>
<li class="lvl-4">
<p>搜索 UID 的工具通过 from tools.search_tool import get_UID 导入，这个内容后面还会介绍。</p>
</li>
<li class="lvl-4">
<p>下面的提示模板说明，强调了需要的是 UID，而不是 URL。刚才说了，这是因为后续的爬虫工具需要一个特定的 UID，来获取该微博大 V 的个人信息（公开）。然后我们会继续利用这些信息让 LLM 为我们写 “勾搭” 文案。</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    # 寻找UID的模板</span><br><span class="line">    template = &quot;&quot;&quot;given the &#123;flower&#125; I want you to get a related 微博 UID.</span><br><span class="line">                  Your answer should contain only a UID.</span><br><span class="line">                  The URL always starts with https://weibo.com/u/</span><br><span class="line">                  for example, if https://weibo.com/u/1669879400 is her 微博, then 1669879400 is her UID</span><br><span class="line">                  This is only the example don&#x27;t give me this, but the actual UID&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="定制的-SerpAPI：getUID">定制的 SerpAPI：getUID</h3>
<p>上面的程序只是调用了代理，但是没有给出具体的工具实现。现在我们来继续实现搜索大 V 的 UID 的功能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 导入一个搜索UID的工具</span><br><span class="line">from tools.search_tool import get_UID</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个具体的实现，在代码 \tools\search_tool.py 中。</p>
<p>说到通过 LangChain 来搜索微博，相信你会马上想到已经多次使用过的 SerpAPI。我们先来试一试标准的 SerpAPI，看看它能否满足我们的需求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from langchain.utilities import SerpAPIWrapper</span><br><span class="line"></span><br><span class="line">def get_UID(flower: str):</span><br><span class="line">    &quot;&quot;&quot;Searches for Linkedin or twitter Profile Page.&quot;&quot;&quot;</span><br><span class="line">    search = SerpAPIWrapper()</span><br><span class="line">    res = search.run(f&quot;&#123;flower&#125;&quot;)</span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>写好了这段代码，第一步就可以说是完成了。下面我们跑一遍 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，看看程序会给出我们什么样的结果。</p>
<p><img src="https://static001.geekbang.org/resource/image/a2/d8/a22043515a4b9686c58ccedcda2075d8.jpg?wh=1336x670" alt=""></p>
<p>结果还好，不算太失望，SerpAPI 找到了一个貌似喜欢牡丹花的大 V，名叫戏精牡丹，搜到的信息也都是真实的。看起来他蛮适合为我们的牡丹花代言。然而，这个大 V 的微博 ID 肯定不是 6。</p>
<p>中间哪里或许是出了点小问题。</p>
<p>像这样的错误，明显发生在 LangChain 内部，那你的 trouble_shooting 也只能通过 Debug 来解决。这里，我就忽略掉一长串的错误排查过程，直接指出问题的根本原因所在。</p>
<p>让我们把断点设置在 SerpAPIWrapper 类的_process_response 中。</p>
<p><img src="https://static001.geekbang.org/resource/image/86/88/86238bae7452cde52f23e4d6ea3a1688.jpg?wh=1909x1398" alt=""></p>
<p>当程序进入 <code>if &quot;organic_results&quot; in res.keys()</code> 这段逻辑之后，我发现，它返回的总是一个 snippet（摘要文字），而不是 link（URL）。</p>
<p><img src="https://static001.geekbang.org/resource/image/38/be/38643a041b2f24ed9e8405a485580cbe.jpg?wh=1275x1473" alt=""></p>
<p>无论这背后的逻辑何在，这并不是我们所想要的。在 Debug 过程中，我们发现，新浪微博的 UID，实际上包含在 URL 中，也就是 <a href="https://link.juejin.cn/?target=https%3A%2F%2Fweibo.com%2Fu%2F6053338099" title="https://weibo.com/u/6053338099">weibo.com/u/605333809…</a>。因此，如果我们不返回微博的简短说明（戏精牡丹，搞笑视频自媒体……），而是返回 URL，会更有利于大模型提炼出 UID。</p>
<p><img src="https://static001.geekbang.org/resource/image/2a/c0/2a78e4b1cc0734f1c5ce171a05f153c0.jpg?wh=1275x465" alt=""></p>
<p>如何做呢？直接修改 LangChain 的 SerpAPIWrapper 类的_process_response 源代码肯定不是一个好办法。</p>
<p>因此，这里我们可以继承 SerpAPIWrapper 类，并构造一个 CustomSerpAPIWrapper 类，在这个类中，我们重构_process_response 这个静态方法。</p>
<p>新的 search_tool.py 完整代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># 导入SerpAPIWrapper</span><br><span class="line">from langchain.utilities import SerpAPIWrapper</span><br><span class="line"></span><br><span class="line"># 重新定制SerpAPIWrapper，重构_process_response，返回URL</span><br><span class="line">class CustomSerpAPIWrapper(SerpAPIWrapper):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CustomSerpAPIWrapper, self).__init__()</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def _process_response(res: dict) -&gt; str:</span><br><span class="line">        &quot;&quot;&quot;Process response from SerpAPI.&quot;&quot;&quot;</span><br><span class="line">        if &quot;error&quot; in res.keys():</span><br><span class="line">            raise ValueError(f&quot;Got error from SerpAPI: &#123;res[&#x27;error&#x27;]&#125;&quot;)</span><br><span class="line">        if &quot;answer_box_list&quot; in res.keys():</span><br><span class="line">            res[&quot;answer_box&quot;] = res[&quot;answer_box_list&quot;]</span><br><span class="line">        &#x27;&#x27;&#x27;删去很多无关代码&#x27;&#x27;&#x27;</span><br><span class="line">        snippets = []</span><br><span class="line">        if &quot;knowledge_graph&quot; in res.keys():</span><br><span class="line">            knowledge_graph = res[&quot;knowledge_graph&quot;]</span><br><span class="line">            title = knowledge_graph[&quot;title&quot;] if &quot;title&quot; in knowledge_graph else &quot;&quot;</span><br><span class="line">            if &quot;description&quot; in knowledge_graph.keys():</span><br><span class="line">                snippets.append(knowledge_graph[&quot;description&quot;])</span><br><span class="line">            for key, value in knowledge_graph.items():</span><br><span class="line">                if (</span><br><span class="line">                    isinstance(key, str)</span><br><span class="line">                    and isinstance(value, str)</span><br><span class="line">                    and key not in [&quot;title&quot;, &quot;description&quot;]</span><br><span class="line">                    and not key.endswith(&quot;_stick&quot;)</span><br><span class="line">                    and not key.endswith(&quot;_link&quot;)</span><br><span class="line">                    and not value.startswith(&quot;http&quot;)</span><br><span class="line">                ):</span><br><span class="line">                    snippets.append(f&quot;&#123;title&#125; &#123;key&#125;: &#123;value&#125;.&quot;)</span><br><span class="line">        if &quot;organic_results&quot; in res.keys():</span><br><span class="line">            first_organic_result = res[&quot;organic_results&quot;][0]</span><br><span class="line">            if &quot;snippet&quot; in first_organic_result.keys():</span><br><span class="line">                # 此处是关键修改</span><br><span class="line">                # snippets.append(first_organic_result[&quot;snippet&quot;])</span><br><span class="line">                snippets.append(first_organic_result[&quot;link&quot;])                </span><br><span class="line">            elif &quot;snippet_highlighted_words&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;snippet_highlighted_words&quot;])</span><br><span class="line">            elif &quot;rich_snippet&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;rich_snippet&quot;])</span><br><span class="line">            elif &quot;rich_snippet_table&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;rich_snippet_table&quot;])</span><br><span class="line">            elif &quot;link&quot; in first_organic_result.keys():</span><br><span class="line">                snippets.append(first_organic_result[&quot;link&quot;])</span><br><span class="line">        if &quot;buying_guide&quot; in res.keys():</span><br><span class="line">            snippets.append(res[&quot;buying_guide&quot;])</span><br><span class="line">        if &quot;local_results&quot; in res.keys() and &quot;places&quot; in res[&quot;local_results&quot;].keys():</span><br><span class="line">            snippets.append(res[&quot;local_results&quot;][&quot;places&quot;])</span><br><span class="line"></span><br><span class="line">        if len(snippets) &gt; 0:</span><br><span class="line">            return str(snippets)</span><br><span class="line">        else:</span><br><span class="line">            return &quot;No good search result found&quot;</span><br><span class="line"></span><br><span class="line"># 获取与某种鲜花相关的微博UID的函数</span><br><span class="line">def get_UID(flower: str):</span><br><span class="line">    &quot;&quot;&quot;Searches for Linkedin or twitter Profile Page.&quot;&quot;&quot;</span><br><span class="line">    # search = SerpAPIWrapper()</span><br><span class="line">    search = CustomSerpAPIWrapper()</span><br><span class="line">    res = search.run(f&quot;&#123;flower&#125;&quot;)</span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>唯一的区别就是，我们在下面的逻辑中返回了 link，而不是 snippet。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">        if &quot;organic_results&quot; in res.keys():</span><br><span class="line">            first_organic_result = res[&quot;organic_results&quot;][0]</span><br><span class="line">            if &quot;snippet&quot; in first_organic_result.keys():</span><br><span class="line">                # snippets.append(first_organic_result[&quot;snippet&quot;])</span><br><span class="line">                snippets.append(first_organic_result[&quot;link&quot;]) </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>再次 Debug，我们发现返回的 snippets 里面包含了 URL 信息，其中 UID 信息包含在 URL 中了。</p>
<p><img src="https://static001.geekbang.org/resource/image/06/5e/06da407d1f6d93eeaefa6e9f450cdf5e.jpg?wh=1837x1631" alt=""></p>
<p>此时运行主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，会发现代理中返回了 URL 信息，并且经过进一步思考，提炼出了 UID。</p>
<p><img src="https://static001.geekbang.org/resource/image/ab/6f/ab67de9be2b4f26be53c0e8af713f16f.jpg?wh=819x326" alt=""></p>
<h2 id="第二步：爬取大-V-资料">第二步：爬取大 V 资料</h2>
<p>好的，第一步虽然是有磕有绊，但是经过了调整的 CustomSerpAPIWrapper 工具和代理，在 LLM 的帮助之下，总算是不辱使命，完成了找到 UID 的任务。</p>
<p>这位大 V，看起来又喜欢牡丹，又喜欢搞笑。我们很想和他联络一下，也许他很适合为我们的牡丹花品牌代言。（到底是否适合，不必特别认真哈，总之搜索 “牡丹”，Agent 给了这个 ID，就可以了。咱学的是 LangChain，不是真的要找他代言）</p>
<p>不过，知己知彼，百战不殆。想要和他沟通，就得了解他更多。下面，我们将使用爬虫程序，通过 UID 来爬取他的更多信息。</p>
<h3 id="主程序-findbigV-py-2">主程序 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a></h3>
<p>第二步完成之后，主程序代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 设置OpenAI API密钥</span><br><span class="line">import os</span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;Your OpenAI API Key&#x27;</span><br><span class="line">os.environ[&quot;SERPAPI_API_KEY&quot;] = &#x27;Your SerpAPI Key&#x27;</span><br><span class="line"></span><br><span class="line"># 导入所取的库</span><br><span class="line">import re</span><br><span class="line">from agents.weibo_agent import lookup_V</span><br><span class="line">from tools.general_tool import remove_non_chinese_fields</span><br><span class="line">from tools.scraping_tool import get_data</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line"></span><br><span class="line">    # 拿到UID</span><br><span class="line">    response_UID = lookup_V(flower_type = &quot;牡丹&quot; )</span><br><span class="line"></span><br><span class="line">    # 抽取UID里面的数字</span><br><span class="line">    UID = re.findall(r&#x27;\d+&#x27;, response_UID)[0]</span><br><span class="line">    print(&quot;这位鲜花大V的微博ID是&quot;, UID)</span><br><span class="line"></span><br><span class="line">    # 根据UID爬取大V信息</span><br><span class="line">    person_info = get_data(UID)</span><br><span class="line">    print(person_info)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从第一步到第二步，我们主要是完成了一次微博信息的爬取。</p>
<h3 id="scraping-tool-py-中的-scrape-weibo-方法">scraping_tool.py 中的 scrape_weibo 方法</h3>
<p>第二步中的关键逻辑是 scraping_tool.py 中的 scrape_weibo 方法，具体代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 导入所需的库</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># 定义爬取微博用户信息的函数</span><br><span class="line">def scrape_weibo(url: str):</span><br><span class="line">    &#x27;&#x27;&#x27;爬取相关鲜花服务商的资料&#x27;&#x27;&#x27;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36&quot;,</span><br><span class="line">        &quot;Referer&quot;: &quot;https://weibo.com&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    cookies = &#123;</span><br><span class="line">        &quot;cookie&quot;: &#x27;&#x27;&#x27;SINAGLOBAL=3762226753815.13.1696496172299; ALF=1699182321; SCF=AiOo8xtPwGonZcAbYyHXZbz9ixm97mWi0vHt_VvuOKB-u4-rcvlGtWCrE6MfMucpxiOy5bYpkIFNWTj7nYGcyp4.; _sc_token=v2%3A2qyeqD3cTZFNTl0sn3KAYe4fNqzMUEP-C7nxNsd_Q1r-vpYMlF2K3xc4vWNuLNBbp3RsohghkJdlSVN09cymVo5AKAm0V92004V8cSRe9O5v9B65jd4yiG_sATDeB06GnjiJulXUrEF_6XsHh1ozK6jvbTKEUIkF7v0_BlbX6IcWrPkwh6xL_WM_0YUV2v7CtNPwyxfbAjaWnG32TsxG_ftN3s5m7qfaRftU6iTOSnE%3D; XSRF-TOKEN=4o0E6jaUQ0BlN77az0sURTg3; PC_TOKEN=dcf0e7607f; login_sid_t=36ebf31f1b3694fb71e77e35d30f052f; cross_origin_proto=SSL; WBStorage=4d96c54e|undefined; _s_tentry=passport.weibo.com; UOR=www.google.com,weibo.com,login.sina.com.cn; Apache=7563213131783.361.1696667509205; ULV=1696667509207:2:2:2:7563213131783.361.1696667509205:1696496172302; wb_view_log=3440*14401; WBtopGlobal_register_version=2023100716; crossidccode=CODE-gz-1QP2Jh-13l47h-79FGqrAQgQbR8ccb7b504; SSOLoginState=1696667553; SUB=_2A25IJWfwDeThGeFJ6lsQ-SbNzjuIHXVr5gm4rDV8PUJbkNAbLUWtkW1NfJd_XHamKIzj5RlT_-RGMma6z3YQZUK3; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFDKvBlvg14YuHk_4c6MEH_5NHD95QNS024eK.ReK-NWs4DqcjZCJ8oIN.pSKzceBtt; WBPSESS=gyY2mn77F4p5VxWF2IB_yFR0phHVTNfaJAHAMprnW7MeUr-NHPZNyeeyKae3tHELlc_RbcI1XPSz-TjSJqWrIXs-yh1fwhxL4mSDrnpPZEogFt8ScF5NEwSqPGn7x2KMAgTHtWde-3MBm6orQ98PDA==&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url, headers=headers, cookies=cookies)</span><br><span class="line">    time.sleep(3)   # 加上3s 的延时防止被反爬</span><br><span class="line">    return response.text</span><br><span class="line"></span><br><span class="line"># 根据UID构建URL爬取信息</span><br><span class="line">def get_data(id):</span><br><span class="line">    url = &quot;https://weibo.com/ajax/profile/detail?uid=&#123;&#125;&quot;.format(id)</span><br><span class="line">    html = scrape_weibo(url)</span><br><span class="line">    response = json.loads(html)</span><br><span class="line"></span><br><span class="line">    return response</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我这段爬虫代码特别简洁，不需要过多的解释，唯一需要说明的部分是怎么找到你自己的 Cookies。</p>
<p>Cookie 是由服务器发送到用户浏览器的一小段数据，并可能在随后的请求中被回传。它的主要目的是让服务器知道用户的上下文信息或状态。在 Web 爬虫中，使用正确的 Cookie 可以模拟登录状态，从而获取到需要权限的网页内容。</p>
<p>首先，我是用 QQ ID 登录的微博，我发现通过这样的方式找到的 Cookie 能用得比较久。</p>
<p>然后，从我的浏览器中获取 Cookie，以下是简单步骤：</p>
<ol>
<li class="lvl-4">
<p>使用浏览器（如 Chrome、Firefox）访问微博并登录。</p>
</li>
<li class="lvl-4">
<p>登录后，右键单击页面并选择 “检查”（Inspect）。</p>
</li>
<li class="lvl-4">
<p>打开开发者工具，点击 Network 选项卡。</p>
</li>
<li class="lvl-4">
<p>在页面上进行一些操作（如刷新页面），然后在 Network 选项卡下查看请求列表。</p>
</li>
<li class="lvl-4">
<p>选择任一请求项，然后在右侧的 Headers 选项卡中查找 Request Headers 部分。</p>
</li>
<li class="lvl-4">
<p>在这部分中，你应该可以看到一个名为 Cookie 的字段，这就是你需要的 Cookie 值。</p>
</li>
</ol>
<p>将获取到的完整 Cookie 值复制（挺长的），并替换上述代码中的 <code>&quot;你的Cookie&quot;</code> 部分。</p>
<p><img src="https://static001.geekbang.org/resource/image/92/38/92ea6832ea69c8a1342a62180b7da538.jpg?wh=3842x1944" alt=""></p>
<p>但请注意，微博的 Cookie 可能有过期时间，所以如果你发现一段时间后你的爬虫无法正常工作，你可能需要再次获取新的 Cookie。同时，频繁地爬取或大量请求可能会导致你的账号被封禁，所以请谨慎使用爬虫。</p>
<p>此时，运行 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，就得到了下面的输出。</p>
<p><img src="https://static001.geekbang.org/resource/image/e6/94/e696b6dae6332a486763e29e9f310594.jpg?wh=1318x665" alt=""></p>
<h3 id="精简爬取输出">精简爬取输出</h3>
<p>最后一个步骤，是精简上面的输出，因为类似 <code>'word_color': '#FFEA8011', 'background_color': '#FF181818'</code> 这样的内容会占据很多 Token 空间，而且对于 LLM 总结整理信息，也没啥作用。</p>
<p>因此，我创建了一个额外的步骤，就是 \ tools\general_tool.py 中的 remove_non_chinese_fields 函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">def contains_chinese(s):</span><br><span class="line">    return bool(re.search(&#x27;[\u4e00-\u9fa5]&#x27;, s))</span><br><span class="line"></span><br><span class="line">def remove_non_chinese_fields(d):</span><br><span class="line">    if isinstance(d, dict):</span><br><span class="line">        to_remove = [key for key, value in d.items() if isinstance(value, (str, int, float, bool)) and (not contains_chinese(str(value)))]</span><br><span class="line">        for key in to_remove:</span><br><span class="line">            del d[key]</span><br><span class="line">        </span><br><span class="line">        for key, value in d.items():</span><br><span class="line">            if isinstance(value, (dict, list)):</span><br><span class="line">                remove_non_chinese_fields(value)</span><br><span class="line">    elif isinstance(d, list):</span><br><span class="line">        to_remove_indices = []</span><br><span class="line">        for i, item in enumerate(d):</span><br><span class="line">            if isinstance(item, (str, int, float, bool)) and (not contains_chinese(str(item))):</span><br><span class="line">                to_remove_indices.append(i)</span><br><span class="line">            else:</span><br><span class="line">                remove_non_chinese_fields(item)</span><br><span class="line">        </span><br><span class="line">        for index in reversed(to_remove_indices):</span><br><span class="line">            d.pop(index)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a> 中，调用这个函数，对爬虫的输出结果进行了精简。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    # 移除无用的信息</span><br><span class="line">    remove_non_chinese_fields(person_info)</span><br><span class="line">    print(person_info)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>重新运行 <a target="_blank" rel="noopener" href="http://findbigV.py">findbigV.py</a>，结果如下：</p>
<p><img src="https://static001.geekbang.org/resource/image/16/10/16aaa4d952428bb1960ecfee9df6df10.jpg?wh=1310x217" alt=""></p>
<p>此时，爬取的内容就只剩下了干货。</p>
<h2 id="总结时刻">总结时刻</h2>
<p>这节课我们完成了前两步的工作。分别是，找到适合推广某种鲜花的大 V 的微博 UID，并且爬取了大 V 的资料。这为我们后续生成文本、进一步链接大 V 打下了良好的基础。</p>
<p>其中，我们用到了大量之前学习过的 LangChain 组件，具体包括：</p>
<ol>
<li class="lvl-4">
<p>用提示模板告诉大模型我们要找到内容（UID）。</p>
</li>
<li class="lvl-4">
<p>调用 LLM。</p>
</li>
<li class="lvl-4">
<p>使用 Chain。</p>
</li>
<li class="lvl-4">
<p>使用 Agent。</p>
</li>
<li class="lvl-4">
<p>在 Agent 中，我们使用了一个 Customized Tool，因为 LangChain 内置的 SerpAPI Tool 不能完全满足我们的需要。这给了我们一个好机会创建自己的 “私人定制” Tool。</p>
</li>
</ol>
<p>在下节课中，我们还要继续利用大模型的总结文本、生成文本的功能，来为我们撰写能够打动大 V 和咱易速鲜花合作的文案，我们还将利用 Output Parser 把文案解析成需要的格式，部署到网络服务器端。敬请期待！</p>
<h2 id="思考题">思考题</h2>
<ol>
<li class="lvl-4">
<p>如果 Agent 不返回 UID，而是返回 URL，是不是也能够完成这个任务？你可以尝试重构提示模板以及后续逻辑，返回 URL，然后手动从 URL 中解析出 UID。</p>
</li>
<li class="lvl-4">
<p>研究一下 SerpAPIWrapper 类的_process_response 中的代码，看看这个方法具体是怎么设计的，用来实现了什么功能？</p>
</li>
</ol>
<p>期待在留言区看到你的分享，如果觉得内容对你有帮助，也欢迎分享给有需要的朋友！</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E9%B2%9C%E8%8A%B1%E7%BD%91%E7%BB%9C%E7%94%B5%E5%95%86%E7%9A%84%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%EF%BC%88%E4%B8%8B%EF%BC%89/" rel="prev" title="">
      <i class="fa fa-chevron-left"></i> 
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/12/08/%E7%BD%91%E9%A1%B5%E5%AF%BC%E5%85%A5/%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%9A%E9%80%9A%E8%BF%87%E9%93%BE%E5%92%8C%E4%BB%A3%E7%90%86%E6%9F%A5%E8%AF%A2%E9%B2%9C%E8%8A%B1%E4%BF%A1%E6%81%AF/" rel="next" title="">
       <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%80%9C%E4%BA%BA%E8%84%89%E5%B7%A5%E5%85%B7%E2%80%9D-%E9%A1%B9%E7%9B%AE%E8%AF%B4%E6%98%8E"><span class="nav-number">1.</span> <span class="nav-text">“人脉工具” 项目说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-number">2.</span> <span class="nav-text">项目的技术实现细节</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E6%89%BE%E5%88%B0%E5%A4%A7-V"><span class="nav-number">3.</span> <span class="nav-text">第一步：找到大 V</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E7%A8%8B%E5%BA%8F-findbigV-py"><span class="nav-number">4.</span> <span class="nav-text">主程序 findbigV.py</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%AE%E5%8D%9A-Agent%EF%BC%9A%E6%9F%A5%E6%89%BE%E5%A4%A7-V-%E7%9A%84-ID"><span class="nav-number">4.1.</span> <span class="nav-text">微博 Agent：查找大 V 的 ID</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E5%88%B6%E7%9A%84-SerpAPI%EF%BC%9AgetUID"><span class="nav-number">4.2.</span> <span class="nav-text">定制的 SerpAPI：getUID</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E7%88%AC%E5%8F%96%E5%A4%A7-V-%E8%B5%84%E6%96%99"><span class="nav-number">5.</span> <span class="nav-text">第二步：爬取大 V 资料</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E7%A8%8B%E5%BA%8F-findbigV-py-2"><span class="nav-number">5.1.</span> <span class="nav-text">主程序 findbigV.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scraping-tool-py-%E4%B8%AD%E7%9A%84-scrape-weibo-%E6%96%B9%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">scraping_tool.py 中的 scrape_weibo 方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B2%BE%E7%AE%80%E7%88%AC%E5%8F%96%E8%BE%93%E5%87%BA"><span class="nav-number">5.3.</span> <span class="nav-text">精简爬取输出</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%97%B6%E5%88%BB"><span class="nav-number">6.</span> <span class="nav-text">总结时刻</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="nav-number">7.</span> <span class="nav-text">思考题</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">听</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">653</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">听</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
